{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca53a17",
   "metadata": {},
   "source": [
    "#### nltk library installation for BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89eb21f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\danis\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\danis\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\danis\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\danis\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\danis\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\danis\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e4a9e",
   "metadata": {},
   "source": [
    "### Extracting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e64285bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# Path to dataset file\n",
    "dataset_path = 'MLDS_hw2_1_data.tar.gz'\n",
    "\n",
    "with tarfile.open(dataset_path, 'r:gz') as tar:\n",
    "    tar.extractall(path='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d237e86",
   "metadata": {},
   "source": [
    "### Loads captions and feature files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "90f1855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class VideoCaptionDataset(Dataset):\n",
    "    def __init__(self, id_file, feat_folder, captions_data, vocab):\n",
    "        self.feat_folder = feat_folder\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        # Load video IDs from id.txt\n",
    "        with open(id_file, 'r') as f:\n",
    "            self.video_ids = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "        # Numericalize captions\n",
    "        self.captions_data = {item['id']: self.vocab.numericalize(item['caption'][0]) for item in captions_data}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the video ID\n",
    "        video_id = self.video_ids[idx]\n",
    "        \n",
    "        # Load the precomputed features from the .npy file\n",
    "        feat_path = os.path.join(self.feat_folder, video_id + '.npy')\n",
    "        features = np.load(feat_path)\n",
    "        features = torch.tensor(features, dtype=torch.float32)\n",
    "        \n",
    "        # Get the corresponding numericalized caption and convert to tensor\n",
    "        caption = torch.tensor(self.captions_data[video_id], dtype=torch.long)\n",
    "        \n",
    "        return features, caption\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "da615ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def pad_collate_fn(batch):\n",
    "\n",
    "    \n",
    "    batch_features, batch_captions = zip(*batch)\n",
    "    \n",
    "    # Stack the features\n",
    "    batch_features = torch.stack(batch_features, dim=0)\n",
    "    \n",
    "    # Find the length of the longest caption\n",
    "    max_length = max([len(caption) for caption in batch_captions])\n",
    "    \n",
    "    # Pad all captions to the same length with <PAD> token\n",
    "    padded_captions = []\n",
    "    for caption in batch_captions:\n",
    "        padded_caption = F.pad(torch.tensor(caption), (0, max_length - len(caption)), value=vocab.word2idx[vocab.pad_token])\n",
    "        padded_captions.append(padded_caption)\n",
    "    \n",
    "    # Stack padded captions\n",
    "    padded_captions = torch.stack(padded_captions, dim=0)\n",
    "    \n",
    "    return batch_features, padded_captions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe24ca7f",
   "metadata": {},
   "source": [
    "### DataLoader for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ea0f97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the training data\n",
    "train_id_file = './data/MLDS_hw2_1_data/training_data/id.txt'\n",
    "train_feat_folder = './data/MLDS_hw2_1_data/training_data/feat'\n",
    "train_captions_file = './data/MLDS_hw2_1_data/training_label.json'\n",
    "\n",
    "# Paths to the testing data\n",
    "test_id_file = './data/MLDS_hw2_1_data/testing_data/id.txt'\n",
    "test_feat_folder = './data/MLDS_hw2_1_data/testing_data/feat'\n",
    "test_captions_file = './data/MLDS_hw2_1_data/testing_label.json'\n",
    "\n",
    "# Load captions JSON file for training\n",
    "with open(train_captions_file, 'r') as f:\n",
    "    train_captions = json.load(f)\n",
    "\n",
    "# Load captions JSON file for testing\n",
    "with open(test_captions_file, 'r') as f:\n",
    "    test_captions = json.load(f)\n",
    "\n",
    "# Creates the vocabulary and build it from the captions\n",
    "vocab = Vocabulary(min_freq=4)\n",
    "all_captions = list(itertools.chain.from_iterable([item['caption'] for item in train_captions]))\n",
    "vocab.build_vocab(all_captions)\n",
    "\n",
    "# dataset and dataloader for training\n",
    "train_dataset = VideoCaptionDataset(id_file=train_id_file, feat_folder=train_feat_folder, captions_data=train_captions, vocab=vocab)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=pad_collate_fn)\n",
    "\n",
    "# dataset and dataloader for testing\n",
    "test_dataset = VideoCaptionDataset(id_file=test_id_file, feat_folder=test_feat_folder, captions_data=test_captions, vocab=vocab)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=pad_collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42afad57",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2e4b2f",
   "metadata": {},
   "source": [
    "#### Vocabulary Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7bcf33e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=1):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.min_freq = min_freq\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.bos_token = \"<BOS>\"\n",
    "        self.eos_token = \"<EOS>\"\n",
    "        self.unk_token = \"<UNK>\"\n",
    "        self.special_tokens = [self.pad_token, self.bos_token, self.eos_token, self.unk_token]\n",
    "        self.word_counter = Counter()\n",
    "\n",
    "    def build_vocab(self, captions):\n",
    "        for caption in captions:\n",
    "            tokens = caption.split() \n",
    "            self.word_counter.update(tokens)\n",
    "        \n",
    "\n",
    "        idx = 0\n",
    "        for token in self.special_tokens:\n",
    "            self.word2idx[token] = idx\n",
    "            self.idx2word[idx] = token\n",
    "            idx += 1\n",
    "        \n",
    "\n",
    "        for word, count in self.word_counter.items():\n",
    "            if count >= self.min_freq:\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "\n",
    "        return [self.word2idx.get(word, self.word2idx[self.unk_token]) for word in text.split()]\n",
    "\n",
    "    def denumericalize(self, indices):\n",
    "\n",
    "        return [self.idx2word.get(idx, self.unk_token) for idx in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba513b",
   "metadata": {},
   "source": [
    "#### Builds vocabulary from the captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cab0e915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3594\n"
     ]
    }
   ],
   "source": [
    "# Extracts all captions from the JSON data\n",
    "all_captions = list(itertools.chain.from_iterable([item['caption'] for item in train_captions]))\n",
    "\n",
    "# Creates the vocabulary and build it from the captions\n",
    "vocab = Vocabulary(min_freq=3)\n",
    "vocab.build_vocab(all_captions)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab.word2idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eeb0c5",
   "metadata": {},
   "source": [
    "#### Numericalizes the captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0d4517ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3594\n"
     ]
    }
   ],
   "source": [
    "# Extracts all captions from the JSON data\n",
    "all_captions = list(itertools.chain.from_iterable([item['caption'] for item in train_captions]))\n",
    "\n",
    "# Creates the vocabulary and build it from the captions\n",
    "vocab = Vocabulary(min_freq=3)\n",
    "vocab.build_vocab(all_captions)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab.word2idx)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a87c8e2",
   "metadata": {},
   "source": [
    "### Testing dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ed6fd3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Feature shape: torch.Size([4, 80, 4096])\n",
      "Captions: tensor([[   4,  185,   15, 1049,  868,    0,    0,    0],\n",
      "        [   4,   57,   15,  273, 1096,  148,    8,    3],\n",
      "        [   4,  117,   15,  158,   31,    8,  551,    0],\n",
      "        [   4, 2073,   15,   51,  603,   31,    8,  967]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danis\\AppData\\Local\\Temp\\ipykernel_15344\\2609429317.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_caption = F.pad(torch.tensor(caption), (0, max_length - len(caption)), value=vocab.word2idx[vocab.pad_token])\n"
     ]
    }
   ],
   "source": [
    "# Test the training DataLoader\n",
    "for i, (features, captions) in enumerate(train_dataloader):\n",
    "    print(f'Batch {i+1}')\n",
    "    print(f'Feature shape: {features.shape}')\n",
    "    print(f'Captions: {captions}') \n",
    "    break  # to check the first batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023fee8",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "07bdeeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, feature_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc = nn.Linear(feature_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, features):\n",
    "        # features: (batch_size, num_frames, feature_size)\n",
    "        features = self.fc(features) \n",
    "        features = self.relu(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc8c8e",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "27b7d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size, embedding_size, num_layers=1):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size + hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.attention = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, features, captions, hidden=None):\n",
    "        batch_size = features.size(0)\n",
    "        num_frames = features.size(1)\n",
    "        hidden_size = features.size(2)\n",
    "        \n",
    "\n",
    "        if hidden is None:\n",
    "            h_0 = torch.zeros(1, batch_size, hidden_size).to(features.device)  \n",
    "            c_0 = torch.zeros(1, batch_size, hidden_size).to(features.device)  \n",
    "            hidden = (h_0, c_0)\n",
    "        \n",
    "        # Embed the captions\n",
    "        embeddings = self.embedding(captions)  \n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(embeddings.size(1)):\n",
    "            # Apply attention over the video features\n",
    "            attention_weights = torch.bmm(features, hidden[0][-1].unsqueeze(2)).squeeze(2) \n",
    "            attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "            attention_applied = torch.bmm(attention_weights.unsqueeze(1), features)  \n",
    "            \n",
    "            # Concatenate the attention-applied video features with the current word embedding\n",
    "            lstm_input = torch.cat((attention_applied.squeeze(1), embeddings[:, t, :]), dim=1) \n",
    "            \n",
    "            # Pass through LSTM\n",
    "            lstm_output, hidden = self.lstm(lstm_input.unsqueeze(1), hidden)  \n",
    "            \n",
    "            # Generate the output (next word prediction)\n",
    "            output = self.fc(lstm_output.squeeze(1))\n",
    "            outputs.append(output)\n",
    "        \n",
    "        # Stack outputs along the time dimension\n",
    "        outputs = torch.stack(outputs, dim=1) \n",
    "        \n",
    "        return outputs, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb944d0",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2b69b70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        # Passes the features through the encoder\n",
    "        encoder_outputs = self.encoder(features)\n",
    "        \n",
    "        # Passes the encoded features and captions to the decoder\n",
    "        outputs, _ = self.decoder(encoder_outputs, captions) \n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "09393f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Hyperparameters\n",
    "feature_size = 4096  # feature size\n",
    "hidden_size = 512    # size of hidden state in the LSTM\n",
    "vocab_size = len(vocab.word2idx)  # Size of the vocabulary\n",
    "embedding_size = 256  # Size of the word embeddings\n",
    "num_layers = 1  # Number of LSTM layers\n",
    "\n",
    "# Initialize encoder and decoder\n",
    "encoder = Encoder(feature_size, hidden_size)\n",
    "decoder = DecoderWithAttention(hidden_size, vocab_size, embedding_size, num_layers)\n",
    "\n",
    "# Create Seq2Seq model\n",
    "seq2seq_model = Seq2Seq(encoder, decoder)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx[vocab.pad_token])\n",
    "optimizer = optim.Adam(seq2seq_model.parameters(), lr=0.00005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61232cc4",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c2611444",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danis\\AppData\\Local\\Temp\\ipykernel_15344\\2609429317.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_caption = F.pad(torch.tensor(caption), (0, max_length - len(caption)), value=vocab.word2idx[vocab.pad_token])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Step 1, Loss: 8.02110481262207\n",
      "Epoch 1/100, Step 11, Loss: 7.864769458770752\n",
      "Epoch 1/100, Step 21, Loss: 7.748021602630615\n",
      "Epoch 1/100, Step 31, Loss: 7.611619472503662\n",
      "Epoch 1/100, Step 41, Loss: 7.205143451690674\n",
      "Epoch 1/100, Step 51, Loss: 7.001648902893066\n",
      "Epoch 1/100, Step 61, Loss: 6.374505996704102\n",
      "Epoch 1/100, Step 71, Loss: 6.467390060424805\n",
      "Epoch 1/100, Step 81, Loss: 6.102726459503174\n",
      "Epoch 1/100, Step 91, Loss: 6.46403694152832\n",
      "Epoch 1/100, Step 101, Loss: 5.951260089874268\n",
      "Epoch 1/100, Step 111, Loss: 5.48821496963501\n",
      "Epoch 1/100, Step 121, Loss: 5.579092025756836\n",
      "Epoch 1/100, Step 131, Loss: 5.6846795082092285\n",
      "Epoch 1/100, Step 141, Loss: 5.632481098175049\n",
      "Epoch 1/100, Step 151, Loss: 5.295943260192871\n",
      "Epoch 1/100, Step 161, Loss: 6.054490566253662\n",
      "Epoch 1/100, Step 171, Loss: 4.541777610778809\n",
      "Epoch 1/100, Step 181, Loss: 4.910405158996582\n",
      "Epoch 1/100, Step 191, Loss: 5.198885440826416\n",
      "Epoch 1/100, Step 201, Loss: 5.6328020095825195\n",
      "Epoch 1/100, Step 211, Loss: 6.2275543212890625\n",
      "Epoch 1/100, Step 221, Loss: 5.484579563140869\n",
      "Epoch 1/100, Step 231, Loss: 6.1997599601745605\n",
      "Epoch 1/100, Step 241, Loss: 5.5649333000183105\n",
      "Epoch 1/100, Step 251, Loss: 5.6287736892700195\n",
      "Epoch 1/100, Step 261, Loss: 6.1780219078063965\n",
      "Epoch 1/100, Step 271, Loss: 6.198678016662598\n",
      "Epoch 1/100, Step 281, Loss: 5.4602203369140625\n",
      "Epoch 1/100, Step 291, Loss: 6.126011371612549\n",
      "Epoch 1/100, Step 301, Loss: 5.252743721008301\n",
      "Epoch 1/100, Step 311, Loss: 5.752121448516846\n",
      "Epoch 1/100, Step 321, Loss: 5.143368721008301\n",
      "Epoch 1/100, Step 331, Loss: 5.952009677886963\n",
      "Epoch 1/100, Step 341, Loss: 5.569059371948242\n",
      "Epoch 1/100, Step 351, Loss: 6.055813789367676\n",
      "Epoch 1/100, Step 361, Loss: 4.904695987701416\n",
      "Epoch 2/100, Step 1, Loss: 5.011216640472412\n",
      "Epoch 2/100, Step 11, Loss: 5.719529628753662\n",
      "Epoch 2/100, Step 21, Loss: 5.595730781555176\n",
      "Epoch 2/100, Step 31, Loss: 5.628201007843018\n",
      "Epoch 2/100, Step 41, Loss: 5.805042266845703\n",
      "Epoch 2/100, Step 51, Loss: 5.822065830230713\n",
      "Epoch 2/100, Step 61, Loss: 5.090363502502441\n",
      "Epoch 2/100, Step 71, Loss: 4.613747596740723\n",
      "Epoch 2/100, Step 81, Loss: 4.545672416687012\n",
      "Epoch 2/100, Step 91, Loss: 5.049314022064209\n",
      "Epoch 2/100, Step 101, Loss: 5.158661842346191\n",
      "Epoch 2/100, Step 111, Loss: 4.817266464233398\n",
      "Epoch 2/100, Step 121, Loss: 5.555263519287109\n",
      "Epoch 2/100, Step 131, Loss: 5.0314412117004395\n",
      "Epoch 2/100, Step 141, Loss: 5.5738654136657715\n",
      "Epoch 2/100, Step 151, Loss: 4.8681135177612305\n",
      "Epoch 2/100, Step 161, Loss: 4.437424659729004\n",
      "Epoch 2/100, Step 171, Loss: 5.804763317108154\n",
      "Epoch 2/100, Step 181, Loss: 4.664271354675293\n",
      "Epoch 2/100, Step 191, Loss: 4.84455680847168\n",
      "Epoch 2/100, Step 201, Loss: 5.090605735778809\n",
      "Epoch 2/100, Step 211, Loss: 5.341547012329102\n",
      "Epoch 2/100, Step 221, Loss: 5.328049182891846\n",
      "Epoch 2/100, Step 231, Loss: 5.577505588531494\n",
      "Epoch 2/100, Step 241, Loss: 5.265364646911621\n",
      "Epoch 2/100, Step 251, Loss: 5.552571773529053\n",
      "Epoch 2/100, Step 261, Loss: 5.7866435050964355\n",
      "Epoch 2/100, Step 271, Loss: 5.087677001953125\n",
      "Epoch 2/100, Step 281, Loss: 5.502481937408447\n",
      "Epoch 2/100, Step 291, Loss: 4.928281307220459\n",
      "Epoch 2/100, Step 301, Loss: 5.128214359283447\n",
      "Epoch 2/100, Step 311, Loss: 4.686596870422363\n",
      "Epoch 2/100, Step 321, Loss: 4.729414939880371\n",
      "Epoch 2/100, Step 331, Loss: 5.162606716156006\n",
      "Epoch 2/100, Step 341, Loss: 5.263420581817627\n",
      "Epoch 2/100, Step 351, Loss: 5.093918323516846\n",
      "Epoch 2/100, Step 361, Loss: 5.53324031829834\n",
      "Epoch 3/100, Step 1, Loss: 4.1993184089660645\n",
      "Epoch 3/100, Step 11, Loss: 5.301671981811523\n",
      "Epoch 3/100, Step 21, Loss: 4.730996608734131\n",
      "Epoch 3/100, Step 31, Loss: 5.098052501678467\n",
      "Epoch 3/100, Step 41, Loss: 4.376736164093018\n",
      "Epoch 3/100, Step 51, Loss: 5.307150840759277\n",
      "Epoch 3/100, Step 61, Loss: 4.880712509155273\n",
      "Epoch 3/100, Step 71, Loss: 4.609705448150635\n",
      "Epoch 3/100, Step 81, Loss: 5.908968448638916\n",
      "Epoch 3/100, Step 91, Loss: 5.036813259124756\n",
      "Epoch 3/100, Step 101, Loss: 4.871626377105713\n",
      "Epoch 3/100, Step 111, Loss: 4.782374858856201\n",
      "Epoch 3/100, Step 121, Loss: 5.1681294441223145\n",
      "Epoch 3/100, Step 131, Loss: 4.747620582580566\n",
      "Epoch 3/100, Step 141, Loss: 4.540958881378174\n",
      "Epoch 3/100, Step 151, Loss: 4.899035930633545\n",
      "Epoch 3/100, Step 161, Loss: 5.210146903991699\n",
      "Epoch 3/100, Step 171, Loss: 4.741146087646484\n",
      "Epoch 3/100, Step 181, Loss: 4.978531837463379\n",
      "Epoch 3/100, Step 191, Loss: 4.499251842498779\n",
      "Epoch 3/100, Step 201, Loss: 4.70235013961792\n",
      "Epoch 3/100, Step 211, Loss: 4.207651615142822\n",
      "Epoch 3/100, Step 221, Loss: 4.68261194229126\n",
      "Epoch 3/100, Step 231, Loss: 5.23382568359375\n",
      "Epoch 3/100, Step 241, Loss: 4.95697546005249\n",
      "Epoch 3/100, Step 251, Loss: 4.866579055786133\n",
      "Epoch 3/100, Step 261, Loss: 5.407793998718262\n",
      "Epoch 3/100, Step 271, Loss: 5.066469669342041\n",
      "Epoch 3/100, Step 281, Loss: 5.579567909240723\n",
      "Epoch 3/100, Step 291, Loss: 4.63239049911499\n",
      "Epoch 3/100, Step 301, Loss: 5.363544940948486\n",
      "Epoch 3/100, Step 311, Loss: 4.439601898193359\n",
      "Epoch 3/100, Step 321, Loss: 5.244042873382568\n",
      "Epoch 3/100, Step 331, Loss: 5.2364301681518555\n",
      "Epoch 3/100, Step 341, Loss: 4.519794464111328\n",
      "Epoch 3/100, Step 351, Loss: 4.813244342803955\n",
      "Epoch 3/100, Step 361, Loss: 4.978945255279541\n",
      "Epoch 4/100, Step 1, Loss: 4.809230804443359\n",
      "Epoch 4/100, Step 11, Loss: 4.569395542144775\n",
      "Epoch 4/100, Step 21, Loss: 4.898533821105957\n",
      "Epoch 4/100, Step 31, Loss: 4.6419854164123535\n",
      "Epoch 4/100, Step 41, Loss: 4.607354640960693\n",
      "Epoch 4/100, Step 51, Loss: 4.921173572540283\n",
      "Epoch 4/100, Step 61, Loss: 5.243645191192627\n",
      "Epoch 4/100, Step 71, Loss: 4.49621057510376\n",
      "Epoch 4/100, Step 81, Loss: 4.069009780883789\n",
      "Epoch 4/100, Step 91, Loss: 4.729138374328613\n",
      "Epoch 4/100, Step 101, Loss: 4.732841491699219\n",
      "Epoch 4/100, Step 111, Loss: 3.6876420974731445\n",
      "Epoch 4/100, Step 121, Loss: 5.7005181312561035\n",
      "Epoch 4/100, Step 131, Loss: 4.677579879760742\n",
      "Epoch 4/100, Step 141, Loss: 5.280691623687744\n",
      "Epoch 4/100, Step 151, Loss: 4.773056507110596\n",
      "Epoch 4/100, Step 161, Loss: 5.257733345031738\n",
      "Epoch 4/100, Step 171, Loss: 5.0901079177856445\n",
      "Epoch 4/100, Step 181, Loss: 4.94799280166626\n",
      "Epoch 4/100, Step 191, Loss: 5.089216232299805\n",
      "Epoch 4/100, Step 201, Loss: 5.195864200592041\n",
      "Epoch 4/100, Step 211, Loss: 4.831666469573975\n",
      "Epoch 4/100, Step 221, Loss: 4.95399284362793\n",
      "Epoch 4/100, Step 231, Loss: 4.806179046630859\n",
      "Epoch 4/100, Step 241, Loss: 4.819840431213379\n",
      "Epoch 4/100, Step 251, Loss: 5.19761848449707\n",
      "Epoch 4/100, Step 261, Loss: 4.270915985107422\n",
      "Epoch 4/100, Step 271, Loss: 4.926563262939453\n",
      "Epoch 4/100, Step 281, Loss: 4.8760247230529785\n",
      "Epoch 4/100, Step 291, Loss: 4.346729755401611\n",
      "Epoch 4/100, Step 301, Loss: 4.967733860015869\n",
      "Epoch 4/100, Step 311, Loss: 5.219913959503174\n",
      "Epoch 4/100, Step 321, Loss: 5.344686985015869\n",
      "Epoch 4/100, Step 331, Loss: 4.927364826202393\n",
      "Epoch 4/100, Step 341, Loss: 4.322894096374512\n",
      "Epoch 4/100, Step 351, Loss: 4.750778675079346\n",
      "Epoch 4/100, Step 361, Loss: 4.837092399597168\n",
      "Epoch 5/100, Step 1, Loss: 4.101431369781494\n",
      "Epoch 5/100, Step 11, Loss: 4.063177585601807\n",
      "Epoch 5/100, Step 21, Loss: 5.054142951965332\n",
      "Epoch 5/100, Step 31, Loss: 4.425553321838379\n",
      "Epoch 5/100, Step 41, Loss: 5.306230068206787\n",
      "Epoch 5/100, Step 51, Loss: 4.612253189086914\n",
      "Epoch 5/100, Step 61, Loss: 5.299759387969971\n",
      "Epoch 5/100, Step 71, Loss: 3.8233184814453125\n",
      "Epoch 5/100, Step 81, Loss: 4.898781776428223\n",
      "Epoch 5/100, Step 91, Loss: 4.726344585418701\n",
      "Epoch 5/100, Step 101, Loss: 4.884463787078857\n",
      "Epoch 5/100, Step 111, Loss: 3.9156854152679443\n",
      "Epoch 5/100, Step 121, Loss: 4.821310520172119\n",
      "Epoch 5/100, Step 131, Loss: 4.381804943084717\n",
      "Epoch 5/100, Step 141, Loss: 4.750556468963623\n",
      "Epoch 5/100, Step 151, Loss: 4.435657024383545\n",
      "Epoch 5/100, Step 161, Loss: 3.9138541221618652\n",
      "Epoch 5/100, Step 171, Loss: 4.795436859130859\n",
      "Epoch 5/100, Step 181, Loss: 4.675665855407715\n",
      "Epoch 5/100, Step 191, Loss: 5.083338737487793\n",
      "Epoch 5/100, Step 201, Loss: 4.696212291717529\n",
      "Epoch 5/100, Step 211, Loss: 4.336908340454102\n",
      "Epoch 5/100, Step 221, Loss: 4.894226551055908\n",
      "Epoch 5/100, Step 231, Loss: 4.249969482421875\n",
      "Epoch 5/100, Step 241, Loss: 4.437417507171631\n",
      "Epoch 5/100, Step 251, Loss: 4.225936412811279\n",
      "Epoch 5/100, Step 261, Loss: 3.903745174407959\n",
      "Epoch 5/100, Step 271, Loss: 5.161539077758789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Step 281, Loss: 4.439336776733398\n",
      "Epoch 5/100, Step 291, Loss: 4.118088245391846\n",
      "Epoch 5/100, Step 301, Loss: 4.35374641418457\n",
      "Epoch 5/100, Step 311, Loss: 4.140695095062256\n",
      "Epoch 5/100, Step 321, Loss: 4.83750581741333\n",
      "Epoch 5/100, Step 331, Loss: 4.868332862854004\n",
      "Epoch 5/100, Step 341, Loss: 4.352811813354492\n",
      "Epoch 5/100, Step 351, Loss: 5.0759077072143555\n",
      "Epoch 5/100, Step 361, Loss: 4.855072975158691\n",
      "Epoch 6/100, Step 1, Loss: 3.718043804168701\n",
      "Epoch 6/100, Step 11, Loss: 4.647392272949219\n",
      "Epoch 6/100, Step 21, Loss: 4.5905632972717285\n",
      "Epoch 6/100, Step 31, Loss: 4.913887977600098\n",
      "Epoch 6/100, Step 41, Loss: 4.607448101043701\n",
      "Epoch 6/100, Step 51, Loss: 4.435571193695068\n",
      "Epoch 6/100, Step 61, Loss: 4.495782375335693\n",
      "Epoch 6/100, Step 71, Loss: 4.603936195373535\n",
      "Epoch 6/100, Step 81, Loss: 4.718899250030518\n",
      "Epoch 6/100, Step 91, Loss: 4.880528450012207\n",
      "Epoch 6/100, Step 101, Loss: 4.410231590270996\n",
      "Epoch 6/100, Step 111, Loss: 4.568432807922363\n",
      "Epoch 6/100, Step 121, Loss: 4.427426338195801\n",
      "Epoch 6/100, Step 131, Loss: 4.321441173553467\n",
      "Epoch 6/100, Step 141, Loss: 4.3047709465026855\n",
      "Epoch 6/100, Step 151, Loss: 5.109753608703613\n",
      "Epoch 6/100, Step 161, Loss: 3.845029830932617\n",
      "Epoch 6/100, Step 171, Loss: 4.210056781768799\n",
      "Epoch 6/100, Step 181, Loss: 4.044510841369629\n",
      "Epoch 6/100, Step 191, Loss: 4.894977569580078\n",
      "Epoch 6/100, Step 201, Loss: 4.158577919006348\n",
      "Epoch 6/100, Step 211, Loss: 4.273056507110596\n",
      "Epoch 6/100, Step 221, Loss: 4.0805277824401855\n",
      "Epoch 6/100, Step 231, Loss: 4.565853595733643\n",
      "Epoch 6/100, Step 241, Loss: 3.7403626441955566\n",
      "Epoch 6/100, Step 251, Loss: 5.0988311767578125\n",
      "Epoch 6/100, Step 261, Loss: 4.516642093658447\n",
      "Epoch 6/100, Step 271, Loss: 4.769801139831543\n",
      "Epoch 6/100, Step 281, Loss: 4.661075115203857\n",
      "Epoch 6/100, Step 291, Loss: 3.613255739212036\n",
      "Epoch 6/100, Step 301, Loss: 4.455515384674072\n",
      "Epoch 6/100, Step 311, Loss: 4.526405334472656\n",
      "Epoch 6/100, Step 321, Loss: 4.403425216674805\n",
      "Epoch 6/100, Step 331, Loss: 4.702088832855225\n",
      "Epoch 6/100, Step 341, Loss: 4.357001304626465\n",
      "Epoch 6/100, Step 351, Loss: 4.319244384765625\n",
      "Epoch 6/100, Step 361, Loss: 4.361114501953125\n",
      "Epoch 7/100, Step 1, Loss: 3.523169755935669\n",
      "Epoch 7/100, Step 11, Loss: 3.9405863285064697\n",
      "Epoch 7/100, Step 21, Loss: 4.1885905265808105\n",
      "Epoch 7/100, Step 31, Loss: 3.901059865951538\n",
      "Epoch 7/100, Step 41, Loss: 4.608697414398193\n",
      "Epoch 7/100, Step 51, Loss: 3.7892777919769287\n",
      "Epoch 7/100, Step 61, Loss: 4.670938968658447\n",
      "Epoch 7/100, Step 71, Loss: 4.258362770080566\n",
      "Epoch 7/100, Step 81, Loss: 4.425194263458252\n",
      "Epoch 7/100, Step 91, Loss: 4.63541316986084\n",
      "Epoch 7/100, Step 101, Loss: 4.60938024520874\n",
      "Epoch 7/100, Step 111, Loss: 4.588131904602051\n",
      "Epoch 7/100, Step 121, Loss: 4.133420944213867\n",
      "Epoch 7/100, Step 131, Loss: 4.950688362121582\n",
      "Epoch 7/100, Step 141, Loss: 4.496890068054199\n",
      "Epoch 7/100, Step 151, Loss: 5.083435535430908\n",
      "Epoch 7/100, Step 161, Loss: 5.008585453033447\n",
      "Epoch 7/100, Step 171, Loss: 4.634387493133545\n",
      "Epoch 7/100, Step 181, Loss: 4.565981388092041\n",
      "Epoch 7/100, Step 191, Loss: 4.702277660369873\n",
      "Epoch 7/100, Step 201, Loss: 4.103416919708252\n",
      "Epoch 7/100, Step 211, Loss: 4.0830559730529785\n",
      "Epoch 7/100, Step 221, Loss: 3.7990927696228027\n",
      "Epoch 7/100, Step 231, Loss: 4.786752223968506\n",
      "Epoch 7/100, Step 241, Loss: 3.878056526184082\n",
      "Epoch 7/100, Step 251, Loss: 4.1378655433654785\n",
      "Epoch 7/100, Step 261, Loss: 4.757360935211182\n",
      "Epoch 7/100, Step 271, Loss: 4.802494049072266\n",
      "Epoch 7/100, Step 281, Loss: 3.8731796741485596\n",
      "Epoch 7/100, Step 291, Loss: 4.145211696624756\n",
      "Epoch 7/100, Step 301, Loss: 5.2189459800720215\n",
      "Epoch 7/100, Step 311, Loss: 4.356893062591553\n",
      "Epoch 7/100, Step 321, Loss: 4.320997714996338\n",
      "Epoch 7/100, Step 331, Loss: 3.8523221015930176\n",
      "Epoch 7/100, Step 341, Loss: 3.8051271438598633\n",
      "Epoch 7/100, Step 351, Loss: 3.8015480041503906\n",
      "Epoch 7/100, Step 361, Loss: 4.649749279022217\n",
      "Epoch 8/100, Step 1, Loss: 4.637093544006348\n",
      "Epoch 8/100, Step 11, Loss: 4.187703609466553\n",
      "Epoch 8/100, Step 21, Loss: 4.544025421142578\n",
      "Epoch 8/100, Step 31, Loss: 3.9763693809509277\n",
      "Epoch 8/100, Step 41, Loss: 4.610354423522949\n",
      "Epoch 8/100, Step 51, Loss: 4.593430042266846\n",
      "Epoch 8/100, Step 61, Loss: 4.1092753410339355\n",
      "Epoch 8/100, Step 71, Loss: 4.376349449157715\n",
      "Epoch 8/100, Step 81, Loss: 4.291441440582275\n",
      "Epoch 8/100, Step 91, Loss: 4.378619194030762\n",
      "Epoch 8/100, Step 101, Loss: 4.135008335113525\n",
      "Epoch 8/100, Step 111, Loss: 4.3876633644104\n",
      "Epoch 8/100, Step 121, Loss: 4.564032554626465\n",
      "Epoch 8/100, Step 131, Loss: 4.087646961212158\n",
      "Epoch 8/100, Step 141, Loss: 3.725027322769165\n",
      "Epoch 8/100, Step 151, Loss: 4.38031005859375\n",
      "Epoch 8/100, Step 161, Loss: 3.8956172466278076\n",
      "Epoch 8/100, Step 171, Loss: 4.385908126831055\n",
      "Epoch 8/100, Step 181, Loss: 3.617825984954834\n",
      "Epoch 8/100, Step 191, Loss: 3.2793049812316895\n",
      "Epoch 8/100, Step 201, Loss: 4.066091537475586\n",
      "Epoch 8/100, Step 211, Loss: 4.958313465118408\n",
      "Epoch 8/100, Step 221, Loss: 4.464653015136719\n",
      "Epoch 8/100, Step 231, Loss: 4.151910305023193\n",
      "Epoch 8/100, Step 241, Loss: 4.587299346923828\n",
      "Epoch 8/100, Step 251, Loss: 4.325024604797363\n",
      "Epoch 8/100, Step 261, Loss: 3.722223997116089\n",
      "Epoch 8/100, Step 271, Loss: 4.3464860916137695\n",
      "Epoch 8/100, Step 281, Loss: 4.702166557312012\n",
      "Epoch 8/100, Step 291, Loss: 3.5421664714813232\n",
      "Epoch 8/100, Step 301, Loss: 4.331137657165527\n",
      "Epoch 8/100, Step 311, Loss: 3.464310884475708\n",
      "Epoch 8/100, Step 321, Loss: 4.220139503479004\n",
      "Epoch 8/100, Step 331, Loss: 4.195606708526611\n",
      "Epoch 8/100, Step 341, Loss: 2.9857680797576904\n",
      "Epoch 8/100, Step 351, Loss: 4.638102054595947\n",
      "Epoch 8/100, Step 361, Loss: 4.4523606300354\n",
      "Epoch 9/100, Step 1, Loss: 3.9858386516571045\n",
      "Epoch 9/100, Step 11, Loss: 4.5809502601623535\n",
      "Epoch 9/100, Step 21, Loss: 3.9457499980926514\n",
      "Epoch 9/100, Step 31, Loss: 4.319197654724121\n",
      "Epoch 9/100, Step 41, Loss: 3.8160345554351807\n",
      "Epoch 9/100, Step 51, Loss: 4.238317489624023\n",
      "Epoch 9/100, Step 61, Loss: 4.125452041625977\n",
      "Epoch 9/100, Step 71, Loss: 3.674814462661743\n",
      "Epoch 9/100, Step 81, Loss: 2.8729922771453857\n",
      "Epoch 9/100, Step 91, Loss: 4.195268630981445\n",
      "Epoch 9/100, Step 101, Loss: 3.8465473651885986\n",
      "Epoch 9/100, Step 111, Loss: 4.4952592849731445\n",
      "Epoch 9/100, Step 121, Loss: 4.919950485229492\n",
      "Epoch 9/100, Step 131, Loss: 4.5326690673828125\n",
      "Epoch 9/100, Step 141, Loss: 4.35100793838501\n",
      "Epoch 9/100, Step 151, Loss: 3.9178431034088135\n",
      "Epoch 9/100, Step 161, Loss: 3.8022751808166504\n",
      "Epoch 9/100, Step 171, Loss: 4.24356746673584\n",
      "Epoch 9/100, Step 181, Loss: 4.395083904266357\n",
      "Epoch 9/100, Step 191, Loss: 4.534512996673584\n",
      "Epoch 9/100, Step 201, Loss: 3.786381721496582\n",
      "Epoch 9/100, Step 211, Loss: 4.097650527954102\n",
      "Epoch 9/100, Step 221, Loss: 4.362022399902344\n",
      "Epoch 9/100, Step 231, Loss: 4.375833988189697\n",
      "Epoch 9/100, Step 241, Loss: 3.8733744621276855\n",
      "Epoch 9/100, Step 251, Loss: 4.0633134841918945\n",
      "Epoch 9/100, Step 261, Loss: 4.1652374267578125\n",
      "Epoch 9/100, Step 271, Loss: 3.472074031829834\n",
      "Epoch 9/100, Step 281, Loss: 4.422246932983398\n",
      "Epoch 9/100, Step 291, Loss: 3.9899916648864746\n",
      "Epoch 9/100, Step 301, Loss: 3.536857843399048\n",
      "Epoch 9/100, Step 311, Loss: 4.201085090637207\n",
      "Epoch 9/100, Step 321, Loss: 3.853654384613037\n",
      "Epoch 9/100, Step 331, Loss: 3.9024901390075684\n",
      "Epoch 9/100, Step 341, Loss: 3.5218310356140137\n",
      "Epoch 9/100, Step 351, Loss: 4.436766147613525\n",
      "Epoch 9/100, Step 361, Loss: 4.116951942443848\n",
      "Epoch 10/100, Step 1, Loss: 3.897465944290161\n",
      "Epoch 10/100, Step 11, Loss: 3.6444883346557617\n",
      "Epoch 10/100, Step 21, Loss: 3.949958086013794\n",
      "Epoch 10/100, Step 31, Loss: 4.429026126861572\n",
      "Epoch 10/100, Step 41, Loss: 3.7719433307647705\n",
      "Epoch 10/100, Step 51, Loss: 4.339053630828857\n",
      "Epoch 10/100, Step 61, Loss: 3.7235116958618164\n",
      "Epoch 10/100, Step 71, Loss: 4.045890808105469\n",
      "Epoch 10/100, Step 81, Loss: 4.239907741546631\n",
      "Epoch 10/100, Step 91, Loss: 4.394809722900391\n",
      "Epoch 10/100, Step 101, Loss: 3.914940357208252\n",
      "Epoch 10/100, Step 111, Loss: 3.8118598461151123\n",
      "Epoch 10/100, Step 121, Loss: 3.640254020690918\n",
      "Epoch 10/100, Step 131, Loss: 3.40731143951416\n",
      "Epoch 10/100, Step 141, Loss: 3.586251974105835\n",
      "Epoch 10/100, Step 151, Loss: 4.198527812957764\n",
      "Epoch 10/100, Step 161, Loss: 4.3951005935668945\n",
      "Epoch 10/100, Step 171, Loss: 3.840935230255127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Step 181, Loss: 3.433279514312744\n",
      "Epoch 10/100, Step 191, Loss: 4.651848316192627\n",
      "Epoch 10/100, Step 201, Loss: 4.2104105949401855\n",
      "Epoch 10/100, Step 211, Loss: 3.5983073711395264\n",
      "Epoch 10/100, Step 221, Loss: 3.4761078357696533\n",
      "Epoch 10/100, Step 231, Loss: 3.55800724029541\n",
      "Epoch 10/100, Step 241, Loss: 3.6871140003204346\n",
      "Epoch 10/100, Step 251, Loss: 4.039853572845459\n",
      "Epoch 10/100, Step 261, Loss: 3.794186592102051\n",
      "Epoch 10/100, Step 271, Loss: 3.5654118061065674\n",
      "Epoch 10/100, Step 281, Loss: 3.6807358264923096\n",
      "Epoch 10/100, Step 291, Loss: 4.493956089019775\n",
      "Epoch 10/100, Step 301, Loss: 3.8432729244232178\n",
      "Epoch 10/100, Step 311, Loss: 4.189023017883301\n",
      "Epoch 10/100, Step 321, Loss: 3.8341801166534424\n",
      "Epoch 10/100, Step 331, Loss: 3.4202959537506104\n",
      "Epoch 10/100, Step 341, Loss: 3.429445743560791\n",
      "Epoch 10/100, Step 351, Loss: 3.994781732559204\n",
      "Epoch 10/100, Step 361, Loss: 3.885603189468384\n",
      "Epoch 11/100, Step 1, Loss: 4.19248104095459\n",
      "Epoch 11/100, Step 11, Loss: 4.004110813140869\n",
      "Epoch 11/100, Step 21, Loss: 4.184117317199707\n",
      "Epoch 11/100, Step 31, Loss: 3.9713001251220703\n",
      "Epoch 11/100, Step 41, Loss: 3.8616888523101807\n",
      "Epoch 11/100, Step 51, Loss: 3.580901861190796\n",
      "Epoch 11/100, Step 61, Loss: 3.7424569129943848\n",
      "Epoch 11/100, Step 71, Loss: 3.370441436767578\n",
      "Epoch 11/100, Step 81, Loss: 3.95025372505188\n",
      "Epoch 11/100, Step 91, Loss: 3.639167547225952\n",
      "Epoch 11/100, Step 101, Loss: 3.6508989334106445\n",
      "Epoch 11/100, Step 111, Loss: 3.342064619064331\n",
      "Epoch 11/100, Step 121, Loss: 3.955737829208374\n",
      "Epoch 11/100, Step 131, Loss: 3.242431640625\n",
      "Epoch 11/100, Step 141, Loss: 3.0284247398376465\n",
      "Epoch 11/100, Step 151, Loss: 4.093209743499756\n",
      "Epoch 11/100, Step 161, Loss: 4.3027663230896\n",
      "Epoch 11/100, Step 171, Loss: 3.8464317321777344\n",
      "Epoch 11/100, Step 181, Loss: 3.5182104110717773\n",
      "Epoch 11/100, Step 191, Loss: 2.9933531284332275\n",
      "Epoch 11/100, Step 201, Loss: 3.675374746322632\n",
      "Epoch 11/100, Step 211, Loss: 4.443735599517822\n",
      "Epoch 11/100, Step 221, Loss: 3.4485371112823486\n",
      "Epoch 11/100, Step 231, Loss: 3.7049779891967773\n",
      "Epoch 11/100, Step 241, Loss: 4.211283206939697\n",
      "Epoch 11/100, Step 251, Loss: 4.31885290145874\n",
      "Epoch 11/100, Step 261, Loss: 3.509512424468994\n",
      "Epoch 11/100, Step 271, Loss: 3.8920161724090576\n",
      "Epoch 11/100, Step 281, Loss: 4.262139320373535\n",
      "Epoch 11/100, Step 291, Loss: 3.7594642639160156\n",
      "Epoch 11/100, Step 301, Loss: 3.8572890758514404\n",
      "Epoch 11/100, Step 311, Loss: 3.7551865577697754\n",
      "Epoch 11/100, Step 321, Loss: 3.670628547668457\n",
      "Epoch 11/100, Step 331, Loss: 3.930007219314575\n",
      "Epoch 11/100, Step 341, Loss: 3.6072282791137695\n",
      "Epoch 11/100, Step 351, Loss: 3.8373613357543945\n",
      "Epoch 11/100, Step 361, Loss: 4.0896430015563965\n",
      "Epoch 12/100, Step 1, Loss: 3.9422247409820557\n",
      "Epoch 12/100, Step 11, Loss: 3.49898624420166\n",
      "Epoch 12/100, Step 21, Loss: 3.9282827377319336\n",
      "Epoch 12/100, Step 31, Loss: 3.412567138671875\n",
      "Epoch 12/100, Step 41, Loss: 3.361802339553833\n",
      "Epoch 12/100, Step 51, Loss: 4.094547271728516\n",
      "Epoch 12/100, Step 61, Loss: 3.6272270679473877\n",
      "Epoch 12/100, Step 71, Loss: 3.8228518962860107\n",
      "Epoch 12/100, Step 81, Loss: 3.2824599742889404\n",
      "Epoch 12/100, Step 91, Loss: 3.414013385772705\n",
      "Epoch 12/100, Step 101, Loss: 3.828357219696045\n",
      "Epoch 12/100, Step 111, Loss: 4.7724289894104\n",
      "Epoch 12/100, Step 121, Loss: 3.5796689987182617\n",
      "Epoch 12/100, Step 131, Loss: 3.5050530433654785\n",
      "Epoch 12/100, Step 141, Loss: 3.3925411701202393\n",
      "Epoch 12/100, Step 151, Loss: 3.683351516723633\n",
      "Epoch 12/100, Step 161, Loss: 3.2436814308166504\n",
      "Epoch 12/100, Step 171, Loss: 3.4506075382232666\n",
      "Epoch 12/100, Step 181, Loss: 4.2380595207214355\n",
      "Epoch 12/100, Step 191, Loss: 3.7285661697387695\n",
      "Epoch 12/100, Step 201, Loss: 3.1158149242401123\n",
      "Epoch 12/100, Step 211, Loss: 3.192678451538086\n",
      "Epoch 12/100, Step 221, Loss: 3.7230701446533203\n",
      "Epoch 12/100, Step 231, Loss: 4.051202297210693\n",
      "Epoch 12/100, Step 241, Loss: 3.849107503890991\n",
      "Epoch 12/100, Step 251, Loss: 3.139747142791748\n",
      "Epoch 12/100, Step 261, Loss: 3.3232603073120117\n",
      "Epoch 12/100, Step 271, Loss: 2.807342767715454\n",
      "Epoch 12/100, Step 281, Loss: 3.6655919551849365\n",
      "Epoch 12/100, Step 291, Loss: 2.8474199771881104\n",
      "Epoch 12/100, Step 301, Loss: 2.9208168983459473\n",
      "Epoch 12/100, Step 311, Loss: 3.543546676635742\n",
      "Epoch 12/100, Step 321, Loss: 3.2347004413604736\n",
      "Epoch 12/100, Step 331, Loss: 3.9170279502868652\n",
      "Epoch 12/100, Step 341, Loss: 3.4679722785949707\n",
      "Epoch 12/100, Step 351, Loss: 4.075525760650635\n",
      "Epoch 12/100, Step 361, Loss: 3.6753854751586914\n",
      "Epoch 13/100, Step 1, Loss: 3.5530073642730713\n",
      "Epoch 13/100, Step 11, Loss: 3.0254452228546143\n",
      "Epoch 13/100, Step 21, Loss: 3.7714316844940186\n",
      "Epoch 13/100, Step 31, Loss: 3.282212257385254\n",
      "Epoch 13/100, Step 41, Loss: 3.685260057449341\n",
      "Epoch 13/100, Step 51, Loss: 3.5051209926605225\n",
      "Epoch 13/100, Step 61, Loss: 3.7664783000946045\n",
      "Epoch 13/100, Step 71, Loss: 3.7555062770843506\n",
      "Epoch 13/100, Step 81, Loss: 2.9106438159942627\n",
      "Epoch 13/100, Step 91, Loss: 3.4962217807769775\n",
      "Epoch 13/100, Step 101, Loss: 3.2808938026428223\n",
      "Epoch 13/100, Step 111, Loss: 3.494563341140747\n",
      "Epoch 13/100, Step 121, Loss: 3.433255434036255\n",
      "Epoch 13/100, Step 131, Loss: 2.5493085384368896\n",
      "Epoch 13/100, Step 141, Loss: 3.530043840408325\n",
      "Epoch 13/100, Step 151, Loss: 3.9293620586395264\n",
      "Epoch 13/100, Step 161, Loss: 4.090294361114502\n",
      "Epoch 13/100, Step 171, Loss: 3.7018957138061523\n",
      "Epoch 13/100, Step 181, Loss: 3.4439640045166016\n",
      "Epoch 13/100, Step 191, Loss: 3.2519803047180176\n",
      "Epoch 13/100, Step 201, Loss: 2.4723904132843018\n",
      "Epoch 13/100, Step 211, Loss: 2.6179721355438232\n",
      "Epoch 13/100, Step 221, Loss: 2.9223437309265137\n",
      "Epoch 13/100, Step 231, Loss: 3.5072669982910156\n",
      "Epoch 13/100, Step 241, Loss: 3.5404183864593506\n",
      "Epoch 13/100, Step 251, Loss: 3.3592851161956787\n",
      "Epoch 13/100, Step 261, Loss: 4.204475402832031\n",
      "Epoch 13/100, Step 271, Loss: 4.1000237464904785\n",
      "Epoch 13/100, Step 281, Loss: 3.279660940170288\n",
      "Epoch 13/100, Step 291, Loss: 4.151234149932861\n",
      "Epoch 13/100, Step 301, Loss: 3.434689521789551\n",
      "Epoch 13/100, Step 311, Loss: 3.1185944080352783\n",
      "Epoch 13/100, Step 321, Loss: 4.054337978363037\n",
      "Epoch 13/100, Step 331, Loss: 3.5984137058258057\n",
      "Epoch 13/100, Step 341, Loss: 4.0918755531311035\n",
      "Epoch 13/100, Step 351, Loss: 3.8221700191497803\n",
      "Epoch 13/100, Step 361, Loss: 3.786940336227417\n",
      "Epoch 14/100, Step 1, Loss: 3.445463180541992\n",
      "Epoch 14/100, Step 11, Loss: 3.6259236335754395\n",
      "Epoch 14/100, Step 21, Loss: 4.677736759185791\n",
      "Epoch 14/100, Step 31, Loss: 3.23742938041687\n",
      "Epoch 14/100, Step 41, Loss: 3.5306074619293213\n",
      "Epoch 14/100, Step 51, Loss: 3.672565460205078\n",
      "Epoch 14/100, Step 61, Loss: 2.675528049468994\n",
      "Epoch 14/100, Step 71, Loss: 2.5159380435943604\n",
      "Epoch 14/100, Step 81, Loss: 2.969674825668335\n",
      "Epoch 14/100, Step 91, Loss: 3.048037528991699\n",
      "Epoch 14/100, Step 101, Loss: 2.9452056884765625\n",
      "Epoch 14/100, Step 111, Loss: 3.7738521099090576\n",
      "Epoch 14/100, Step 121, Loss: 3.5350148677825928\n",
      "Epoch 14/100, Step 131, Loss: 3.062790870666504\n",
      "Epoch 14/100, Step 141, Loss: 3.5097575187683105\n",
      "Epoch 14/100, Step 151, Loss: 3.571915626525879\n",
      "Epoch 14/100, Step 161, Loss: 3.2062466144561768\n",
      "Epoch 14/100, Step 171, Loss: 3.495957851409912\n",
      "Epoch 14/100, Step 181, Loss: 3.0553624629974365\n",
      "Epoch 14/100, Step 191, Loss: 3.247626781463623\n",
      "Epoch 14/100, Step 201, Loss: 3.6567797660827637\n",
      "Epoch 14/100, Step 211, Loss: 3.0024495124816895\n",
      "Epoch 14/100, Step 221, Loss: 2.967559576034546\n",
      "Epoch 14/100, Step 231, Loss: 3.461982250213623\n",
      "Epoch 14/100, Step 241, Loss: 3.6615548133850098\n",
      "Epoch 14/100, Step 251, Loss: 2.897303342819214\n",
      "Epoch 14/100, Step 261, Loss: 4.111161231994629\n",
      "Epoch 14/100, Step 271, Loss: 2.86198353767395\n",
      "Epoch 14/100, Step 281, Loss: 3.8096091747283936\n",
      "Epoch 14/100, Step 291, Loss: 3.752406358718872\n",
      "Epoch 14/100, Step 301, Loss: 3.1458675861358643\n",
      "Epoch 14/100, Step 311, Loss: 3.16715145111084\n",
      "Epoch 14/100, Step 321, Loss: 2.3679208755493164\n",
      "Epoch 14/100, Step 331, Loss: 3.4144952297210693\n",
      "Epoch 14/100, Step 341, Loss: 3.071492910385132\n",
      "Epoch 14/100, Step 351, Loss: 3.750880479812622\n",
      "Epoch 14/100, Step 361, Loss: 3.8274402618408203\n",
      "Epoch 15/100, Step 1, Loss: 3.493011474609375\n",
      "Epoch 15/100, Step 11, Loss: 2.5326290130615234\n",
      "Epoch 15/100, Step 21, Loss: 3.231536626815796\n",
      "Epoch 15/100, Step 31, Loss: 2.831141710281372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Step 41, Loss: 3.7060956954956055\n",
      "Epoch 15/100, Step 51, Loss: 3.558755397796631\n",
      "Epoch 15/100, Step 61, Loss: 2.9089372158050537\n",
      "Epoch 15/100, Step 71, Loss: 3.4249231815338135\n",
      "Epoch 15/100, Step 81, Loss: 3.2281486988067627\n",
      "Epoch 15/100, Step 91, Loss: 3.5786423683166504\n",
      "Epoch 15/100, Step 101, Loss: 3.23661208152771\n",
      "Epoch 15/100, Step 111, Loss: 2.9900150299072266\n",
      "Epoch 15/100, Step 121, Loss: 2.8833954334259033\n",
      "Epoch 15/100, Step 131, Loss: 2.7528951168060303\n",
      "Epoch 15/100, Step 141, Loss: 2.829198122024536\n",
      "Epoch 15/100, Step 151, Loss: 3.0560312271118164\n",
      "Epoch 15/100, Step 161, Loss: 2.23795747756958\n",
      "Epoch 15/100, Step 171, Loss: 3.145447254180908\n",
      "Epoch 15/100, Step 181, Loss: 3.632474184036255\n",
      "Epoch 15/100, Step 191, Loss: 3.937760829925537\n",
      "Epoch 15/100, Step 201, Loss: 2.9670252799987793\n",
      "Epoch 15/100, Step 211, Loss: 3.1272683143615723\n",
      "Epoch 15/100, Step 221, Loss: 3.7986674308776855\n",
      "Epoch 15/100, Step 231, Loss: 2.9441683292388916\n",
      "Epoch 15/100, Step 241, Loss: 3.7546679973602295\n",
      "Epoch 15/100, Step 251, Loss: 2.5495316982269287\n",
      "Epoch 15/100, Step 261, Loss: 3.1262834072113037\n",
      "Epoch 15/100, Step 271, Loss: 2.768948793411255\n",
      "Epoch 15/100, Step 281, Loss: 1.6646250486373901\n",
      "Epoch 15/100, Step 291, Loss: 2.741715908050537\n",
      "Epoch 15/100, Step 301, Loss: 3.3496487140655518\n",
      "Epoch 15/100, Step 311, Loss: 3.651747703552246\n",
      "Epoch 15/100, Step 321, Loss: 3.3877735137939453\n",
      "Epoch 15/100, Step 331, Loss: 3.680105209350586\n",
      "Epoch 15/100, Step 341, Loss: 3.2930946350097656\n",
      "Epoch 15/100, Step 351, Loss: 3.760165214538574\n",
      "Epoch 15/100, Step 361, Loss: 3.7077929973602295\n",
      "Epoch 16/100, Step 1, Loss: 3.242649555206299\n",
      "Epoch 16/100, Step 11, Loss: 3.1750216484069824\n",
      "Epoch 16/100, Step 21, Loss: 3.6980180740356445\n",
      "Epoch 16/100, Step 31, Loss: 3.5724117755889893\n",
      "Epoch 16/100, Step 41, Loss: 2.574897527694702\n",
      "Epoch 16/100, Step 51, Loss: 2.9732162952423096\n",
      "Epoch 16/100, Step 61, Loss: 3.188232660293579\n",
      "Epoch 16/100, Step 71, Loss: 3.9675118923187256\n",
      "Epoch 16/100, Step 81, Loss: 3.4260289669036865\n",
      "Epoch 16/100, Step 91, Loss: 2.9503486156463623\n",
      "Epoch 16/100, Step 101, Loss: 3.2311248779296875\n",
      "Epoch 16/100, Step 111, Loss: 3.2509212493896484\n",
      "Epoch 16/100, Step 121, Loss: 2.9525372982025146\n",
      "Epoch 16/100, Step 131, Loss: 2.7908973693847656\n",
      "Epoch 16/100, Step 141, Loss: 3.0409064292907715\n",
      "Epoch 16/100, Step 151, Loss: 2.6710987091064453\n",
      "Epoch 16/100, Step 161, Loss: 3.432678699493408\n",
      "Epoch 16/100, Step 171, Loss: 3.1489593982696533\n",
      "Epoch 16/100, Step 181, Loss: 3.460304021835327\n",
      "Epoch 16/100, Step 191, Loss: 3.131469488143921\n",
      "Epoch 16/100, Step 201, Loss: 3.1733059883117676\n",
      "Epoch 16/100, Step 211, Loss: 2.808349847793579\n",
      "Epoch 16/100, Step 221, Loss: 2.8867626190185547\n",
      "Epoch 16/100, Step 231, Loss: 3.330477476119995\n",
      "Epoch 16/100, Step 241, Loss: 3.077765941619873\n",
      "Epoch 16/100, Step 251, Loss: 3.625403642654419\n",
      "Epoch 16/100, Step 261, Loss: 3.5938634872436523\n",
      "Epoch 16/100, Step 271, Loss: 3.3037474155426025\n",
      "Epoch 16/100, Step 281, Loss: 3.5680480003356934\n",
      "Epoch 16/100, Step 291, Loss: 3.0641419887542725\n",
      "Epoch 16/100, Step 301, Loss: 2.94534969329834\n",
      "Epoch 16/100, Step 311, Loss: 2.9601285457611084\n",
      "Epoch 16/100, Step 321, Loss: 3.407648801803589\n",
      "Epoch 16/100, Step 331, Loss: 2.841118812561035\n",
      "Epoch 16/100, Step 341, Loss: 3.101146697998047\n",
      "Epoch 16/100, Step 351, Loss: 3.3153934478759766\n",
      "Epoch 16/100, Step 361, Loss: 4.272139072418213\n",
      "Epoch 17/100, Step 1, Loss: 2.6988394260406494\n",
      "Epoch 17/100, Step 11, Loss: 2.345444917678833\n",
      "Epoch 17/100, Step 21, Loss: 3.384920597076416\n",
      "Epoch 17/100, Step 31, Loss: 2.368816375732422\n",
      "Epoch 17/100, Step 41, Loss: 3.139561891555786\n",
      "Epoch 17/100, Step 51, Loss: 3.020341396331787\n",
      "Epoch 17/100, Step 61, Loss: 2.7058939933776855\n",
      "Epoch 17/100, Step 71, Loss: 3.4960408210754395\n",
      "Epoch 17/100, Step 81, Loss: 3.154867172241211\n",
      "Epoch 17/100, Step 91, Loss: 2.764129638671875\n",
      "Epoch 17/100, Step 101, Loss: 3.029090642929077\n",
      "Epoch 17/100, Step 111, Loss: 3.373426914215088\n",
      "Epoch 17/100, Step 121, Loss: 2.940246343612671\n",
      "Epoch 17/100, Step 131, Loss: 3.2512919902801514\n",
      "Epoch 17/100, Step 141, Loss: 2.57379412651062\n",
      "Epoch 17/100, Step 151, Loss: 3.9661717414855957\n",
      "Epoch 17/100, Step 161, Loss: 3.714989423751831\n",
      "Epoch 17/100, Step 171, Loss: 3.3222532272338867\n",
      "Epoch 17/100, Step 181, Loss: 2.7328310012817383\n",
      "Epoch 17/100, Step 191, Loss: 3.102146863937378\n",
      "Epoch 17/100, Step 201, Loss: 3.240090847015381\n",
      "Epoch 17/100, Step 211, Loss: 2.7730884552001953\n",
      "Epoch 17/100, Step 221, Loss: 2.958153247833252\n",
      "Epoch 17/100, Step 231, Loss: 3.339808225631714\n",
      "Epoch 17/100, Step 241, Loss: 3.22133469581604\n",
      "Epoch 17/100, Step 251, Loss: 4.122945785522461\n",
      "Epoch 17/100, Step 261, Loss: 2.696185350418091\n",
      "Epoch 17/100, Step 271, Loss: 2.8374006748199463\n",
      "Epoch 17/100, Step 281, Loss: 3.1100432872772217\n",
      "Epoch 17/100, Step 291, Loss: 2.879317045211792\n",
      "Epoch 17/100, Step 301, Loss: 3.1999502182006836\n",
      "Epoch 17/100, Step 311, Loss: 2.7831270694732666\n",
      "Epoch 17/100, Step 321, Loss: 2.9373366832733154\n",
      "Epoch 17/100, Step 331, Loss: 3.0553271770477295\n",
      "Epoch 17/100, Step 341, Loss: 3.250058889389038\n",
      "Epoch 17/100, Step 351, Loss: 2.770433187484741\n",
      "Epoch 17/100, Step 361, Loss: 3.4531617164611816\n",
      "Epoch 18/100, Step 1, Loss: 3.2627389430999756\n",
      "Epoch 18/100, Step 11, Loss: 3.2190027236938477\n",
      "Epoch 18/100, Step 21, Loss: 3.3554887771606445\n",
      "Epoch 18/100, Step 31, Loss: 3.2833058834075928\n",
      "Epoch 18/100, Step 41, Loss: 2.988262414932251\n",
      "Epoch 18/100, Step 51, Loss: 2.6974310874938965\n",
      "Epoch 18/100, Step 61, Loss: 3.0350265502929688\n",
      "Epoch 18/100, Step 71, Loss: 3.670567750930786\n",
      "Epoch 18/100, Step 81, Loss: 3.0538787841796875\n",
      "Epoch 18/100, Step 91, Loss: 2.7579243183135986\n",
      "Epoch 18/100, Step 101, Loss: 2.8151967525482178\n",
      "Epoch 18/100, Step 111, Loss: 2.8977367877960205\n",
      "Epoch 18/100, Step 121, Loss: 3.426417112350464\n",
      "Epoch 18/100, Step 131, Loss: 3.2057507038116455\n",
      "Epoch 18/100, Step 141, Loss: 3.354440212249756\n",
      "Epoch 18/100, Step 151, Loss: 3.051076889038086\n",
      "Epoch 18/100, Step 161, Loss: 3.520143985748291\n",
      "Epoch 18/100, Step 171, Loss: 3.270846366882324\n",
      "Epoch 18/100, Step 181, Loss: 2.7702436447143555\n",
      "Epoch 18/100, Step 191, Loss: 3.0345942974090576\n",
      "Epoch 18/100, Step 201, Loss: 3.1066806316375732\n",
      "Epoch 18/100, Step 211, Loss: 3.465731382369995\n",
      "Epoch 18/100, Step 221, Loss: 3.808964729309082\n",
      "Epoch 18/100, Step 231, Loss: 2.4690027236938477\n",
      "Epoch 18/100, Step 241, Loss: 2.4186832904815674\n",
      "Epoch 18/100, Step 251, Loss: 3.1920270919799805\n",
      "Epoch 18/100, Step 261, Loss: 2.715564250946045\n",
      "Epoch 18/100, Step 271, Loss: 3.2281768321990967\n",
      "Epoch 18/100, Step 281, Loss: 2.9934449195861816\n",
      "Epoch 18/100, Step 291, Loss: 3.1190836429595947\n",
      "Epoch 18/100, Step 301, Loss: 2.998539686203003\n",
      "Epoch 18/100, Step 311, Loss: 2.6449642181396484\n",
      "Epoch 18/100, Step 321, Loss: 2.6634762287139893\n",
      "Epoch 18/100, Step 331, Loss: 2.560664653778076\n",
      "Epoch 18/100, Step 341, Loss: 2.332340717315674\n",
      "Epoch 18/100, Step 351, Loss: 3.1415154933929443\n",
      "Epoch 18/100, Step 361, Loss: 3.557091474533081\n",
      "Epoch 19/100, Step 1, Loss: 2.96378755569458\n",
      "Epoch 19/100, Step 11, Loss: 2.676443576812744\n",
      "Epoch 19/100, Step 21, Loss: 2.9301276206970215\n",
      "Epoch 19/100, Step 31, Loss: 3.1670119762420654\n",
      "Epoch 19/100, Step 41, Loss: 2.4216039180755615\n",
      "Epoch 19/100, Step 51, Loss: 3.1445541381835938\n",
      "Epoch 19/100, Step 61, Loss: 2.2494618892669678\n",
      "Epoch 19/100, Step 71, Loss: 2.8448853492736816\n",
      "Epoch 19/100, Step 81, Loss: 2.604991912841797\n",
      "Epoch 19/100, Step 91, Loss: 3.129603385925293\n",
      "Epoch 19/100, Step 101, Loss: 2.978236198425293\n",
      "Epoch 19/100, Step 111, Loss: 2.719989061355591\n",
      "Epoch 19/100, Step 121, Loss: 2.5827834606170654\n",
      "Epoch 19/100, Step 131, Loss: 2.669222593307495\n",
      "Epoch 19/100, Step 141, Loss: 3.1837899684906006\n",
      "Epoch 19/100, Step 151, Loss: 2.654121160507202\n",
      "Epoch 19/100, Step 161, Loss: 2.940551519393921\n",
      "Epoch 19/100, Step 171, Loss: 3.3901216983795166\n",
      "Epoch 19/100, Step 181, Loss: 3.346261501312256\n",
      "Epoch 19/100, Step 191, Loss: 2.735344409942627\n",
      "Epoch 19/100, Step 201, Loss: 2.25437593460083\n",
      "Epoch 19/100, Step 211, Loss: 3.079110860824585\n",
      "Epoch 19/100, Step 221, Loss: 2.471731424331665\n",
      "Epoch 19/100, Step 231, Loss: 2.613976240158081\n",
      "Epoch 19/100, Step 241, Loss: 2.571704149246216\n",
      "Epoch 19/100, Step 251, Loss: 2.8908138275146484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100, Step 261, Loss: 2.29032564163208\n",
      "Epoch 19/100, Step 271, Loss: 2.7634308338165283\n",
      "Epoch 19/100, Step 281, Loss: 2.3991410732269287\n",
      "Epoch 19/100, Step 291, Loss: 3.1455435752868652\n",
      "Epoch 19/100, Step 301, Loss: 2.346346139907837\n",
      "Epoch 19/100, Step 311, Loss: 3.4039244651794434\n",
      "Epoch 19/100, Step 321, Loss: 2.9404807090759277\n",
      "Epoch 19/100, Step 331, Loss: 2.3965885639190674\n",
      "Epoch 19/100, Step 341, Loss: 2.047685146331787\n",
      "Epoch 19/100, Step 351, Loss: 3.2175233364105225\n",
      "Epoch 19/100, Step 361, Loss: 3.1747334003448486\n",
      "Epoch 20/100, Step 1, Loss: 2.6091017723083496\n",
      "Epoch 20/100, Step 11, Loss: 3.0474154949188232\n",
      "Epoch 20/100, Step 21, Loss: 2.113675594329834\n",
      "Epoch 20/100, Step 31, Loss: 2.347351312637329\n",
      "Epoch 20/100, Step 41, Loss: 2.237776517868042\n",
      "Epoch 20/100, Step 51, Loss: 2.8153810501098633\n",
      "Epoch 20/100, Step 61, Loss: 2.6249876022338867\n",
      "Epoch 20/100, Step 71, Loss: 2.7033579349517822\n",
      "Epoch 20/100, Step 81, Loss: 3.120803117752075\n",
      "Epoch 20/100, Step 91, Loss: 2.809589385986328\n",
      "Epoch 20/100, Step 101, Loss: 2.1772100925445557\n",
      "Epoch 20/100, Step 111, Loss: 3.054868698120117\n",
      "Epoch 20/100, Step 121, Loss: 2.9423553943634033\n",
      "Epoch 20/100, Step 131, Loss: 3.2297282218933105\n",
      "Epoch 20/100, Step 141, Loss: 2.3808507919311523\n",
      "Epoch 20/100, Step 151, Loss: 2.4307360649108887\n",
      "Epoch 20/100, Step 161, Loss: 2.9938814640045166\n",
      "Epoch 20/100, Step 171, Loss: 2.9606056213378906\n",
      "Epoch 20/100, Step 181, Loss: 2.919548749923706\n",
      "Epoch 20/100, Step 191, Loss: 2.2046382427215576\n",
      "Epoch 20/100, Step 201, Loss: 3.044830322265625\n",
      "Epoch 20/100, Step 211, Loss: 2.346304178237915\n",
      "Epoch 20/100, Step 221, Loss: 2.6585495471954346\n",
      "Epoch 20/100, Step 231, Loss: 2.7029497623443604\n",
      "Epoch 20/100, Step 241, Loss: 2.660822629928589\n",
      "Epoch 20/100, Step 251, Loss: 2.2663931846618652\n",
      "Epoch 20/100, Step 261, Loss: 2.8995816707611084\n",
      "Epoch 20/100, Step 271, Loss: 3.121788740158081\n",
      "Epoch 20/100, Step 281, Loss: 2.864785671234131\n",
      "Epoch 20/100, Step 291, Loss: 2.3795108795166016\n",
      "Epoch 20/100, Step 301, Loss: 2.7698001861572266\n",
      "Epoch 20/100, Step 311, Loss: 2.989222764968872\n",
      "Epoch 20/100, Step 321, Loss: 2.874382972717285\n",
      "Epoch 20/100, Step 331, Loss: 2.826561450958252\n",
      "Epoch 20/100, Step 341, Loss: 2.713172674179077\n",
      "Epoch 20/100, Step 351, Loss: 2.379112482070923\n",
      "Epoch 20/100, Step 361, Loss: 2.959249973297119\n",
      "Epoch 21/100, Step 1, Loss: 2.300067901611328\n",
      "Epoch 21/100, Step 11, Loss: 2.81182861328125\n",
      "Epoch 21/100, Step 21, Loss: 2.750882863998413\n",
      "Epoch 21/100, Step 31, Loss: 2.6133997440338135\n",
      "Epoch 21/100, Step 41, Loss: 2.624293565750122\n",
      "Epoch 21/100, Step 51, Loss: 3.104440689086914\n",
      "Epoch 21/100, Step 61, Loss: 2.314788818359375\n",
      "Epoch 21/100, Step 71, Loss: 2.5276002883911133\n",
      "Epoch 21/100, Step 81, Loss: 2.685556173324585\n",
      "Epoch 21/100, Step 91, Loss: 2.477022409439087\n",
      "Epoch 21/100, Step 101, Loss: 1.8476169109344482\n",
      "Epoch 21/100, Step 111, Loss: 2.21405291557312\n",
      "Epoch 21/100, Step 121, Loss: 2.2651336193084717\n",
      "Epoch 21/100, Step 131, Loss: 1.8998043537139893\n",
      "Epoch 21/100, Step 141, Loss: 3.381622076034546\n",
      "Epoch 21/100, Step 151, Loss: 3.64874005317688\n",
      "Epoch 21/100, Step 161, Loss: 2.5273990631103516\n",
      "Epoch 21/100, Step 171, Loss: 2.715409994125366\n",
      "Epoch 21/100, Step 181, Loss: 2.2280845642089844\n",
      "Epoch 21/100, Step 191, Loss: 2.850703716278076\n",
      "Epoch 21/100, Step 201, Loss: 2.0671722888946533\n",
      "Epoch 21/100, Step 211, Loss: 2.334792137145996\n",
      "Epoch 21/100, Step 221, Loss: 2.25628924369812\n",
      "Epoch 21/100, Step 231, Loss: 2.625288724899292\n",
      "Epoch 21/100, Step 241, Loss: 2.174034357070923\n",
      "Epoch 21/100, Step 251, Loss: 2.634650945663452\n",
      "Epoch 21/100, Step 261, Loss: 2.4807024002075195\n",
      "Epoch 21/100, Step 271, Loss: 2.293116807937622\n",
      "Epoch 21/100, Step 281, Loss: 2.546647787094116\n",
      "Epoch 21/100, Step 291, Loss: 2.4680330753326416\n",
      "Epoch 21/100, Step 301, Loss: 3.201460361480713\n",
      "Epoch 21/100, Step 311, Loss: 3.157458782196045\n",
      "Epoch 21/100, Step 321, Loss: 3.1872949600219727\n",
      "Epoch 21/100, Step 331, Loss: 2.079663038253784\n",
      "Epoch 21/100, Step 341, Loss: 2.3443381786346436\n",
      "Epoch 21/100, Step 351, Loss: 3.0885024070739746\n",
      "Epoch 21/100, Step 361, Loss: 2.6895439624786377\n",
      "Epoch 22/100, Step 1, Loss: 2.5812435150146484\n",
      "Epoch 22/100, Step 11, Loss: 2.896371841430664\n",
      "Epoch 22/100, Step 21, Loss: 3.051332712173462\n",
      "Epoch 22/100, Step 31, Loss: 1.935883641242981\n",
      "Epoch 22/100, Step 41, Loss: 2.799596071243286\n",
      "Epoch 22/100, Step 51, Loss: 2.5558183193206787\n",
      "Epoch 22/100, Step 61, Loss: 2.559011936187744\n",
      "Epoch 22/100, Step 71, Loss: 2.0213260650634766\n",
      "Epoch 22/100, Step 81, Loss: 2.6677069664001465\n",
      "Epoch 22/100, Step 91, Loss: 2.223825693130493\n",
      "Epoch 22/100, Step 101, Loss: 3.5586376190185547\n",
      "Epoch 22/100, Step 111, Loss: 2.2371644973754883\n",
      "Epoch 22/100, Step 121, Loss: 3.157013416290283\n",
      "Epoch 22/100, Step 131, Loss: 2.664454698562622\n",
      "Epoch 22/100, Step 141, Loss: 2.452134132385254\n",
      "Epoch 22/100, Step 151, Loss: 2.4079806804656982\n",
      "Epoch 22/100, Step 161, Loss: 2.6322593688964844\n",
      "Epoch 22/100, Step 171, Loss: 3.1307430267333984\n",
      "Epoch 22/100, Step 181, Loss: 2.66776967048645\n",
      "Epoch 22/100, Step 191, Loss: 2.413303852081299\n",
      "Epoch 22/100, Step 201, Loss: 2.2511935234069824\n",
      "Epoch 22/100, Step 211, Loss: 2.366589307785034\n",
      "Epoch 22/100, Step 221, Loss: 2.462655544281006\n",
      "Epoch 22/100, Step 231, Loss: 2.666590452194214\n",
      "Epoch 22/100, Step 241, Loss: 3.1954784393310547\n",
      "Epoch 22/100, Step 251, Loss: 2.608083486557007\n",
      "Epoch 22/100, Step 261, Loss: 2.921724557876587\n",
      "Epoch 22/100, Step 271, Loss: 2.125575542449951\n",
      "Epoch 22/100, Step 281, Loss: 2.4281697273254395\n",
      "Epoch 22/100, Step 291, Loss: 2.3105688095092773\n",
      "Epoch 22/100, Step 301, Loss: 2.397742986679077\n",
      "Epoch 22/100, Step 311, Loss: 2.4086754322052\n",
      "Epoch 22/100, Step 321, Loss: 2.649829864501953\n",
      "Epoch 22/100, Step 331, Loss: 2.1126677989959717\n",
      "Epoch 22/100, Step 341, Loss: 2.259038209915161\n",
      "Epoch 22/100, Step 351, Loss: 2.611551284790039\n",
      "Epoch 22/100, Step 361, Loss: 2.7561280727386475\n",
      "Epoch 23/100, Step 1, Loss: 2.934735059738159\n",
      "Epoch 23/100, Step 11, Loss: 2.1800997257232666\n",
      "Epoch 23/100, Step 21, Loss: 2.654634714126587\n",
      "Epoch 23/100, Step 31, Loss: 2.372633934020996\n",
      "Epoch 23/100, Step 41, Loss: 2.187962055206299\n",
      "Epoch 23/100, Step 51, Loss: 1.7350242137908936\n",
      "Epoch 23/100, Step 61, Loss: 2.675184965133667\n",
      "Epoch 23/100, Step 71, Loss: 1.9847710132598877\n",
      "Epoch 23/100, Step 81, Loss: 2.964372396469116\n",
      "Epoch 23/100, Step 91, Loss: 2.3570590019226074\n",
      "Epoch 23/100, Step 101, Loss: 1.8662034273147583\n",
      "Epoch 23/100, Step 111, Loss: 2.5475778579711914\n",
      "Epoch 23/100, Step 121, Loss: 2.358823299407959\n",
      "Epoch 23/100, Step 131, Loss: 2.5613934993743896\n",
      "Epoch 23/100, Step 141, Loss: 2.6040661334991455\n",
      "Epoch 23/100, Step 151, Loss: 2.9952666759490967\n",
      "Epoch 23/100, Step 161, Loss: 2.6397197246551514\n",
      "Epoch 23/100, Step 171, Loss: 1.8770265579223633\n",
      "Epoch 23/100, Step 181, Loss: 2.7075819969177246\n",
      "Epoch 23/100, Step 191, Loss: 2.433544158935547\n",
      "Epoch 23/100, Step 201, Loss: 1.9178639650344849\n",
      "Epoch 23/100, Step 211, Loss: 2.974787712097168\n",
      "Epoch 23/100, Step 221, Loss: 2.712832450866699\n",
      "Epoch 23/100, Step 231, Loss: 2.6101930141448975\n",
      "Epoch 23/100, Step 241, Loss: 2.2906227111816406\n",
      "Epoch 23/100, Step 251, Loss: 2.591872453689575\n",
      "Epoch 23/100, Step 261, Loss: 2.100531816482544\n",
      "Epoch 23/100, Step 271, Loss: 2.4874114990234375\n",
      "Epoch 23/100, Step 281, Loss: 2.6652259826660156\n",
      "Epoch 23/100, Step 291, Loss: 2.757737636566162\n",
      "Epoch 23/100, Step 301, Loss: 2.621159791946411\n",
      "Epoch 23/100, Step 311, Loss: 2.828936815261841\n",
      "Epoch 23/100, Step 321, Loss: 2.0275614261627197\n",
      "Epoch 23/100, Step 331, Loss: 2.1468191146850586\n",
      "Epoch 23/100, Step 341, Loss: 2.389270305633545\n",
      "Epoch 23/100, Step 351, Loss: 1.6986502408981323\n",
      "Epoch 23/100, Step 361, Loss: 2.2168805599212646\n",
      "Epoch 24/100, Step 1, Loss: 1.9646729230880737\n",
      "Epoch 24/100, Step 11, Loss: 2.6252710819244385\n",
      "Epoch 24/100, Step 21, Loss: 1.918069839477539\n",
      "Epoch 24/100, Step 31, Loss: 2.118943452835083\n",
      "Epoch 24/100, Step 41, Loss: 2.209113597869873\n",
      "Epoch 24/100, Step 51, Loss: 1.8069714307785034\n",
      "Epoch 24/100, Step 61, Loss: 2.67934250831604\n",
      "Epoch 24/100, Step 71, Loss: 2.0054783821105957\n",
      "Epoch 24/100, Step 81, Loss: 2.968712568283081\n",
      "Epoch 24/100, Step 91, Loss: 2.763885021209717\n",
      "Epoch 24/100, Step 101, Loss: 1.8616697788238525\n",
      "Epoch 24/100, Step 111, Loss: 2.2686753273010254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100, Step 121, Loss: 1.5018043518066406\n",
      "Epoch 24/100, Step 131, Loss: 1.9742450714111328\n",
      "Epoch 24/100, Step 141, Loss: 2.3827602863311768\n",
      "Epoch 24/100, Step 151, Loss: 1.5930135250091553\n",
      "Epoch 24/100, Step 161, Loss: 2.8260014057159424\n",
      "Epoch 24/100, Step 171, Loss: 1.4859927892684937\n",
      "Epoch 24/100, Step 181, Loss: 1.777292251586914\n",
      "Epoch 24/100, Step 191, Loss: 2.0554990768432617\n",
      "Epoch 24/100, Step 201, Loss: 2.668644428253174\n",
      "Epoch 24/100, Step 211, Loss: 2.5380334854125977\n",
      "Epoch 24/100, Step 221, Loss: 2.0837066173553467\n",
      "Epoch 24/100, Step 231, Loss: 2.0615506172180176\n",
      "Epoch 24/100, Step 241, Loss: 2.5047855377197266\n",
      "Epoch 24/100, Step 251, Loss: 2.7849576473236084\n",
      "Epoch 24/100, Step 261, Loss: 2.0214157104492188\n",
      "Epoch 24/100, Step 271, Loss: 2.034599781036377\n",
      "Epoch 24/100, Step 281, Loss: 1.8874285221099854\n",
      "Epoch 24/100, Step 291, Loss: 2.6097512245178223\n",
      "Epoch 24/100, Step 301, Loss: 2.4113290309906006\n",
      "Epoch 24/100, Step 311, Loss: 1.4095110893249512\n",
      "Epoch 24/100, Step 321, Loss: 2.1788837909698486\n",
      "Epoch 24/100, Step 331, Loss: 2.3678059577941895\n",
      "Epoch 24/100, Step 341, Loss: 2.6234140396118164\n",
      "Epoch 24/100, Step 351, Loss: 2.573289155960083\n",
      "Epoch 24/100, Step 361, Loss: 2.6029605865478516\n",
      "Epoch 25/100, Step 1, Loss: 2.560396671295166\n",
      "Epoch 25/100, Step 11, Loss: 1.7515270709991455\n",
      "Epoch 25/100, Step 21, Loss: 2.6808149814605713\n",
      "Epoch 25/100, Step 31, Loss: 2.6175076961517334\n",
      "Epoch 25/100, Step 41, Loss: 2.338850259780884\n",
      "Epoch 25/100, Step 51, Loss: 2.376774311065674\n",
      "Epoch 25/100, Step 61, Loss: 2.3428308963775635\n",
      "Epoch 25/100, Step 71, Loss: 2.060535430908203\n",
      "Epoch 25/100, Step 81, Loss: 1.5028129816055298\n",
      "Epoch 25/100, Step 91, Loss: 2.3034167289733887\n",
      "Epoch 25/100, Step 101, Loss: 2.7075984477996826\n",
      "Epoch 25/100, Step 111, Loss: 2.207681894302368\n",
      "Epoch 25/100, Step 121, Loss: 2.517850399017334\n",
      "Epoch 25/100, Step 131, Loss: 2.593914031982422\n",
      "Epoch 25/100, Step 141, Loss: 1.8493330478668213\n",
      "Epoch 25/100, Step 151, Loss: 1.9734126329421997\n",
      "Epoch 25/100, Step 161, Loss: 2.0813052654266357\n",
      "Epoch 25/100, Step 171, Loss: 1.6199748516082764\n",
      "Epoch 25/100, Step 181, Loss: 2.6300830841064453\n",
      "Epoch 25/100, Step 191, Loss: 2.102245569229126\n",
      "Epoch 25/100, Step 201, Loss: 1.9595987796783447\n",
      "Epoch 25/100, Step 211, Loss: 2.143235683441162\n",
      "Epoch 25/100, Step 221, Loss: 1.7495133876800537\n",
      "Epoch 25/100, Step 231, Loss: 2.522615671157837\n",
      "Epoch 25/100, Step 241, Loss: 2.5155439376831055\n",
      "Epoch 25/100, Step 251, Loss: 1.9160443544387817\n",
      "Epoch 25/100, Step 261, Loss: 1.6538854837417603\n",
      "Epoch 25/100, Step 271, Loss: 2.0850515365600586\n",
      "Epoch 25/100, Step 281, Loss: 1.946420669555664\n",
      "Epoch 25/100, Step 291, Loss: 2.11484694480896\n",
      "Epoch 25/100, Step 301, Loss: 2.1733336448669434\n",
      "Epoch 25/100, Step 311, Loss: 2.0364129543304443\n",
      "Epoch 25/100, Step 321, Loss: 2.3747713565826416\n",
      "Epoch 25/100, Step 331, Loss: 2.0096983909606934\n",
      "Epoch 25/100, Step 341, Loss: 1.801604151725769\n",
      "Epoch 25/100, Step 351, Loss: 2.145819902420044\n",
      "Epoch 25/100, Step 361, Loss: 1.3231257200241089\n",
      "Epoch 26/100, Step 1, Loss: 2.6106958389282227\n",
      "Epoch 26/100, Step 11, Loss: 1.732214331626892\n",
      "Epoch 26/100, Step 21, Loss: 2.3887345790863037\n",
      "Epoch 26/100, Step 31, Loss: 1.4826855659484863\n",
      "Epoch 26/100, Step 41, Loss: 1.7937012910842896\n",
      "Epoch 26/100, Step 51, Loss: 2.3801400661468506\n",
      "Epoch 26/100, Step 61, Loss: 2.3294215202331543\n",
      "Epoch 26/100, Step 71, Loss: 2.29190993309021\n",
      "Epoch 26/100, Step 81, Loss: 1.6157522201538086\n",
      "Epoch 26/100, Step 91, Loss: 2.0979418754577637\n",
      "Epoch 26/100, Step 101, Loss: 2.2040202617645264\n",
      "Epoch 26/100, Step 111, Loss: 2.0391032695770264\n",
      "Epoch 26/100, Step 121, Loss: 2.158298969268799\n",
      "Epoch 26/100, Step 131, Loss: 1.7677810192108154\n",
      "Epoch 26/100, Step 141, Loss: 2.263354539871216\n",
      "Epoch 26/100, Step 151, Loss: 1.727139949798584\n",
      "Epoch 26/100, Step 161, Loss: 1.5989149808883667\n",
      "Epoch 26/100, Step 171, Loss: 2.1685643196105957\n",
      "Epoch 26/100, Step 181, Loss: 2.000014543533325\n",
      "Epoch 26/100, Step 191, Loss: 2.3963029384613037\n",
      "Epoch 26/100, Step 201, Loss: 2.2297561168670654\n",
      "Epoch 26/100, Step 211, Loss: 1.9696950912475586\n",
      "Epoch 26/100, Step 221, Loss: 1.8343802690505981\n",
      "Epoch 26/100, Step 231, Loss: 2.345759868621826\n",
      "Epoch 26/100, Step 241, Loss: 1.5418821573257446\n",
      "Epoch 26/100, Step 251, Loss: 1.4037086963653564\n",
      "Epoch 26/100, Step 261, Loss: 1.7324976921081543\n",
      "Epoch 26/100, Step 271, Loss: 1.9253156185150146\n",
      "Epoch 26/100, Step 281, Loss: 2.4984993934631348\n",
      "Epoch 26/100, Step 291, Loss: 2.6108930110931396\n",
      "Epoch 26/100, Step 301, Loss: 1.3467931747436523\n",
      "Epoch 26/100, Step 311, Loss: 2.2055094242095947\n",
      "Epoch 26/100, Step 321, Loss: 2.2763547897338867\n",
      "Epoch 26/100, Step 331, Loss: 1.4707603454589844\n",
      "Epoch 26/100, Step 341, Loss: 2.5377840995788574\n",
      "Epoch 26/100, Step 351, Loss: 2.7482893466949463\n",
      "Epoch 26/100, Step 361, Loss: 2.2447710037231445\n",
      "Epoch 27/100, Step 1, Loss: 2.367654800415039\n",
      "Epoch 27/100, Step 11, Loss: 2.2918636798858643\n",
      "Epoch 27/100, Step 21, Loss: 2.5190846920013428\n",
      "Epoch 27/100, Step 31, Loss: 2.327134132385254\n",
      "Epoch 27/100, Step 41, Loss: 1.9612408876419067\n",
      "Epoch 27/100, Step 51, Loss: 1.9219385385513306\n",
      "Epoch 27/100, Step 61, Loss: 1.9253090620040894\n",
      "Epoch 27/100, Step 71, Loss: 1.8502386808395386\n",
      "Epoch 27/100, Step 81, Loss: 1.9999455213546753\n",
      "Epoch 27/100, Step 91, Loss: 1.8747375011444092\n",
      "Epoch 27/100, Step 101, Loss: 1.428898572921753\n",
      "Epoch 27/100, Step 111, Loss: 2.2961127758026123\n",
      "Epoch 27/100, Step 121, Loss: 1.244000792503357\n",
      "Epoch 27/100, Step 131, Loss: 1.562467336654663\n",
      "Epoch 27/100, Step 141, Loss: 2.4033563137054443\n",
      "Epoch 27/100, Step 151, Loss: 1.6971265077590942\n",
      "Epoch 27/100, Step 161, Loss: 1.9599641561508179\n",
      "Epoch 27/100, Step 171, Loss: 1.601222038269043\n",
      "Epoch 27/100, Step 181, Loss: 1.8356444835662842\n",
      "Epoch 27/100, Step 191, Loss: 2.065356731414795\n",
      "Epoch 27/100, Step 201, Loss: 1.9384177923202515\n",
      "Epoch 27/100, Step 211, Loss: 1.8265769481658936\n",
      "Epoch 27/100, Step 221, Loss: 1.9964040517807007\n",
      "Epoch 27/100, Step 231, Loss: 1.4831500053405762\n",
      "Epoch 27/100, Step 241, Loss: 2.6364991664886475\n",
      "Epoch 27/100, Step 251, Loss: 1.7445905208587646\n",
      "Epoch 27/100, Step 261, Loss: 1.7909706830978394\n",
      "Epoch 27/100, Step 271, Loss: 2.040032386779785\n",
      "Epoch 27/100, Step 281, Loss: 2.508673667907715\n",
      "Epoch 27/100, Step 291, Loss: 1.5241340398788452\n",
      "Epoch 27/100, Step 301, Loss: 1.7095592021942139\n",
      "Epoch 27/100, Step 311, Loss: 2.2746450901031494\n",
      "Epoch 27/100, Step 321, Loss: 1.9974318742752075\n",
      "Epoch 27/100, Step 331, Loss: 2.915154457092285\n",
      "Epoch 27/100, Step 341, Loss: 1.9994566440582275\n",
      "Epoch 27/100, Step 351, Loss: 1.7628893852233887\n",
      "Epoch 27/100, Step 361, Loss: 2.356257915496826\n",
      "Epoch 28/100, Step 1, Loss: 1.611315131187439\n",
      "Epoch 28/100, Step 11, Loss: 1.8614715337753296\n",
      "Epoch 28/100, Step 21, Loss: 1.7168978452682495\n",
      "Epoch 28/100, Step 31, Loss: 1.6138101816177368\n",
      "Epoch 28/100, Step 41, Loss: 1.265761375427246\n",
      "Epoch 28/100, Step 51, Loss: 2.126237392425537\n",
      "Epoch 28/100, Step 61, Loss: 2.418592691421509\n",
      "Epoch 28/100, Step 71, Loss: 1.7248024940490723\n",
      "Epoch 28/100, Step 81, Loss: 1.9246474504470825\n",
      "Epoch 28/100, Step 91, Loss: 2.247603178024292\n",
      "Epoch 28/100, Step 101, Loss: 1.901439905166626\n",
      "Epoch 28/100, Step 111, Loss: 1.7224316596984863\n",
      "Epoch 28/100, Step 121, Loss: 1.5052303075790405\n",
      "Epoch 28/100, Step 131, Loss: 2.2883222103118896\n",
      "Epoch 28/100, Step 141, Loss: 1.7518583536148071\n",
      "Epoch 28/100, Step 151, Loss: 1.9709123373031616\n",
      "Epoch 28/100, Step 161, Loss: 2.1807000637054443\n",
      "Epoch 28/100, Step 171, Loss: 1.714274287223816\n",
      "Epoch 28/100, Step 181, Loss: 1.995500087738037\n",
      "Epoch 28/100, Step 191, Loss: 1.6947381496429443\n",
      "Epoch 28/100, Step 201, Loss: 1.7488553524017334\n",
      "Epoch 28/100, Step 211, Loss: 1.7463877201080322\n",
      "Epoch 28/100, Step 221, Loss: 2.05670428276062\n",
      "Epoch 28/100, Step 231, Loss: 1.590362787246704\n",
      "Epoch 28/100, Step 241, Loss: 2.1275711059570312\n",
      "Epoch 28/100, Step 251, Loss: 2.7384867668151855\n",
      "Epoch 28/100, Step 261, Loss: 1.755316972732544\n",
      "Epoch 28/100, Step 271, Loss: 1.906263828277588\n",
      "Epoch 28/100, Step 281, Loss: 1.4777061939239502\n",
      "Epoch 28/100, Step 291, Loss: 2.1098742485046387\n",
      "Epoch 28/100, Step 301, Loss: 2.261733055114746\n",
      "Epoch 28/100, Step 311, Loss: 1.573039174079895\n",
      "Epoch 28/100, Step 321, Loss: 1.7247939109802246\n",
      "Epoch 28/100, Step 331, Loss: 1.948866605758667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100, Step 341, Loss: 1.8501349687576294\n",
      "Epoch 28/100, Step 351, Loss: 1.3280413150787354\n",
      "Epoch 28/100, Step 361, Loss: 1.849678874015808\n",
      "Epoch 29/100, Step 1, Loss: 2.28460693359375\n",
      "Epoch 29/100, Step 11, Loss: 2.7102251052856445\n",
      "Epoch 29/100, Step 21, Loss: 2.336930751800537\n",
      "Epoch 29/100, Step 31, Loss: 1.5581122636795044\n",
      "Epoch 29/100, Step 41, Loss: 1.2602171897888184\n",
      "Epoch 29/100, Step 51, Loss: 1.9756364822387695\n",
      "Epoch 29/100, Step 61, Loss: 1.797582745552063\n",
      "Epoch 29/100, Step 71, Loss: 1.500029444694519\n",
      "Epoch 29/100, Step 81, Loss: 2.1754848957061768\n",
      "Epoch 29/100, Step 91, Loss: 1.9700956344604492\n",
      "Epoch 29/100, Step 101, Loss: 1.3857269287109375\n",
      "Epoch 29/100, Step 111, Loss: 2.20163893699646\n",
      "Epoch 29/100, Step 121, Loss: 2.064619541168213\n",
      "Epoch 29/100, Step 131, Loss: 2.1262166500091553\n",
      "Epoch 29/100, Step 141, Loss: 1.4202625751495361\n",
      "Epoch 29/100, Step 151, Loss: 1.3071367740631104\n",
      "Epoch 29/100, Step 161, Loss: 2.2709224224090576\n",
      "Epoch 29/100, Step 171, Loss: 2.439394950866699\n",
      "Epoch 29/100, Step 181, Loss: 1.7942314147949219\n",
      "Epoch 29/100, Step 191, Loss: 1.504908561706543\n",
      "Epoch 29/100, Step 201, Loss: 2.249302625656128\n",
      "Epoch 29/100, Step 211, Loss: 1.8819754123687744\n",
      "Epoch 29/100, Step 221, Loss: 2.1079630851745605\n",
      "Epoch 29/100, Step 231, Loss: 1.9752678871154785\n",
      "Epoch 29/100, Step 241, Loss: 2.0748133659362793\n",
      "Epoch 29/100, Step 251, Loss: 1.9435046911239624\n",
      "Epoch 29/100, Step 261, Loss: 1.6884685754776\n",
      "Epoch 29/100, Step 271, Loss: 1.520819902420044\n",
      "Epoch 29/100, Step 281, Loss: 1.837143898010254\n",
      "Epoch 29/100, Step 291, Loss: 1.312558650970459\n",
      "Epoch 29/100, Step 301, Loss: 1.9271411895751953\n",
      "Epoch 29/100, Step 311, Loss: 2.053396701812744\n",
      "Epoch 29/100, Step 321, Loss: 1.7528250217437744\n",
      "Epoch 29/100, Step 331, Loss: 1.7229664325714111\n",
      "Epoch 29/100, Step 341, Loss: 2.186354637145996\n",
      "Epoch 29/100, Step 351, Loss: 2.1970620155334473\n",
      "Epoch 29/100, Step 361, Loss: 1.8322032690048218\n",
      "Epoch 30/100, Step 1, Loss: 1.2884703874588013\n",
      "Epoch 30/100, Step 11, Loss: 1.8165878057479858\n",
      "Epoch 30/100, Step 21, Loss: 1.8024762868881226\n",
      "Epoch 30/100, Step 31, Loss: 2.113413095474243\n",
      "Epoch 30/100, Step 41, Loss: 1.3953486680984497\n",
      "Epoch 30/100, Step 51, Loss: 1.3623896837234497\n",
      "Epoch 30/100, Step 61, Loss: 1.4212031364440918\n",
      "Epoch 30/100, Step 71, Loss: 1.7786856889724731\n",
      "Epoch 30/100, Step 81, Loss: 1.3756556510925293\n",
      "Epoch 30/100, Step 91, Loss: 1.5129539966583252\n",
      "Epoch 30/100, Step 101, Loss: 1.4063599109649658\n",
      "Epoch 30/100, Step 111, Loss: 1.7041693925857544\n",
      "Epoch 30/100, Step 121, Loss: 1.9510329961776733\n",
      "Epoch 30/100, Step 131, Loss: 1.5311157703399658\n",
      "Epoch 30/100, Step 141, Loss: 1.2898210287094116\n",
      "Epoch 30/100, Step 151, Loss: 1.7243311405181885\n",
      "Epoch 30/100, Step 161, Loss: 2.219695568084717\n",
      "Epoch 30/100, Step 171, Loss: 2.1871180534362793\n",
      "Epoch 30/100, Step 181, Loss: 1.500518560409546\n",
      "Epoch 30/100, Step 191, Loss: 1.4264224767684937\n",
      "Epoch 30/100, Step 201, Loss: 2.2831778526306152\n",
      "Epoch 30/100, Step 211, Loss: 1.3254388570785522\n",
      "Epoch 30/100, Step 221, Loss: 1.976691722869873\n",
      "Epoch 30/100, Step 231, Loss: 1.2605692148208618\n",
      "Epoch 30/100, Step 241, Loss: 1.4938278198242188\n",
      "Epoch 30/100, Step 251, Loss: 1.552031397819519\n",
      "Epoch 30/100, Step 261, Loss: 1.9904166460037231\n",
      "Epoch 30/100, Step 271, Loss: 1.4417262077331543\n",
      "Epoch 30/100, Step 281, Loss: 1.4019206762313843\n",
      "Epoch 30/100, Step 291, Loss: 1.6602314710617065\n",
      "Epoch 30/100, Step 301, Loss: 1.4082545042037964\n",
      "Epoch 30/100, Step 311, Loss: 2.0905303955078125\n",
      "Epoch 30/100, Step 321, Loss: 1.8057775497436523\n",
      "Epoch 30/100, Step 331, Loss: 2.0571401119232178\n",
      "Epoch 30/100, Step 341, Loss: 1.54154634475708\n",
      "Epoch 30/100, Step 351, Loss: 1.3417326211929321\n",
      "Epoch 30/100, Step 361, Loss: 1.6587179899215698\n",
      "Epoch 31/100, Step 1, Loss: 1.5753633975982666\n",
      "Epoch 31/100, Step 11, Loss: 1.647390604019165\n",
      "Epoch 31/100, Step 21, Loss: 1.6482187509536743\n",
      "Epoch 31/100, Step 31, Loss: 1.4574276208877563\n",
      "Epoch 31/100, Step 41, Loss: 1.9547969102859497\n",
      "Epoch 31/100, Step 51, Loss: 1.8474295139312744\n",
      "Epoch 31/100, Step 61, Loss: 1.2689785957336426\n",
      "Epoch 31/100, Step 71, Loss: 1.6169285774230957\n",
      "Epoch 31/100, Step 81, Loss: 1.3796802759170532\n",
      "Epoch 31/100, Step 91, Loss: 1.498143196105957\n",
      "Epoch 31/100, Step 101, Loss: 2.1505329608917236\n",
      "Epoch 31/100, Step 111, Loss: 1.3354626893997192\n",
      "Epoch 31/100, Step 121, Loss: 1.581024408340454\n",
      "Epoch 31/100, Step 131, Loss: 1.3169084787368774\n",
      "Epoch 31/100, Step 141, Loss: 2.3176474571228027\n",
      "Epoch 31/100, Step 151, Loss: 2.1663999557495117\n",
      "Epoch 31/100, Step 161, Loss: 1.8831703662872314\n",
      "Epoch 31/100, Step 171, Loss: 1.9889448881149292\n",
      "Epoch 31/100, Step 181, Loss: 1.6387985944747925\n",
      "Epoch 31/100, Step 191, Loss: 2.0510339736938477\n",
      "Epoch 31/100, Step 201, Loss: 1.4766008853912354\n",
      "Epoch 31/100, Step 211, Loss: 1.2867094278335571\n",
      "Epoch 31/100, Step 221, Loss: 1.2601439952850342\n",
      "Epoch 31/100, Step 231, Loss: 1.3565105199813843\n",
      "Epoch 31/100, Step 241, Loss: 1.8700294494628906\n",
      "Epoch 31/100, Step 251, Loss: 1.649381399154663\n",
      "Epoch 31/100, Step 261, Loss: 1.505461573600769\n",
      "Epoch 31/100, Step 271, Loss: 1.4168858528137207\n",
      "Epoch 31/100, Step 281, Loss: 0.9641104936599731\n",
      "Epoch 31/100, Step 291, Loss: 1.5284911394119263\n",
      "Epoch 31/100, Step 301, Loss: 1.8345997333526611\n",
      "Epoch 31/100, Step 311, Loss: 1.5702369213104248\n",
      "Epoch 31/100, Step 321, Loss: 1.7067267894744873\n",
      "Epoch 31/100, Step 331, Loss: 1.75462806224823\n",
      "Epoch 31/100, Step 341, Loss: 1.2500643730163574\n",
      "Epoch 31/100, Step 351, Loss: 0.970268964767456\n",
      "Epoch 31/100, Step 361, Loss: 1.380663514137268\n",
      "Epoch 32/100, Step 1, Loss: 1.637579083442688\n",
      "Epoch 32/100, Step 11, Loss: 1.5035486221313477\n",
      "Epoch 32/100, Step 21, Loss: 1.1224058866500854\n",
      "Epoch 32/100, Step 31, Loss: 1.083349585533142\n",
      "Epoch 32/100, Step 41, Loss: 1.6376744508743286\n",
      "Epoch 32/100, Step 51, Loss: 1.7653142213821411\n",
      "Epoch 32/100, Step 61, Loss: 2.3882291316986084\n",
      "Epoch 32/100, Step 71, Loss: 1.4945236444473267\n",
      "Epoch 32/100, Step 81, Loss: 1.3351176977157593\n",
      "Epoch 32/100, Step 91, Loss: 1.9197757244110107\n",
      "Epoch 32/100, Step 101, Loss: 1.3374006748199463\n",
      "Epoch 32/100, Step 111, Loss: 1.5842500925064087\n",
      "Epoch 32/100, Step 121, Loss: 1.88677978515625\n",
      "Epoch 32/100, Step 131, Loss: 1.488627552986145\n",
      "Epoch 32/100, Step 141, Loss: 2.098876953125\n",
      "Epoch 32/100, Step 151, Loss: 1.5992926359176636\n",
      "Epoch 32/100, Step 161, Loss: 1.815269947052002\n",
      "Epoch 32/100, Step 171, Loss: 1.6852915287017822\n",
      "Epoch 32/100, Step 181, Loss: 1.481080412864685\n",
      "Epoch 32/100, Step 191, Loss: 1.5782907009124756\n",
      "Epoch 32/100, Step 201, Loss: 1.347939372062683\n",
      "Epoch 32/100, Step 211, Loss: 1.5214720964431763\n",
      "Epoch 32/100, Step 221, Loss: 1.4909015893936157\n",
      "Epoch 32/100, Step 231, Loss: 1.9196583032608032\n",
      "Epoch 32/100, Step 241, Loss: 1.7581642866134644\n",
      "Epoch 32/100, Step 251, Loss: 1.8171021938323975\n",
      "Epoch 32/100, Step 261, Loss: 1.4912042617797852\n",
      "Epoch 32/100, Step 271, Loss: 1.60169517993927\n",
      "Epoch 32/100, Step 281, Loss: 1.6067779064178467\n",
      "Epoch 32/100, Step 291, Loss: 1.154387354850769\n",
      "Epoch 32/100, Step 301, Loss: 1.9394034147262573\n",
      "Epoch 32/100, Step 311, Loss: 1.9723901748657227\n",
      "Epoch 32/100, Step 321, Loss: 1.7266370058059692\n",
      "Epoch 32/100, Step 331, Loss: 1.7004460096359253\n",
      "Epoch 32/100, Step 341, Loss: 2.2567334175109863\n",
      "Epoch 32/100, Step 351, Loss: 1.7084544897079468\n",
      "Epoch 32/100, Step 361, Loss: 1.177716851234436\n",
      "Epoch 33/100, Step 1, Loss: 1.4569423198699951\n",
      "Epoch 33/100, Step 11, Loss: 1.9590173959732056\n",
      "Epoch 33/100, Step 21, Loss: 1.1103585958480835\n",
      "Epoch 33/100, Step 31, Loss: 0.9902933239936829\n",
      "Epoch 33/100, Step 41, Loss: 1.4866880178451538\n",
      "Epoch 33/100, Step 51, Loss: 1.4336494207382202\n",
      "Epoch 33/100, Step 61, Loss: 1.341348648071289\n",
      "Epoch 33/100, Step 71, Loss: 1.8058433532714844\n",
      "Epoch 33/100, Step 81, Loss: 1.1445612907409668\n",
      "Epoch 33/100, Step 91, Loss: 1.2087360620498657\n",
      "Epoch 33/100, Step 101, Loss: 1.375159502029419\n",
      "Epoch 33/100, Step 111, Loss: 1.4670968055725098\n",
      "Epoch 33/100, Step 121, Loss: 1.0081413984298706\n",
      "Epoch 33/100, Step 131, Loss: 1.1712127923965454\n",
      "Epoch 33/100, Step 141, Loss: 1.4637969732284546\n",
      "Epoch 33/100, Step 151, Loss: 1.5521385669708252\n",
      "Epoch 33/100, Step 161, Loss: 1.2679728269577026\n",
      "Epoch 33/100, Step 171, Loss: 1.5712977647781372\n",
      "Epoch 33/100, Step 181, Loss: 1.14418363571167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100, Step 191, Loss: 1.693976879119873\n",
      "Epoch 33/100, Step 201, Loss: 0.9707023501396179\n",
      "Epoch 33/100, Step 211, Loss: 1.4501733779907227\n",
      "Epoch 33/100, Step 221, Loss: 1.3485803604125977\n",
      "Epoch 33/100, Step 231, Loss: 1.2469815015792847\n",
      "Epoch 33/100, Step 241, Loss: 1.4300544261932373\n",
      "Epoch 33/100, Step 251, Loss: 1.6326584815979004\n",
      "Epoch 33/100, Step 261, Loss: 1.7960292100906372\n",
      "Epoch 33/100, Step 271, Loss: 1.1495803594589233\n",
      "Epoch 33/100, Step 281, Loss: 0.9055989384651184\n",
      "Epoch 33/100, Step 291, Loss: 1.5467348098754883\n",
      "Epoch 33/100, Step 301, Loss: 1.6585365533828735\n",
      "Epoch 33/100, Step 311, Loss: 1.4232083559036255\n",
      "Epoch 33/100, Step 321, Loss: 1.319531798362732\n",
      "Epoch 33/100, Step 331, Loss: 1.7763291597366333\n",
      "Epoch 33/100, Step 341, Loss: 0.9498095512390137\n",
      "Epoch 33/100, Step 351, Loss: 0.9406037330627441\n",
      "Epoch 33/100, Step 361, Loss: 1.5985623598098755\n",
      "Epoch 34/100, Step 1, Loss: 1.3483681678771973\n",
      "Epoch 34/100, Step 11, Loss: 0.9693371653556824\n",
      "Epoch 34/100, Step 21, Loss: 1.4243972301483154\n",
      "Epoch 34/100, Step 31, Loss: 1.1296221017837524\n",
      "Epoch 34/100, Step 41, Loss: 0.9894505739212036\n",
      "Epoch 34/100, Step 51, Loss: 1.3620812892913818\n",
      "Epoch 34/100, Step 61, Loss: 1.7576029300689697\n",
      "Epoch 34/100, Step 71, Loss: 1.1685045957565308\n",
      "Epoch 34/100, Step 81, Loss: 1.919554352760315\n",
      "Epoch 34/100, Step 91, Loss: 1.7497795820236206\n",
      "Epoch 34/100, Step 101, Loss: 1.27413010597229\n",
      "Epoch 34/100, Step 111, Loss: 1.1474477052688599\n",
      "Epoch 34/100, Step 121, Loss: 1.470900535583496\n",
      "Epoch 34/100, Step 131, Loss: 0.8339999914169312\n",
      "Epoch 34/100, Step 141, Loss: 1.643322467803955\n",
      "Epoch 34/100, Step 151, Loss: 1.6410257816314697\n",
      "Epoch 34/100, Step 161, Loss: 1.2752467393875122\n",
      "Epoch 34/100, Step 171, Loss: 1.5869355201721191\n",
      "Epoch 34/100, Step 181, Loss: 1.325048804283142\n",
      "Epoch 34/100, Step 191, Loss: 1.3191615343093872\n",
      "Epoch 34/100, Step 201, Loss: 2.080209732055664\n",
      "Epoch 34/100, Step 211, Loss: 1.433681845664978\n",
      "Epoch 34/100, Step 221, Loss: 1.6331920623779297\n",
      "Epoch 34/100, Step 231, Loss: 1.49171781539917\n",
      "Epoch 34/100, Step 241, Loss: 1.1587376594543457\n",
      "Epoch 34/100, Step 251, Loss: 1.3374956846237183\n",
      "Epoch 34/100, Step 261, Loss: 1.702882170677185\n",
      "Epoch 34/100, Step 271, Loss: 1.4701592922210693\n",
      "Epoch 34/100, Step 281, Loss: 1.0743688344955444\n",
      "Epoch 34/100, Step 291, Loss: 1.5644471645355225\n",
      "Epoch 34/100, Step 301, Loss: 1.0313681364059448\n",
      "Epoch 34/100, Step 311, Loss: 1.5257563591003418\n",
      "Epoch 34/100, Step 321, Loss: 1.1043305397033691\n",
      "Epoch 34/100, Step 331, Loss: 1.6259510517120361\n",
      "Epoch 34/100, Step 341, Loss: 0.9283461570739746\n",
      "Epoch 34/100, Step 351, Loss: 1.8240606784820557\n",
      "Epoch 34/100, Step 361, Loss: 1.6551921367645264\n",
      "Epoch 35/100, Step 1, Loss: 1.2224255800247192\n",
      "Epoch 35/100, Step 11, Loss: 1.4999217987060547\n",
      "Epoch 35/100, Step 21, Loss: 1.4852604866027832\n",
      "Epoch 35/100, Step 31, Loss: 1.1329102516174316\n",
      "Epoch 35/100, Step 41, Loss: 1.1359513998031616\n",
      "Epoch 35/100, Step 51, Loss: 1.2843639850616455\n",
      "Epoch 35/100, Step 61, Loss: 1.363174319267273\n",
      "Epoch 35/100, Step 71, Loss: 1.6744977235794067\n",
      "Epoch 35/100, Step 81, Loss: 1.1736334562301636\n",
      "Epoch 35/100, Step 91, Loss: 1.3926968574523926\n",
      "Epoch 35/100, Step 101, Loss: 1.741862177848816\n",
      "Epoch 35/100, Step 111, Loss: 1.3645236492156982\n",
      "Epoch 35/100, Step 121, Loss: 1.2285380363464355\n",
      "Epoch 35/100, Step 131, Loss: 0.862271249294281\n",
      "Epoch 35/100, Step 141, Loss: 1.4306458234786987\n",
      "Epoch 35/100, Step 151, Loss: 1.2700393199920654\n",
      "Epoch 35/100, Step 161, Loss: 1.6017346382141113\n",
      "Epoch 35/100, Step 171, Loss: 1.3678191900253296\n",
      "Epoch 35/100, Step 181, Loss: 1.5410759449005127\n",
      "Epoch 35/100, Step 191, Loss: 1.0203553438186646\n",
      "Epoch 35/100, Step 201, Loss: 1.2642614841461182\n",
      "Epoch 35/100, Step 211, Loss: 1.7418935298919678\n",
      "Epoch 35/100, Step 221, Loss: 1.6538599729537964\n",
      "Epoch 35/100, Step 231, Loss: 1.3107237815856934\n",
      "Epoch 35/100, Step 241, Loss: 1.3398743867874146\n",
      "Epoch 35/100, Step 251, Loss: 1.4303734302520752\n",
      "Epoch 35/100, Step 261, Loss: 1.2895879745483398\n",
      "Epoch 35/100, Step 271, Loss: 1.2597687244415283\n",
      "Epoch 35/100, Step 281, Loss: 2.0747127532958984\n",
      "Epoch 35/100, Step 291, Loss: 1.1638761758804321\n",
      "Epoch 35/100, Step 301, Loss: 1.2308058738708496\n",
      "Epoch 35/100, Step 311, Loss: 0.9975086450576782\n",
      "Epoch 35/100, Step 321, Loss: 1.6816354990005493\n",
      "Epoch 35/100, Step 331, Loss: 1.772884488105774\n",
      "Epoch 35/100, Step 341, Loss: 1.7199441194534302\n",
      "Epoch 35/100, Step 351, Loss: 1.3902018070220947\n",
      "Epoch 35/100, Step 361, Loss: 1.2662742137908936\n",
      "Epoch 36/100, Step 1, Loss: 0.922700047492981\n",
      "Epoch 36/100, Step 11, Loss: 1.1588984727859497\n",
      "Epoch 36/100, Step 21, Loss: 1.1386011838912964\n",
      "Epoch 36/100, Step 31, Loss: 1.7808606624603271\n",
      "Epoch 36/100, Step 41, Loss: 1.283115267753601\n",
      "Epoch 36/100, Step 51, Loss: 0.7948684692382812\n",
      "Epoch 36/100, Step 61, Loss: 1.828033208847046\n",
      "Epoch 36/100, Step 71, Loss: 1.2574771642684937\n",
      "Epoch 36/100, Step 81, Loss: 1.239113688468933\n",
      "Epoch 36/100, Step 91, Loss: 0.9648826718330383\n",
      "Epoch 36/100, Step 101, Loss: 0.8589377999305725\n",
      "Epoch 36/100, Step 111, Loss: 1.029921531677246\n",
      "Epoch 36/100, Step 121, Loss: 1.034881830215454\n",
      "Epoch 36/100, Step 131, Loss: 1.2094095945358276\n",
      "Epoch 36/100, Step 141, Loss: 1.2076672315597534\n",
      "Epoch 36/100, Step 151, Loss: 1.1095902919769287\n",
      "Epoch 36/100, Step 161, Loss: 1.3265174627304077\n",
      "Epoch 36/100, Step 171, Loss: 1.1942167282104492\n",
      "Epoch 36/100, Step 181, Loss: 1.2318781614303589\n",
      "Epoch 36/100, Step 191, Loss: 1.5253844261169434\n",
      "Epoch 36/100, Step 201, Loss: 1.2012547254562378\n",
      "Epoch 36/100, Step 211, Loss: 1.4393017292022705\n",
      "Epoch 36/100, Step 221, Loss: 0.9843242764472961\n",
      "Epoch 36/100, Step 231, Loss: 1.1007603406906128\n",
      "Epoch 36/100, Step 241, Loss: 1.2697118520736694\n",
      "Epoch 36/100, Step 251, Loss: 1.5916738510131836\n",
      "Epoch 36/100, Step 261, Loss: 1.4121863842010498\n",
      "Epoch 36/100, Step 271, Loss: 0.6810335516929626\n",
      "Epoch 36/100, Step 281, Loss: 1.088881015777588\n",
      "Epoch 36/100, Step 291, Loss: 1.4507012367248535\n",
      "Epoch 36/100, Step 301, Loss: 0.9237009882926941\n",
      "Epoch 36/100, Step 311, Loss: 1.3650808334350586\n",
      "Epoch 36/100, Step 321, Loss: 1.4174154996871948\n",
      "Epoch 36/100, Step 331, Loss: 0.9431614279747009\n",
      "Epoch 36/100, Step 341, Loss: 1.2406402826309204\n",
      "Epoch 36/100, Step 351, Loss: 1.2277933359146118\n",
      "Epoch 36/100, Step 361, Loss: 1.4767378568649292\n",
      "Epoch 37/100, Step 1, Loss: 1.510591983795166\n",
      "Epoch 37/100, Step 11, Loss: 1.2427016496658325\n",
      "Epoch 37/100, Step 21, Loss: 0.8608613610267639\n",
      "Epoch 37/100, Step 31, Loss: 1.787645936012268\n",
      "Epoch 37/100, Step 41, Loss: 0.6658248901367188\n",
      "Epoch 37/100, Step 51, Loss: 1.5024762153625488\n",
      "Epoch 37/100, Step 61, Loss: 1.0015759468078613\n",
      "Epoch 37/100, Step 71, Loss: 1.2824054956436157\n",
      "Epoch 37/100, Step 81, Loss: 1.8367700576782227\n",
      "Epoch 37/100, Step 91, Loss: 0.8617636561393738\n",
      "Epoch 37/100, Step 101, Loss: 1.0603357553482056\n",
      "Epoch 37/100, Step 111, Loss: 1.0113461017608643\n",
      "Epoch 37/100, Step 121, Loss: 0.9239053130149841\n",
      "Epoch 37/100, Step 131, Loss: 1.3617228269577026\n",
      "Epoch 37/100, Step 141, Loss: 1.1870083808898926\n",
      "Epoch 37/100, Step 151, Loss: 1.376193881034851\n",
      "Epoch 37/100, Step 161, Loss: 1.0111799240112305\n",
      "Epoch 37/100, Step 171, Loss: 0.9519787430763245\n",
      "Epoch 37/100, Step 181, Loss: 0.9341022372245789\n",
      "Epoch 37/100, Step 191, Loss: 1.521820068359375\n",
      "Epoch 37/100, Step 201, Loss: 1.4651678800582886\n",
      "Epoch 37/100, Step 211, Loss: 0.9324809312820435\n",
      "Epoch 37/100, Step 221, Loss: 0.7235279679298401\n",
      "Epoch 37/100, Step 231, Loss: 1.0060361623764038\n",
      "Epoch 37/100, Step 241, Loss: 1.0705070495605469\n",
      "Epoch 37/100, Step 251, Loss: 0.8717263340950012\n",
      "Epoch 37/100, Step 261, Loss: 1.6231459379196167\n",
      "Epoch 37/100, Step 271, Loss: 1.1657016277313232\n",
      "Epoch 37/100, Step 281, Loss: 0.809456467628479\n",
      "Epoch 37/100, Step 291, Loss: 1.037263035774231\n",
      "Epoch 37/100, Step 301, Loss: 1.432122826576233\n",
      "Epoch 37/100, Step 311, Loss: 1.3042869567871094\n",
      "Epoch 37/100, Step 321, Loss: 0.9699854850769043\n",
      "Epoch 37/100, Step 331, Loss: 0.6894330382347107\n",
      "Epoch 37/100, Step 341, Loss: 1.6559101343154907\n",
      "Epoch 37/100, Step 351, Loss: 1.1337603330612183\n",
      "Epoch 37/100, Step 361, Loss: 1.5083400011062622\n",
      "Epoch 38/100, Step 1, Loss: 1.289491891860962\n",
      "Epoch 38/100, Step 11, Loss: 1.0568515062332153\n",
      "Epoch 38/100, Step 21, Loss: 1.5392385721206665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100, Step 31, Loss: 1.3107028007507324\n",
      "Epoch 38/100, Step 41, Loss: 1.0696032047271729\n",
      "Epoch 38/100, Step 51, Loss: 1.4279401302337646\n",
      "Epoch 38/100, Step 61, Loss: 0.8846351504325867\n",
      "Epoch 38/100, Step 71, Loss: 1.5099462270736694\n",
      "Epoch 38/100, Step 81, Loss: 0.6084270477294922\n",
      "Epoch 38/100, Step 91, Loss: 0.7251667380332947\n",
      "Epoch 38/100, Step 101, Loss: 0.7757107615470886\n",
      "Epoch 38/100, Step 111, Loss: 0.9137650728225708\n",
      "Epoch 38/100, Step 121, Loss: 1.3095182180404663\n",
      "Epoch 38/100, Step 131, Loss: 1.244731068611145\n",
      "Epoch 38/100, Step 141, Loss: 1.0465623140335083\n",
      "Epoch 38/100, Step 151, Loss: 1.660347819328308\n",
      "Epoch 38/100, Step 161, Loss: 0.8161086440086365\n",
      "Epoch 38/100, Step 171, Loss: 0.7365796566009521\n",
      "Epoch 38/100, Step 181, Loss: 1.2606785297393799\n",
      "Epoch 38/100, Step 191, Loss: 1.6459550857543945\n",
      "Epoch 38/100, Step 201, Loss: 1.337121605873108\n",
      "Epoch 38/100, Step 211, Loss: 1.0109496116638184\n",
      "Epoch 38/100, Step 221, Loss: 0.8966121673583984\n",
      "Epoch 38/100, Step 231, Loss: 1.0853644609451294\n",
      "Epoch 38/100, Step 241, Loss: 0.6737610101699829\n",
      "Epoch 38/100, Step 251, Loss: 0.5644132494926453\n",
      "Epoch 38/100, Step 261, Loss: 0.631561815738678\n",
      "Epoch 38/100, Step 271, Loss: 1.255703330039978\n",
      "Epoch 38/100, Step 281, Loss: 1.073652744293213\n",
      "Epoch 38/100, Step 291, Loss: 1.2231941223144531\n",
      "Epoch 38/100, Step 301, Loss: 0.8335124850273132\n",
      "Epoch 38/100, Step 311, Loss: 1.1837373971939087\n",
      "Epoch 38/100, Step 321, Loss: 1.3980443477630615\n",
      "Epoch 38/100, Step 331, Loss: 0.945079505443573\n",
      "Epoch 38/100, Step 341, Loss: 1.1379040479660034\n",
      "Epoch 38/100, Step 351, Loss: 1.4438822269439697\n",
      "Epoch 38/100, Step 361, Loss: 1.1399251222610474\n",
      "Epoch 39/100, Step 1, Loss: 0.8247609734535217\n",
      "Epoch 39/100, Step 11, Loss: 1.2728172540664673\n",
      "Epoch 39/100, Step 21, Loss: 0.8905503749847412\n",
      "Epoch 39/100, Step 31, Loss: 0.9574402570724487\n",
      "Epoch 39/100, Step 41, Loss: 0.7982752323150635\n",
      "Epoch 39/100, Step 51, Loss: 0.9570566415786743\n",
      "Epoch 39/100, Step 61, Loss: 0.967866837978363\n",
      "Epoch 39/100, Step 71, Loss: 0.9655282497406006\n",
      "Epoch 39/100, Step 81, Loss: 0.8338972330093384\n",
      "Epoch 39/100, Step 91, Loss: 1.1245943307876587\n",
      "Epoch 39/100, Step 101, Loss: 1.1091080904006958\n",
      "Epoch 39/100, Step 111, Loss: 0.574380099773407\n",
      "Epoch 39/100, Step 121, Loss: 0.5898435115814209\n",
      "Epoch 39/100, Step 131, Loss: 0.722534716129303\n",
      "Epoch 39/100, Step 141, Loss: 1.0723077058792114\n",
      "Epoch 39/100, Step 151, Loss: 1.0392036437988281\n",
      "Epoch 39/100, Step 161, Loss: 1.2010174989700317\n",
      "Epoch 39/100, Step 171, Loss: 0.40118756890296936\n",
      "Epoch 39/100, Step 181, Loss: 1.233771800994873\n",
      "Epoch 39/100, Step 191, Loss: 1.4752639532089233\n",
      "Epoch 39/100, Step 201, Loss: 1.162358045578003\n",
      "Epoch 39/100, Step 211, Loss: 0.8424060344696045\n",
      "Epoch 39/100, Step 221, Loss: 0.8919889330863953\n",
      "Epoch 39/100, Step 231, Loss: 0.9276944994926453\n",
      "Epoch 39/100, Step 241, Loss: 1.2169581651687622\n",
      "Epoch 39/100, Step 251, Loss: 1.0417916774749756\n",
      "Epoch 39/100, Step 261, Loss: 0.8506338596343994\n",
      "Epoch 39/100, Step 271, Loss: 0.8333173394203186\n",
      "Epoch 39/100, Step 281, Loss: 0.908191978931427\n",
      "Epoch 39/100, Step 291, Loss: 1.0066648721694946\n",
      "Epoch 39/100, Step 301, Loss: 1.0592917203903198\n",
      "Epoch 39/100, Step 311, Loss: 1.0908830165863037\n",
      "Epoch 39/100, Step 321, Loss: 0.9780926704406738\n",
      "Epoch 39/100, Step 331, Loss: 0.8280491232872009\n",
      "Epoch 39/100, Step 341, Loss: 1.5765619277954102\n",
      "Epoch 39/100, Step 351, Loss: 1.1890497207641602\n",
      "Epoch 39/100, Step 361, Loss: 1.091801404953003\n",
      "Epoch 40/100, Step 1, Loss: 1.4066179990768433\n",
      "Epoch 40/100, Step 11, Loss: 0.8677601218223572\n",
      "Epoch 40/100, Step 21, Loss: 0.7470813393592834\n",
      "Epoch 40/100, Step 31, Loss: 0.5293460488319397\n",
      "Epoch 40/100, Step 41, Loss: 0.8075080513954163\n",
      "Epoch 40/100, Step 51, Loss: 1.0673456192016602\n",
      "Epoch 40/100, Step 61, Loss: 0.8192208409309387\n",
      "Epoch 40/100, Step 71, Loss: 0.5631096363067627\n",
      "Epoch 40/100, Step 81, Loss: 0.8901628255844116\n",
      "Epoch 40/100, Step 91, Loss: 0.8534407615661621\n",
      "Epoch 40/100, Step 101, Loss: 1.3395262956619263\n",
      "Epoch 40/100, Step 111, Loss: 0.7828337550163269\n",
      "Epoch 40/100, Step 121, Loss: 0.7312643527984619\n",
      "Epoch 40/100, Step 131, Loss: 1.1456681489944458\n",
      "Epoch 40/100, Step 141, Loss: 1.4035805463790894\n",
      "Epoch 40/100, Step 151, Loss: 0.7745729684829712\n",
      "Epoch 40/100, Step 161, Loss: 0.9414749145507812\n",
      "Epoch 40/100, Step 171, Loss: 0.8702512979507446\n",
      "Epoch 40/100, Step 181, Loss: 1.127331018447876\n",
      "Epoch 40/100, Step 191, Loss: 0.9654657244682312\n",
      "Epoch 40/100, Step 201, Loss: 1.2126734256744385\n",
      "Epoch 40/100, Step 211, Loss: 0.9474025964736938\n",
      "Epoch 40/100, Step 221, Loss: 1.1274199485778809\n",
      "Epoch 40/100, Step 231, Loss: 1.015153169631958\n",
      "Epoch 40/100, Step 241, Loss: 1.3148609399795532\n",
      "Epoch 40/100, Step 251, Loss: 1.0872814655303955\n",
      "Epoch 40/100, Step 261, Loss: 1.3573867082595825\n",
      "Epoch 40/100, Step 271, Loss: 1.0428590774536133\n",
      "Epoch 40/100, Step 281, Loss: 0.7768330574035645\n",
      "Epoch 40/100, Step 291, Loss: 1.2388629913330078\n",
      "Epoch 40/100, Step 301, Loss: 0.8293740153312683\n",
      "Epoch 40/100, Step 311, Loss: 1.0965784788131714\n",
      "Epoch 40/100, Step 321, Loss: 1.3548400402069092\n",
      "Epoch 40/100, Step 331, Loss: 1.135185956954956\n",
      "Epoch 40/100, Step 341, Loss: 0.7328488230705261\n",
      "Epoch 40/100, Step 351, Loss: 0.9122079610824585\n",
      "Epoch 40/100, Step 361, Loss: 1.075192928314209\n",
      "Epoch 41/100, Step 1, Loss: 0.5690178871154785\n",
      "Epoch 41/100, Step 11, Loss: 0.7573502063751221\n",
      "Epoch 41/100, Step 21, Loss: 1.0929830074310303\n",
      "Epoch 41/100, Step 31, Loss: 1.8728256225585938\n",
      "Epoch 41/100, Step 41, Loss: 1.1412901878356934\n",
      "Epoch 41/100, Step 51, Loss: 0.5036228895187378\n",
      "Epoch 41/100, Step 61, Loss: 0.6966030597686768\n",
      "Epoch 41/100, Step 71, Loss: 0.8568914532661438\n",
      "Epoch 41/100, Step 81, Loss: 0.8505414128303528\n",
      "Epoch 41/100, Step 91, Loss: 1.2756116390228271\n",
      "Epoch 41/100, Step 101, Loss: 1.5231404304504395\n",
      "Epoch 41/100, Step 111, Loss: 0.8293988108634949\n",
      "Epoch 41/100, Step 121, Loss: 0.947999119758606\n",
      "Epoch 41/100, Step 131, Loss: 1.1556967496871948\n",
      "Epoch 41/100, Step 141, Loss: 0.7750157117843628\n",
      "Epoch 41/100, Step 151, Loss: 0.8618330359458923\n",
      "Epoch 41/100, Step 161, Loss: 0.9136614799499512\n",
      "Epoch 41/100, Step 171, Loss: 1.0199730396270752\n",
      "Epoch 41/100, Step 181, Loss: 0.8161773681640625\n",
      "Epoch 41/100, Step 191, Loss: 0.642395555973053\n",
      "Epoch 41/100, Step 201, Loss: 0.4858284294605255\n",
      "Epoch 41/100, Step 211, Loss: 1.023353099822998\n",
      "Epoch 41/100, Step 221, Loss: 1.2198084592819214\n",
      "Epoch 41/100, Step 231, Loss: 1.2661964893341064\n",
      "Epoch 41/100, Step 241, Loss: 0.7634487152099609\n",
      "Epoch 41/100, Step 251, Loss: 0.9110606908798218\n",
      "Epoch 41/100, Step 261, Loss: 0.8376927971839905\n",
      "Epoch 41/100, Step 271, Loss: 0.7174652218818665\n",
      "Epoch 41/100, Step 281, Loss: 1.0325382947921753\n",
      "Epoch 41/100, Step 291, Loss: 0.9711839556694031\n",
      "Epoch 41/100, Step 301, Loss: 0.9720584750175476\n",
      "Epoch 41/100, Step 311, Loss: 0.7030702233314514\n",
      "Epoch 41/100, Step 321, Loss: 0.8005794286727905\n",
      "Epoch 41/100, Step 331, Loss: 0.6817757487297058\n",
      "Epoch 41/100, Step 341, Loss: 0.9282718300819397\n",
      "Epoch 41/100, Step 351, Loss: 1.2833207845687866\n",
      "Epoch 41/100, Step 361, Loss: 0.8007521033287048\n",
      "Epoch 42/100, Step 1, Loss: 1.10328209400177\n",
      "Epoch 42/100, Step 11, Loss: 0.8239220976829529\n",
      "Epoch 42/100, Step 21, Loss: 0.6839110255241394\n",
      "Epoch 42/100, Step 31, Loss: 1.394325613975525\n",
      "Epoch 42/100, Step 41, Loss: 0.8368028998374939\n",
      "Epoch 42/100, Step 51, Loss: 0.6200692057609558\n",
      "Epoch 42/100, Step 61, Loss: 0.7296520471572876\n",
      "Epoch 42/100, Step 71, Loss: 0.7767855525016785\n",
      "Epoch 42/100, Step 81, Loss: 1.0394585132598877\n",
      "Epoch 42/100, Step 91, Loss: 0.9295631051063538\n",
      "Epoch 42/100, Step 101, Loss: 1.1953147649765015\n",
      "Epoch 42/100, Step 111, Loss: 1.1353927850723267\n",
      "Epoch 42/100, Step 121, Loss: 0.8175815343856812\n",
      "Epoch 42/100, Step 131, Loss: 1.3064342737197876\n",
      "Epoch 42/100, Step 141, Loss: 0.7457745671272278\n",
      "Epoch 42/100, Step 151, Loss: 1.3139538764953613\n",
      "Epoch 42/100, Step 161, Loss: 0.7329375147819519\n",
      "Epoch 42/100, Step 171, Loss: 0.8082084655761719\n",
      "Epoch 42/100, Step 181, Loss: 0.9255571365356445\n",
      "Epoch 42/100, Step 191, Loss: 0.8615537881851196\n",
      "Epoch 42/100, Step 201, Loss: 0.6165916323661804\n",
      "Epoch 42/100, Step 211, Loss: 0.781641960144043\n",
      "Epoch 42/100, Step 221, Loss: 1.1709492206573486\n",
      "Epoch 42/100, Step 231, Loss: 0.6696671843528748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100, Step 241, Loss: 1.1176172494888306\n",
      "Epoch 42/100, Step 251, Loss: 0.7534047365188599\n",
      "Epoch 42/100, Step 261, Loss: 1.5655808448791504\n",
      "Epoch 42/100, Step 271, Loss: 1.1848092079162598\n",
      "Epoch 42/100, Step 281, Loss: 0.8168073296546936\n",
      "Epoch 42/100, Step 291, Loss: 1.0001156330108643\n",
      "Epoch 42/100, Step 301, Loss: 0.9220693111419678\n",
      "Epoch 42/100, Step 311, Loss: 0.8550592064857483\n",
      "Epoch 42/100, Step 321, Loss: 0.9231992363929749\n",
      "Epoch 42/100, Step 331, Loss: 0.6729952692985535\n",
      "Epoch 42/100, Step 341, Loss: 0.7078519463539124\n",
      "Epoch 42/100, Step 351, Loss: 0.8652878403663635\n",
      "Epoch 42/100, Step 361, Loss: 0.9702801704406738\n",
      "Epoch 43/100, Step 1, Loss: 0.48186200857162476\n",
      "Epoch 43/100, Step 11, Loss: 1.2446736097335815\n",
      "Epoch 43/100, Step 21, Loss: 0.6417281627655029\n",
      "Epoch 43/100, Step 31, Loss: 1.3353004455566406\n",
      "Epoch 43/100, Step 41, Loss: 0.8472495079040527\n",
      "Epoch 43/100, Step 51, Loss: 0.9274616837501526\n",
      "Epoch 43/100, Step 61, Loss: 0.6092960238456726\n",
      "Epoch 43/100, Step 71, Loss: 0.6672877073287964\n",
      "Epoch 43/100, Step 81, Loss: 0.9996911287307739\n",
      "Epoch 43/100, Step 91, Loss: 0.7840837836265564\n",
      "Epoch 43/100, Step 101, Loss: 0.6916415095329285\n",
      "Epoch 43/100, Step 111, Loss: 0.7642020583152771\n",
      "Epoch 43/100, Step 121, Loss: 0.5426174998283386\n",
      "Epoch 43/100, Step 131, Loss: 0.5805564522743225\n",
      "Epoch 43/100, Step 141, Loss: 1.3293904066085815\n",
      "Epoch 43/100, Step 151, Loss: 1.0036487579345703\n",
      "Epoch 43/100, Step 161, Loss: 0.6014478206634521\n",
      "Epoch 43/100, Step 171, Loss: 1.0754271745681763\n",
      "Epoch 43/100, Step 181, Loss: 0.9507562518119812\n",
      "Epoch 43/100, Step 191, Loss: 1.3684148788452148\n",
      "Epoch 43/100, Step 201, Loss: 0.8115284442901611\n",
      "Epoch 43/100, Step 211, Loss: 0.9907180666923523\n",
      "Epoch 43/100, Step 221, Loss: 0.8557881116867065\n",
      "Epoch 43/100, Step 231, Loss: 0.5280196666717529\n",
      "Epoch 43/100, Step 241, Loss: 0.8458142876625061\n",
      "Epoch 43/100, Step 251, Loss: 0.5803959965705872\n",
      "Epoch 43/100, Step 261, Loss: 0.9804585576057434\n",
      "Epoch 43/100, Step 271, Loss: 0.625779390335083\n",
      "Epoch 43/100, Step 281, Loss: 0.6096736788749695\n",
      "Epoch 43/100, Step 291, Loss: 0.6365038752555847\n",
      "Epoch 43/100, Step 301, Loss: 1.1764814853668213\n",
      "Epoch 43/100, Step 311, Loss: 1.0874872207641602\n",
      "Epoch 43/100, Step 321, Loss: 0.6728099584579468\n",
      "Epoch 43/100, Step 331, Loss: 0.4504377543926239\n",
      "Epoch 43/100, Step 341, Loss: 0.6043909788131714\n",
      "Epoch 43/100, Step 351, Loss: 0.8254300355911255\n",
      "Epoch 43/100, Step 361, Loss: 1.1195701360702515\n",
      "Epoch 44/100, Step 1, Loss: 0.9208695292472839\n",
      "Epoch 44/100, Step 11, Loss: 1.3581082820892334\n",
      "Epoch 44/100, Step 21, Loss: 0.7518163919448853\n",
      "Epoch 44/100, Step 31, Loss: 0.5841200947761536\n",
      "Epoch 44/100, Step 41, Loss: 0.5230795741081238\n",
      "Epoch 44/100, Step 51, Loss: 0.7092374563217163\n",
      "Epoch 44/100, Step 61, Loss: 0.7362672090530396\n",
      "Epoch 44/100, Step 71, Loss: 1.1109681129455566\n",
      "Epoch 44/100, Step 81, Loss: 0.8032745718955994\n",
      "Epoch 44/100, Step 91, Loss: 0.880474328994751\n",
      "Epoch 44/100, Step 101, Loss: 0.8763232231140137\n",
      "Epoch 44/100, Step 111, Loss: 0.7649223804473877\n",
      "Epoch 44/100, Step 121, Loss: 0.46548211574554443\n",
      "Epoch 44/100, Step 131, Loss: 0.6510190367698669\n",
      "Epoch 44/100, Step 141, Loss: 0.49809524416923523\n",
      "Epoch 44/100, Step 151, Loss: 0.6364260911941528\n",
      "Epoch 44/100, Step 161, Loss: 0.6014756560325623\n",
      "Epoch 44/100, Step 171, Loss: 0.5079083442687988\n",
      "Epoch 44/100, Step 181, Loss: 0.9556933045387268\n",
      "Epoch 44/100, Step 191, Loss: 0.5715645551681519\n",
      "Epoch 44/100, Step 201, Loss: 1.0460987091064453\n",
      "Epoch 44/100, Step 211, Loss: 0.850618839263916\n",
      "Epoch 44/100, Step 221, Loss: 0.9870310425758362\n",
      "Epoch 44/100, Step 231, Loss: 0.5635727643966675\n",
      "Epoch 44/100, Step 241, Loss: 0.7398515343666077\n",
      "Epoch 44/100, Step 251, Loss: 0.9459649324417114\n",
      "Epoch 44/100, Step 261, Loss: 0.6217117309570312\n",
      "Epoch 44/100, Step 271, Loss: 1.2592847347259521\n",
      "Epoch 44/100, Step 281, Loss: 0.7384260296821594\n",
      "Epoch 44/100, Step 291, Loss: 0.7893552780151367\n",
      "Epoch 44/100, Step 301, Loss: 1.2929229736328125\n",
      "Epoch 44/100, Step 311, Loss: 1.1608320474624634\n",
      "Epoch 44/100, Step 321, Loss: 0.9580350518226624\n",
      "Epoch 44/100, Step 331, Loss: 0.7770793437957764\n",
      "Epoch 44/100, Step 341, Loss: 0.8357077240943909\n",
      "Epoch 44/100, Step 351, Loss: 0.6532325148582458\n",
      "Epoch 44/100, Step 361, Loss: 0.8809003829956055\n",
      "Epoch 45/100, Step 1, Loss: 0.5084044337272644\n",
      "Epoch 45/100, Step 11, Loss: 0.7234270572662354\n",
      "Epoch 45/100, Step 21, Loss: 0.6309453248977661\n",
      "Epoch 45/100, Step 31, Loss: 0.5389033555984497\n",
      "Epoch 45/100, Step 41, Loss: 0.540417492389679\n",
      "Epoch 45/100, Step 51, Loss: 1.1994132995605469\n",
      "Epoch 45/100, Step 61, Loss: 0.5875139236450195\n",
      "Epoch 45/100, Step 71, Loss: 0.7894550561904907\n",
      "Epoch 45/100, Step 81, Loss: 0.6954907774925232\n",
      "Epoch 45/100, Step 91, Loss: 0.6369476318359375\n",
      "Epoch 45/100, Step 101, Loss: 0.7678247094154358\n",
      "Epoch 45/100, Step 111, Loss: 0.615934431552887\n",
      "Epoch 45/100, Step 121, Loss: 0.4382191002368927\n",
      "Epoch 45/100, Step 131, Loss: 0.5931578874588013\n",
      "Epoch 45/100, Step 141, Loss: 0.8965153694152832\n",
      "Epoch 45/100, Step 151, Loss: 1.304835557937622\n",
      "Epoch 45/100, Step 161, Loss: 0.6108478903770447\n",
      "Epoch 45/100, Step 171, Loss: 0.6170669198036194\n",
      "Epoch 45/100, Step 181, Loss: 0.6401029825210571\n",
      "Epoch 45/100, Step 191, Loss: 0.895283043384552\n",
      "Epoch 45/100, Step 201, Loss: 0.9596979022026062\n",
      "Epoch 45/100, Step 211, Loss: 0.6007133722305298\n",
      "Epoch 45/100, Step 221, Loss: 0.5448726415634155\n",
      "Epoch 45/100, Step 231, Loss: 0.5447977781295776\n",
      "Epoch 45/100, Step 241, Loss: 0.7995473742485046\n",
      "Epoch 45/100, Step 251, Loss: 0.6764888763427734\n",
      "Epoch 45/100, Step 261, Loss: 0.5661746263504028\n",
      "Epoch 45/100, Step 271, Loss: 0.5845507979393005\n",
      "Epoch 45/100, Step 281, Loss: 0.8318572640419006\n",
      "Epoch 45/100, Step 291, Loss: 0.5478692650794983\n",
      "Epoch 45/100, Step 301, Loss: 1.1443805694580078\n",
      "Epoch 45/100, Step 311, Loss: 0.9062712788581848\n",
      "Epoch 45/100, Step 321, Loss: 1.1339994668960571\n",
      "Epoch 45/100, Step 331, Loss: 0.8988692760467529\n",
      "Epoch 45/100, Step 341, Loss: 1.1926615238189697\n",
      "Epoch 45/100, Step 351, Loss: 1.2324808835983276\n",
      "Epoch 45/100, Step 361, Loss: 0.6264446973800659\n",
      "Epoch 46/100, Step 1, Loss: 0.9205828309059143\n",
      "Epoch 46/100, Step 11, Loss: 0.6451787352561951\n",
      "Epoch 46/100, Step 21, Loss: 0.5930731296539307\n",
      "Epoch 46/100, Step 31, Loss: 0.6849653124809265\n",
      "Epoch 46/100, Step 41, Loss: 0.7816309928894043\n",
      "Epoch 46/100, Step 51, Loss: 1.0407379865646362\n",
      "Epoch 46/100, Step 61, Loss: 0.810904860496521\n",
      "Epoch 46/100, Step 71, Loss: 0.6149854063987732\n",
      "Epoch 46/100, Step 81, Loss: 0.9582667946815491\n",
      "Epoch 46/100, Step 91, Loss: 0.6569655537605286\n",
      "Epoch 46/100, Step 101, Loss: 0.48625457286834717\n",
      "Epoch 46/100, Step 111, Loss: 0.707740068435669\n",
      "Epoch 46/100, Step 121, Loss: 1.2447816133499146\n",
      "Epoch 46/100, Step 131, Loss: 0.5074084401130676\n",
      "Epoch 46/100, Step 141, Loss: 0.5556173920631409\n",
      "Epoch 46/100, Step 151, Loss: 0.5156735777854919\n",
      "Epoch 46/100, Step 161, Loss: 0.8600147366523743\n",
      "Epoch 46/100, Step 171, Loss: 0.7926087975502014\n",
      "Epoch 46/100, Step 181, Loss: 0.7759132385253906\n",
      "Epoch 46/100, Step 191, Loss: 0.7024345397949219\n",
      "Epoch 46/100, Step 201, Loss: 0.5451796054840088\n",
      "Epoch 46/100, Step 211, Loss: 1.1497386693954468\n",
      "Epoch 46/100, Step 221, Loss: 0.8306102156639099\n",
      "Epoch 46/100, Step 231, Loss: 0.8947914838790894\n",
      "Epoch 46/100, Step 241, Loss: 0.822115957736969\n",
      "Epoch 46/100, Step 251, Loss: 0.9919413328170776\n",
      "Epoch 46/100, Step 261, Loss: 0.8018473386764526\n",
      "Epoch 46/100, Step 271, Loss: 1.2275980710983276\n",
      "Epoch 46/100, Step 281, Loss: 0.6823155283927917\n",
      "Epoch 46/100, Step 291, Loss: 0.3997208774089813\n",
      "Epoch 46/100, Step 301, Loss: 0.5109521746635437\n",
      "Epoch 46/100, Step 311, Loss: 0.7739841341972351\n",
      "Epoch 46/100, Step 321, Loss: 0.6358400583267212\n",
      "Epoch 46/100, Step 331, Loss: 0.7719972133636475\n",
      "Epoch 46/100, Step 341, Loss: 0.5486791133880615\n",
      "Epoch 46/100, Step 351, Loss: 0.921688973903656\n",
      "Epoch 46/100, Step 361, Loss: 0.6848264932632446\n",
      "Epoch 47/100, Step 1, Loss: 0.415120005607605\n",
      "Epoch 47/100, Step 11, Loss: 0.49874886870384216\n",
      "Epoch 47/100, Step 21, Loss: 0.7489801645278931\n",
      "Epoch 47/100, Step 31, Loss: 0.5298449993133545\n",
      "Epoch 47/100, Step 41, Loss: 0.658449113368988\n",
      "Epoch 47/100, Step 51, Loss: 0.6556873321533203\n",
      "Epoch 47/100, Step 61, Loss: 0.5649809241294861\n",
      "Epoch 47/100, Step 71, Loss: 0.4466295540332794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100, Step 81, Loss: 0.5214908123016357\n",
      "Epoch 47/100, Step 91, Loss: 0.4791414141654968\n",
      "Epoch 47/100, Step 101, Loss: 0.684723436832428\n",
      "Epoch 47/100, Step 111, Loss: 0.3674705922603607\n",
      "Epoch 47/100, Step 121, Loss: 0.6201081871986389\n",
      "Epoch 47/100, Step 131, Loss: 0.831281304359436\n",
      "Epoch 47/100, Step 141, Loss: 0.7356937527656555\n",
      "Epoch 47/100, Step 151, Loss: 0.7277629375457764\n",
      "Epoch 47/100, Step 161, Loss: 0.3944413959980011\n",
      "Epoch 47/100, Step 171, Loss: 0.6445716619491577\n",
      "Epoch 47/100, Step 181, Loss: 0.7153991460800171\n",
      "Epoch 47/100, Step 191, Loss: 1.0768206119537354\n",
      "Epoch 47/100, Step 201, Loss: 1.0017979145050049\n",
      "Epoch 47/100, Step 211, Loss: 0.420367032289505\n",
      "Epoch 47/100, Step 221, Loss: 0.4356672167778015\n",
      "Epoch 47/100, Step 231, Loss: 0.48660075664520264\n",
      "Epoch 47/100, Step 241, Loss: 0.6296613216400146\n",
      "Epoch 47/100, Step 251, Loss: 0.8472769856452942\n",
      "Epoch 47/100, Step 261, Loss: 0.7212298512458801\n",
      "Epoch 47/100, Step 271, Loss: 0.5089578032493591\n",
      "Epoch 47/100, Step 281, Loss: 0.5648314952850342\n",
      "Epoch 47/100, Step 291, Loss: 0.61414635181427\n",
      "Epoch 47/100, Step 301, Loss: 0.7328292727470398\n",
      "Epoch 47/100, Step 311, Loss: 0.6104125380516052\n",
      "Epoch 47/100, Step 321, Loss: 0.8286668658256531\n",
      "Epoch 47/100, Step 331, Loss: 0.7371307015419006\n",
      "Epoch 47/100, Step 341, Loss: 0.5626774430274963\n",
      "Epoch 47/100, Step 351, Loss: 0.523880124092102\n",
      "Epoch 47/100, Step 361, Loss: 0.6359789371490479\n",
      "Epoch 48/100, Step 1, Loss: 0.41968169808387756\n",
      "Epoch 48/100, Step 11, Loss: 0.7866610288619995\n",
      "Epoch 48/100, Step 21, Loss: 0.48898664116859436\n",
      "Epoch 48/100, Step 31, Loss: 0.4284217655658722\n",
      "Epoch 48/100, Step 41, Loss: 0.6443259716033936\n",
      "Epoch 48/100, Step 51, Loss: 0.432730108499527\n",
      "Epoch 48/100, Step 61, Loss: 0.8555999994277954\n",
      "Epoch 48/100, Step 71, Loss: 0.5275635123252869\n",
      "Epoch 48/100, Step 81, Loss: 0.8649115562438965\n",
      "Epoch 48/100, Step 91, Loss: 0.4975210726261139\n",
      "Epoch 48/100, Step 101, Loss: 0.8820655941963196\n",
      "Epoch 48/100, Step 111, Loss: 0.4068448841571808\n",
      "Epoch 48/100, Step 121, Loss: 0.6139793992042542\n",
      "Epoch 48/100, Step 131, Loss: 0.4692964255809784\n",
      "Epoch 48/100, Step 141, Loss: 0.54217129945755\n",
      "Epoch 48/100, Step 151, Loss: 0.3641152083873749\n",
      "Epoch 48/100, Step 161, Loss: 0.5671791434288025\n",
      "Epoch 48/100, Step 171, Loss: 0.6172040700912476\n",
      "Epoch 48/100, Step 181, Loss: 0.7148131728172302\n",
      "Epoch 48/100, Step 191, Loss: 0.7413758039474487\n",
      "Epoch 48/100, Step 201, Loss: 0.9540905952453613\n",
      "Epoch 48/100, Step 211, Loss: 0.49630501866340637\n",
      "Epoch 48/100, Step 221, Loss: 0.28983160853385925\n",
      "Epoch 48/100, Step 231, Loss: 0.5878762602806091\n",
      "Epoch 48/100, Step 241, Loss: 0.4772820472717285\n",
      "Epoch 48/100, Step 251, Loss: 0.7585302591323853\n",
      "Epoch 48/100, Step 261, Loss: 0.5498714447021484\n",
      "Epoch 48/100, Step 271, Loss: 0.8521289229393005\n",
      "Epoch 48/100, Step 281, Loss: 1.185834527015686\n",
      "Epoch 48/100, Step 291, Loss: 0.6838088631629944\n",
      "Epoch 48/100, Step 301, Loss: 0.48941200971603394\n",
      "Epoch 48/100, Step 311, Loss: 1.1082763671875\n",
      "Epoch 48/100, Step 321, Loss: 0.5090423822402954\n",
      "Epoch 48/100, Step 331, Loss: 0.6107050776481628\n",
      "Epoch 48/100, Step 341, Loss: 0.7132346034049988\n",
      "Epoch 48/100, Step 351, Loss: 0.603382408618927\n",
      "Epoch 48/100, Step 361, Loss: 0.4591448903083801\n",
      "Epoch 49/100, Step 1, Loss: 0.7777993083000183\n",
      "Epoch 49/100, Step 11, Loss: 0.575520396232605\n",
      "Epoch 49/100, Step 21, Loss: 0.373471736907959\n",
      "Epoch 49/100, Step 31, Loss: 0.4782220125198364\n",
      "Epoch 49/100, Step 41, Loss: 0.6074479818344116\n",
      "Epoch 49/100, Step 51, Loss: 0.45075830817222595\n",
      "Epoch 49/100, Step 61, Loss: 0.34723302721977234\n",
      "Epoch 49/100, Step 71, Loss: 0.6489647626876831\n",
      "Epoch 49/100, Step 81, Loss: 0.3631401062011719\n",
      "Epoch 49/100, Step 91, Loss: 0.431153267621994\n",
      "Epoch 49/100, Step 101, Loss: 0.9847326874732971\n",
      "Epoch 49/100, Step 111, Loss: 0.4850945770740509\n",
      "Epoch 49/100, Step 121, Loss: 1.003158450126648\n",
      "Epoch 49/100, Step 131, Loss: 0.6272356510162354\n",
      "Epoch 49/100, Step 141, Loss: 0.6284683346748352\n",
      "Epoch 49/100, Step 151, Loss: 0.7901387214660645\n",
      "Epoch 49/100, Step 161, Loss: 0.44855085015296936\n",
      "Epoch 49/100, Step 171, Loss: 0.348563015460968\n",
      "Epoch 49/100, Step 181, Loss: 0.7679398059844971\n",
      "Epoch 49/100, Step 191, Loss: 0.5272207856178284\n",
      "Epoch 49/100, Step 201, Loss: 0.4433344900608063\n",
      "Epoch 49/100, Step 211, Loss: 1.0402591228485107\n",
      "Epoch 49/100, Step 221, Loss: 0.3443222939968109\n",
      "Epoch 49/100, Step 231, Loss: 0.2575729787349701\n",
      "Epoch 49/100, Step 241, Loss: 0.45331212878227234\n",
      "Epoch 49/100, Step 251, Loss: 0.504982054233551\n",
      "Epoch 49/100, Step 261, Loss: 0.55181884765625\n",
      "Epoch 49/100, Step 271, Loss: 0.5047289133071899\n",
      "Epoch 49/100, Step 281, Loss: 0.545854926109314\n",
      "Epoch 49/100, Step 291, Loss: 0.6081847548484802\n",
      "Epoch 49/100, Step 301, Loss: 0.7654227614402771\n",
      "Epoch 49/100, Step 311, Loss: 1.0393948554992676\n",
      "Epoch 49/100, Step 321, Loss: 0.3132518231868744\n",
      "Epoch 49/100, Step 331, Loss: 1.1051716804504395\n",
      "Epoch 49/100, Step 341, Loss: 0.472360223531723\n",
      "Epoch 49/100, Step 351, Loss: 0.5136510133743286\n",
      "Epoch 49/100, Step 361, Loss: 0.9739291667938232\n",
      "Epoch 50/100, Step 1, Loss: 0.6986446976661682\n",
      "Epoch 50/100, Step 11, Loss: 0.440777063369751\n",
      "Epoch 50/100, Step 21, Loss: 0.48730602860450745\n",
      "Epoch 50/100, Step 31, Loss: 0.749099850654602\n",
      "Epoch 50/100, Step 41, Loss: 0.3532189428806305\n",
      "Epoch 50/100, Step 51, Loss: 0.3223869800567627\n",
      "Epoch 50/100, Step 61, Loss: 0.7036997675895691\n",
      "Epoch 50/100, Step 71, Loss: 0.47302109003067017\n",
      "Epoch 50/100, Step 81, Loss: 0.27926331758499146\n",
      "Epoch 50/100, Step 91, Loss: 0.48358726501464844\n",
      "Epoch 50/100, Step 101, Loss: 0.7796087265014648\n",
      "Epoch 50/100, Step 111, Loss: 0.4008597433567047\n",
      "Epoch 50/100, Step 121, Loss: 0.5636806488037109\n",
      "Epoch 50/100, Step 131, Loss: 0.2859431803226471\n",
      "Epoch 50/100, Step 141, Loss: 0.5676742196083069\n",
      "Epoch 50/100, Step 151, Loss: 0.471940815448761\n",
      "Epoch 50/100, Step 161, Loss: 0.49762389063835144\n",
      "Epoch 50/100, Step 171, Loss: 0.4188063144683838\n",
      "Epoch 50/100, Step 181, Loss: 0.5165151953697205\n",
      "Epoch 50/100, Step 191, Loss: 0.3514203727245331\n",
      "Epoch 50/100, Step 201, Loss: 0.47489944100379944\n",
      "Epoch 50/100, Step 211, Loss: 0.3081481158733368\n",
      "Epoch 50/100, Step 221, Loss: 0.6936172842979431\n",
      "Epoch 50/100, Step 231, Loss: 0.4984923303127289\n",
      "Epoch 50/100, Step 241, Loss: 0.9390174746513367\n",
      "Epoch 50/100, Step 251, Loss: 0.6939315795898438\n",
      "Epoch 50/100, Step 261, Loss: 0.7875047326087952\n",
      "Epoch 50/100, Step 271, Loss: 0.30028173327445984\n",
      "Epoch 50/100, Step 281, Loss: 0.7360607385635376\n",
      "Epoch 50/100, Step 291, Loss: 0.8552011251449585\n",
      "Epoch 50/100, Step 301, Loss: 1.0435051918029785\n",
      "Epoch 50/100, Step 311, Loss: 0.4773207902908325\n",
      "Epoch 50/100, Step 321, Loss: 0.2899419665336609\n",
      "Epoch 50/100, Step 331, Loss: 0.6694946885108948\n",
      "Epoch 50/100, Step 341, Loss: 0.43760454654693604\n",
      "Epoch 50/100, Step 351, Loss: 0.3128432035446167\n",
      "Epoch 50/100, Step 361, Loss: 0.652765154838562\n",
      "Epoch 51/100, Step 1, Loss: 0.5532557368278503\n",
      "Epoch 51/100, Step 11, Loss: 0.4316110908985138\n",
      "Epoch 51/100, Step 21, Loss: 0.49986350536346436\n",
      "Epoch 51/100, Step 31, Loss: 0.4502681493759155\n",
      "Epoch 51/100, Step 41, Loss: 0.37309205532073975\n",
      "Epoch 51/100, Step 51, Loss: 0.5653879046440125\n",
      "Epoch 51/100, Step 61, Loss: 0.3550512194633484\n",
      "Epoch 51/100, Step 71, Loss: 0.22941815853118896\n",
      "Epoch 51/100, Step 81, Loss: 0.5047622919082642\n",
      "Epoch 51/100, Step 91, Loss: 0.25165054202079773\n",
      "Epoch 51/100, Step 101, Loss: 0.3983099162578583\n",
      "Epoch 51/100, Step 111, Loss: 0.4629347622394562\n",
      "Epoch 51/100, Step 121, Loss: 0.6173041462898254\n",
      "Epoch 51/100, Step 131, Loss: 0.42121976613998413\n",
      "Epoch 51/100, Step 141, Loss: 0.6125931143760681\n",
      "Epoch 51/100, Step 151, Loss: 0.8851038813591003\n",
      "Epoch 51/100, Step 161, Loss: 0.5404552817344666\n",
      "Epoch 51/100, Step 171, Loss: 0.6236503720283508\n",
      "Epoch 51/100, Step 181, Loss: 0.8966904878616333\n",
      "Epoch 51/100, Step 191, Loss: 0.31068024039268494\n",
      "Epoch 51/100, Step 201, Loss: 0.2908203601837158\n",
      "Epoch 51/100, Step 211, Loss: 0.43217530846595764\n",
      "Epoch 51/100, Step 221, Loss: 0.4257088303565979\n",
      "Epoch 51/100, Step 231, Loss: 0.5575875639915466\n",
      "Epoch 51/100, Step 241, Loss: 0.7667060494422913\n",
      "Epoch 51/100, Step 251, Loss: 0.36175867915153503\n",
      "Epoch 51/100, Step 261, Loss: 0.5564780831336975\n",
      "Epoch 51/100, Step 271, Loss: 0.5163815021514893\n",
      "Epoch 51/100, Step 281, Loss: 0.4465367794036865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100, Step 291, Loss: 0.6307932138442993\n",
      "Epoch 51/100, Step 301, Loss: 0.975734293460846\n",
      "Epoch 51/100, Step 311, Loss: 0.46180421113967896\n",
      "Epoch 51/100, Step 321, Loss: 0.6560173034667969\n",
      "Epoch 51/100, Step 331, Loss: 0.3325207233428955\n",
      "Epoch 51/100, Step 341, Loss: 0.32646581530570984\n",
      "Epoch 51/100, Step 351, Loss: 0.7416433691978455\n",
      "Epoch 51/100, Step 361, Loss: 0.3166297376155853\n",
      "Epoch 52/100, Step 1, Loss: 0.3893885910511017\n",
      "Epoch 52/100, Step 11, Loss: 0.2623175084590912\n",
      "Epoch 52/100, Step 21, Loss: 0.4982377588748932\n",
      "Epoch 52/100, Step 31, Loss: 0.504220724105835\n",
      "Epoch 52/100, Step 41, Loss: 0.5839739441871643\n",
      "Epoch 52/100, Step 51, Loss: 0.6965935230255127\n",
      "Epoch 52/100, Step 61, Loss: 0.6609988212585449\n",
      "Epoch 52/100, Step 71, Loss: 0.6129423975944519\n",
      "Epoch 52/100, Step 81, Loss: 0.5369455218315125\n",
      "Epoch 52/100, Step 91, Loss: 0.28164538741111755\n",
      "Epoch 52/100, Step 101, Loss: 0.20884393155574799\n",
      "Epoch 52/100, Step 111, Loss: 0.5312389731407166\n",
      "Epoch 52/100, Step 121, Loss: 0.5136070251464844\n",
      "Epoch 52/100, Step 131, Loss: 0.5270086526870728\n",
      "Epoch 52/100, Step 141, Loss: 0.38076451420783997\n",
      "Epoch 52/100, Step 151, Loss: 0.9432189464569092\n",
      "Epoch 52/100, Step 161, Loss: 0.42486241459846497\n",
      "Epoch 52/100, Step 171, Loss: 0.3420827090740204\n",
      "Epoch 52/100, Step 181, Loss: 0.5384959578514099\n",
      "Epoch 52/100, Step 191, Loss: 0.8810946345329285\n",
      "Epoch 52/100, Step 201, Loss: 0.5199745893478394\n",
      "Epoch 52/100, Step 211, Loss: 0.32527005672454834\n",
      "Epoch 52/100, Step 221, Loss: 0.4474180340766907\n",
      "Epoch 52/100, Step 231, Loss: 0.4553983807563782\n",
      "Epoch 52/100, Step 241, Loss: 0.6086735129356384\n",
      "Epoch 52/100, Step 251, Loss: 0.6769633293151855\n",
      "Epoch 52/100, Step 261, Loss: 0.37443581223487854\n",
      "Epoch 52/100, Step 271, Loss: 0.5678448677062988\n",
      "Epoch 52/100, Step 281, Loss: 0.7038794159889221\n",
      "Epoch 52/100, Step 291, Loss: 0.5502871870994568\n",
      "Epoch 52/100, Step 301, Loss: 0.4582197368144989\n",
      "Epoch 52/100, Step 311, Loss: 0.8143728375434875\n",
      "Epoch 52/100, Step 321, Loss: 0.23821137845516205\n",
      "Epoch 52/100, Step 331, Loss: 0.5661579966545105\n",
      "Epoch 52/100, Step 341, Loss: 0.3211989998817444\n",
      "Epoch 52/100, Step 351, Loss: 0.38220492005348206\n",
      "Epoch 52/100, Step 361, Loss: 0.3262690305709839\n",
      "Epoch 53/100, Step 1, Loss: 0.40669307112693787\n",
      "Epoch 53/100, Step 11, Loss: 0.4444999396800995\n",
      "Epoch 53/100, Step 21, Loss: 0.6011476516723633\n",
      "Epoch 53/100, Step 31, Loss: 0.3280327618122101\n",
      "Epoch 53/100, Step 41, Loss: 0.604830265045166\n",
      "Epoch 53/100, Step 51, Loss: 0.3173407316207886\n",
      "Epoch 53/100, Step 61, Loss: 0.2471969872713089\n",
      "Epoch 53/100, Step 71, Loss: 0.44303181767463684\n",
      "Epoch 53/100, Step 81, Loss: 0.35841062664985657\n",
      "Epoch 53/100, Step 91, Loss: 0.5525636672973633\n",
      "Epoch 53/100, Step 101, Loss: 0.5385822653770447\n",
      "Epoch 53/100, Step 111, Loss: 0.39159882068634033\n",
      "Epoch 53/100, Step 121, Loss: 0.29300135374069214\n",
      "Epoch 53/100, Step 131, Loss: 0.28740397095680237\n",
      "Epoch 53/100, Step 141, Loss: 0.4001011848449707\n",
      "Epoch 53/100, Step 151, Loss: 0.3228371739387512\n",
      "Epoch 53/100, Step 161, Loss: 0.24400416016578674\n",
      "Epoch 53/100, Step 171, Loss: 0.2118012011051178\n",
      "Epoch 53/100, Step 181, Loss: 0.31727609038352966\n",
      "Epoch 53/100, Step 191, Loss: 0.25593453645706177\n",
      "Epoch 53/100, Step 201, Loss: 0.580295979976654\n",
      "Epoch 53/100, Step 211, Loss: 0.36077460646629333\n",
      "Epoch 53/100, Step 221, Loss: 0.6246262788772583\n",
      "Epoch 53/100, Step 231, Loss: 0.5405972003936768\n",
      "Epoch 53/100, Step 241, Loss: 0.3979131281375885\n",
      "Epoch 53/100, Step 251, Loss: 0.27190038561820984\n",
      "Epoch 53/100, Step 261, Loss: 0.4114481210708618\n",
      "Epoch 53/100, Step 271, Loss: 0.48727765679359436\n",
      "Epoch 53/100, Step 281, Loss: 0.3651835024356842\n",
      "Epoch 53/100, Step 291, Loss: 0.4210726022720337\n",
      "Epoch 53/100, Step 301, Loss: 0.8107920289039612\n",
      "Epoch 53/100, Step 311, Loss: 0.32710030674934387\n",
      "Epoch 53/100, Step 321, Loss: 0.347364604473114\n",
      "Epoch 53/100, Step 331, Loss: 0.4099178612232208\n",
      "Epoch 53/100, Step 341, Loss: 0.3363332450389862\n",
      "Epoch 53/100, Step 351, Loss: 0.5058699250221252\n",
      "Epoch 53/100, Step 361, Loss: 0.4128817319869995\n",
      "Epoch 54/100, Step 1, Loss: 0.38607317209243774\n",
      "Epoch 54/100, Step 11, Loss: 0.34751614928245544\n",
      "Epoch 54/100, Step 21, Loss: 0.8999064564704895\n",
      "Epoch 54/100, Step 31, Loss: 0.5168531537055969\n",
      "Epoch 54/100, Step 41, Loss: 0.24322247505187988\n",
      "Epoch 54/100, Step 51, Loss: 0.3183457553386688\n",
      "Epoch 54/100, Step 61, Loss: 0.37300142645835876\n",
      "Epoch 54/100, Step 71, Loss: 0.21317525207996368\n",
      "Epoch 54/100, Step 81, Loss: 0.439404159784317\n",
      "Epoch 54/100, Step 91, Loss: 0.30180302262306213\n",
      "Epoch 54/100, Step 101, Loss: 0.5679646730422974\n",
      "Epoch 54/100, Step 111, Loss: 0.30772164463996887\n",
      "Epoch 54/100, Step 121, Loss: 0.2405252903699875\n",
      "Epoch 54/100, Step 131, Loss: 0.41128021478652954\n",
      "Epoch 54/100, Step 141, Loss: 0.6843824982643127\n",
      "Epoch 54/100, Step 151, Loss: 0.4521390199661255\n",
      "Epoch 54/100, Step 161, Loss: 0.34963420033454895\n",
      "Epoch 54/100, Step 171, Loss: 0.28417879343032837\n",
      "Epoch 54/100, Step 181, Loss: 0.32627347111701965\n",
      "Epoch 54/100, Step 191, Loss: 0.2703908383846283\n",
      "Epoch 54/100, Step 201, Loss: 0.6116728186607361\n",
      "Epoch 54/100, Step 211, Loss: 0.5313762426376343\n",
      "Epoch 54/100, Step 221, Loss: 0.37257784605026245\n",
      "Epoch 54/100, Step 231, Loss: 0.3357905447483063\n",
      "Epoch 54/100, Step 241, Loss: 0.5227378010749817\n",
      "Epoch 54/100, Step 251, Loss: 0.26040980219841003\n",
      "Epoch 54/100, Step 261, Loss: 0.36065351963043213\n",
      "Epoch 54/100, Step 271, Loss: 0.22658391296863556\n",
      "Epoch 54/100, Step 281, Loss: 0.3740493059158325\n",
      "Epoch 54/100, Step 291, Loss: 0.33346495032310486\n",
      "Epoch 54/100, Step 301, Loss: 0.32635632157325745\n",
      "Epoch 54/100, Step 311, Loss: 0.47069844603538513\n",
      "Epoch 54/100, Step 321, Loss: 0.32274001836776733\n",
      "Epoch 54/100, Step 331, Loss: 0.38984984159469604\n",
      "Epoch 54/100, Step 341, Loss: 0.5651854276657104\n",
      "Epoch 54/100, Step 351, Loss: 0.29182785749435425\n",
      "Epoch 54/100, Step 361, Loss: 0.30845388770103455\n",
      "Epoch 55/100, Step 1, Loss: 0.5045992732048035\n",
      "Epoch 55/100, Step 11, Loss: 0.43928590416908264\n",
      "Epoch 55/100, Step 21, Loss: 0.31357240676879883\n",
      "Epoch 55/100, Step 31, Loss: 0.19016306102275848\n",
      "Epoch 55/100, Step 41, Loss: 0.3990776538848877\n",
      "Epoch 55/100, Step 51, Loss: 0.7002815008163452\n",
      "Epoch 55/100, Step 61, Loss: 0.33299824595451355\n",
      "Epoch 55/100, Step 71, Loss: 0.4858871102333069\n",
      "Epoch 55/100, Step 81, Loss: 0.3622336685657501\n",
      "Epoch 55/100, Step 91, Loss: 0.3797135353088379\n",
      "Epoch 55/100, Step 101, Loss: 0.4205123484134674\n",
      "Epoch 55/100, Step 111, Loss: 0.8486071228981018\n",
      "Epoch 55/100, Step 121, Loss: 0.6754434108734131\n",
      "Epoch 55/100, Step 131, Loss: 0.21293151378631592\n",
      "Epoch 55/100, Step 141, Loss: 0.530437707901001\n",
      "Epoch 55/100, Step 151, Loss: 0.3860922157764435\n",
      "Epoch 55/100, Step 161, Loss: 0.2830301523208618\n",
      "Epoch 55/100, Step 171, Loss: 0.67811119556427\n",
      "Epoch 55/100, Step 181, Loss: 0.2628805637359619\n",
      "Epoch 55/100, Step 191, Loss: 0.3716915249824524\n",
      "Epoch 55/100, Step 201, Loss: 0.38911136984825134\n",
      "Epoch 55/100, Step 211, Loss: 0.4243045449256897\n",
      "Epoch 55/100, Step 221, Loss: 0.48263150453567505\n",
      "Epoch 55/100, Step 231, Loss: 0.5113065242767334\n",
      "Epoch 55/100, Step 241, Loss: 0.5724900364875793\n",
      "Epoch 55/100, Step 251, Loss: 0.3608804941177368\n",
      "Epoch 55/100, Step 261, Loss: 0.3839285671710968\n",
      "Epoch 55/100, Step 271, Loss: 0.6360146403312683\n",
      "Epoch 55/100, Step 281, Loss: 0.424907922744751\n",
      "Epoch 55/100, Step 291, Loss: 0.5211073160171509\n",
      "Epoch 55/100, Step 301, Loss: 0.4861002564430237\n",
      "Epoch 55/100, Step 311, Loss: 0.3535853624343872\n",
      "Epoch 55/100, Step 321, Loss: 0.48639681935310364\n",
      "Epoch 55/100, Step 331, Loss: 0.1842009574174881\n",
      "Epoch 55/100, Step 341, Loss: 0.31498631834983826\n",
      "Epoch 55/100, Step 351, Loss: 0.4023110568523407\n",
      "Epoch 55/100, Step 361, Loss: 0.3049384653568268\n",
      "Epoch 56/100, Step 1, Loss: 0.6834105849266052\n",
      "Epoch 56/100, Step 11, Loss: 0.3271346688270569\n",
      "Epoch 56/100, Step 21, Loss: 0.1784195899963379\n",
      "Epoch 56/100, Step 31, Loss: 0.4214925169944763\n",
      "Epoch 56/100, Step 41, Loss: 0.3119191825389862\n",
      "Epoch 56/100, Step 51, Loss: 0.28942999243736267\n",
      "Epoch 56/100, Step 61, Loss: 0.3207763135433197\n",
      "Epoch 56/100, Step 71, Loss: 0.19805790483951569\n",
      "Epoch 56/100, Step 81, Loss: 0.5423173904418945\n",
      "Epoch 56/100, Step 91, Loss: 0.3944273889064789\n",
      "Epoch 56/100, Step 101, Loss: 0.22747471928596497\n",
      "Epoch 56/100, Step 111, Loss: 0.25613316893577576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100, Step 121, Loss: 0.46336251497268677\n",
      "Epoch 56/100, Step 131, Loss: 0.31905797123908997\n",
      "Epoch 56/100, Step 141, Loss: 0.19205188751220703\n",
      "Epoch 56/100, Step 151, Loss: 0.4704340398311615\n",
      "Epoch 56/100, Step 161, Loss: 0.382571280002594\n",
      "Epoch 56/100, Step 171, Loss: 0.5398601293563843\n",
      "Epoch 56/100, Step 181, Loss: 0.35174182057380676\n",
      "Epoch 56/100, Step 191, Loss: 0.3943469524383545\n",
      "Epoch 56/100, Step 201, Loss: 0.5408291220664978\n",
      "Epoch 56/100, Step 211, Loss: 0.3777329921722412\n",
      "Epoch 56/100, Step 221, Loss: 0.37811213731765747\n",
      "Epoch 56/100, Step 231, Loss: 0.6734633445739746\n",
      "Epoch 56/100, Step 241, Loss: 0.5866354703903198\n",
      "Epoch 56/100, Step 251, Loss: 0.6304921507835388\n",
      "Epoch 56/100, Step 261, Loss: 0.6778460144996643\n",
      "Epoch 56/100, Step 271, Loss: 0.21828483045101166\n",
      "Epoch 56/100, Step 281, Loss: 0.34425339102745056\n",
      "Epoch 56/100, Step 291, Loss: 0.32867830991744995\n",
      "Epoch 56/100, Step 301, Loss: 0.1994665414094925\n",
      "Epoch 56/100, Step 311, Loss: 0.2519721984863281\n",
      "Epoch 56/100, Step 321, Loss: 0.3815247714519501\n",
      "Epoch 56/100, Step 331, Loss: 0.47033846378326416\n",
      "Epoch 56/100, Step 341, Loss: 0.3009048402309418\n",
      "Epoch 56/100, Step 351, Loss: 0.42352011799812317\n",
      "Epoch 56/100, Step 361, Loss: 0.32798075675964355\n",
      "Epoch 57/100, Step 1, Loss: 0.3337884843349457\n",
      "Epoch 57/100, Step 11, Loss: 0.4439471960067749\n",
      "Epoch 57/100, Step 21, Loss: 0.28860196471214294\n",
      "Epoch 57/100, Step 31, Loss: 0.37864458560943604\n",
      "Epoch 57/100, Step 41, Loss: 0.47474855184555054\n",
      "Epoch 57/100, Step 51, Loss: 0.34151023626327515\n",
      "Epoch 57/100, Step 61, Loss: 0.2270950824022293\n",
      "Epoch 57/100, Step 71, Loss: 0.2821868062019348\n",
      "Epoch 57/100, Step 81, Loss: 0.26144036650657654\n",
      "Epoch 57/100, Step 91, Loss: 0.2584936022758484\n",
      "Epoch 57/100, Step 101, Loss: 0.1959238052368164\n",
      "Epoch 57/100, Step 111, Loss: 0.2688939869403839\n",
      "Epoch 57/100, Step 121, Loss: 0.2221645563840866\n",
      "Epoch 57/100, Step 131, Loss: 0.32836639881134033\n",
      "Epoch 57/100, Step 141, Loss: 0.21366700530052185\n",
      "Epoch 57/100, Step 151, Loss: 0.26792481541633606\n",
      "Epoch 57/100, Step 161, Loss: 0.26980602741241455\n",
      "Epoch 57/100, Step 171, Loss: 0.2731960713863373\n",
      "Epoch 57/100, Step 181, Loss: 0.449405699968338\n",
      "Epoch 57/100, Step 191, Loss: 0.3085278868675232\n",
      "Epoch 57/100, Step 201, Loss: 0.37430503964424133\n",
      "Epoch 57/100, Step 211, Loss: 0.17270052433013916\n",
      "Epoch 57/100, Step 221, Loss: 0.3279610872268677\n",
      "Epoch 57/100, Step 231, Loss: 0.32047194242477417\n",
      "Epoch 57/100, Step 241, Loss: 0.3818676769733429\n",
      "Epoch 57/100, Step 251, Loss: 0.40065231919288635\n",
      "Epoch 57/100, Step 261, Loss: 0.20052647590637207\n",
      "Epoch 57/100, Step 271, Loss: 0.4249117970466614\n",
      "Epoch 57/100, Step 281, Loss: 0.4517400562763214\n",
      "Epoch 57/100, Step 291, Loss: 0.4083087742328644\n",
      "Epoch 57/100, Step 301, Loss: 0.22115939855575562\n",
      "Epoch 57/100, Step 311, Loss: 0.4645252227783203\n",
      "Epoch 57/100, Step 321, Loss: 0.43414461612701416\n",
      "Epoch 57/100, Step 331, Loss: 0.4314839839935303\n",
      "Epoch 57/100, Step 341, Loss: 0.22077664732933044\n",
      "Epoch 57/100, Step 351, Loss: 0.3398785889148712\n",
      "Epoch 57/100, Step 361, Loss: 0.36564987897872925\n",
      "Epoch 58/100, Step 1, Loss: 0.1993362307548523\n",
      "Epoch 58/100, Step 11, Loss: 0.20628975331783295\n",
      "Epoch 58/100, Step 21, Loss: 0.25325465202331543\n",
      "Epoch 58/100, Step 31, Loss: 0.3582676947116852\n",
      "Epoch 58/100, Step 41, Loss: 0.5721413493156433\n",
      "Epoch 58/100, Step 51, Loss: 0.3021133542060852\n",
      "Epoch 58/100, Step 61, Loss: 0.20379063487052917\n",
      "Epoch 58/100, Step 71, Loss: 0.331929087638855\n",
      "Epoch 58/100, Step 81, Loss: 0.32453519105911255\n",
      "Epoch 58/100, Step 91, Loss: 0.5901802778244019\n",
      "Epoch 58/100, Step 101, Loss: 0.21116843819618225\n",
      "Epoch 58/100, Step 111, Loss: 0.30716878175735474\n",
      "Epoch 58/100, Step 121, Loss: 0.44554436206817627\n",
      "Epoch 58/100, Step 131, Loss: 0.29882514476776123\n",
      "Epoch 58/100, Step 141, Loss: 0.3800176978111267\n",
      "Epoch 58/100, Step 151, Loss: 0.31035369634628296\n",
      "Epoch 58/100, Step 161, Loss: 0.24647043645381927\n",
      "Epoch 58/100, Step 171, Loss: 0.24760612845420837\n",
      "Epoch 58/100, Step 181, Loss: 0.2757089138031006\n",
      "Epoch 58/100, Step 191, Loss: 0.3532213866710663\n",
      "Epoch 58/100, Step 201, Loss: 0.3879150152206421\n",
      "Epoch 58/100, Step 211, Loss: 0.3143581449985504\n",
      "Epoch 58/100, Step 221, Loss: 0.5391639471054077\n",
      "Epoch 58/100, Step 231, Loss: 0.2152816653251648\n",
      "Epoch 58/100, Step 241, Loss: 0.22074702382087708\n",
      "Epoch 58/100, Step 251, Loss: 0.3325752019882202\n",
      "Epoch 58/100, Step 261, Loss: 0.2515309154987335\n",
      "Epoch 58/100, Step 271, Loss: 0.448666512966156\n",
      "Epoch 58/100, Step 281, Loss: 0.1866438090801239\n",
      "Epoch 58/100, Step 291, Loss: 0.29205313324928284\n",
      "Epoch 58/100, Step 301, Loss: 0.24302339553833008\n",
      "Epoch 58/100, Step 311, Loss: 0.4373694956302643\n",
      "Epoch 58/100, Step 321, Loss: 0.5691079497337341\n",
      "Epoch 58/100, Step 331, Loss: 0.2867303192615509\n",
      "Epoch 58/100, Step 341, Loss: 0.3380167484283447\n",
      "Epoch 58/100, Step 351, Loss: 0.18377840518951416\n",
      "Epoch 58/100, Step 361, Loss: 0.3860107362270355\n",
      "Epoch 59/100, Step 1, Loss: 0.20162588357925415\n",
      "Epoch 59/100, Step 11, Loss: 0.18647071719169617\n",
      "Epoch 59/100, Step 21, Loss: 0.5124527812004089\n",
      "Epoch 59/100, Step 31, Loss: 0.26603084802627563\n",
      "Epoch 59/100, Step 41, Loss: 0.5762670040130615\n",
      "Epoch 59/100, Step 51, Loss: 0.16349603235721588\n",
      "Epoch 59/100, Step 61, Loss: 0.21681363880634308\n",
      "Epoch 59/100, Step 71, Loss: 0.49375566840171814\n",
      "Epoch 59/100, Step 81, Loss: 0.17493699491024017\n",
      "Epoch 59/100, Step 91, Loss: 0.2640587091445923\n",
      "Epoch 59/100, Step 101, Loss: 0.2401699274778366\n",
      "Epoch 59/100, Step 111, Loss: 0.21967031061649323\n",
      "Epoch 59/100, Step 121, Loss: 0.3168341815471649\n",
      "Epoch 59/100, Step 131, Loss: 0.2573137581348419\n",
      "Epoch 59/100, Step 141, Loss: 0.21424107253551483\n",
      "Epoch 59/100, Step 151, Loss: 0.5346296429634094\n",
      "Epoch 59/100, Step 161, Loss: 0.29262763261795044\n",
      "Epoch 59/100, Step 171, Loss: 0.30034300684928894\n",
      "Epoch 59/100, Step 181, Loss: 0.41989412903785706\n",
      "Epoch 59/100, Step 191, Loss: 0.4046279191970825\n",
      "Epoch 59/100, Step 201, Loss: 0.19731293618679047\n",
      "Epoch 59/100, Step 211, Loss: 0.374940425157547\n",
      "Epoch 59/100, Step 221, Loss: 0.4065442681312561\n",
      "Epoch 59/100, Step 231, Loss: 0.2641213834285736\n",
      "Epoch 59/100, Step 241, Loss: 0.31937405467033386\n",
      "Epoch 59/100, Step 251, Loss: 0.1441817581653595\n",
      "Epoch 59/100, Step 261, Loss: 0.2720717787742615\n",
      "Epoch 59/100, Step 271, Loss: 0.14993524551391602\n",
      "Epoch 59/100, Step 281, Loss: 0.3606676459312439\n",
      "Epoch 59/100, Step 291, Loss: 0.1648843139410019\n",
      "Epoch 59/100, Step 301, Loss: 0.24578857421875\n",
      "Epoch 59/100, Step 311, Loss: 0.3282742202281952\n",
      "Epoch 59/100, Step 321, Loss: 0.4577224552631378\n",
      "Epoch 59/100, Step 331, Loss: 0.261003315448761\n",
      "Epoch 59/100, Step 341, Loss: 0.22855287790298462\n",
      "Epoch 59/100, Step 351, Loss: 0.30267488956451416\n",
      "Epoch 59/100, Step 361, Loss: 0.2575470507144928\n",
      "Epoch 60/100, Step 1, Loss: 0.27083832025527954\n",
      "Epoch 60/100, Step 11, Loss: 0.2557624280452728\n",
      "Epoch 60/100, Step 21, Loss: 0.23954716324806213\n",
      "Epoch 60/100, Step 31, Loss: 0.3000805974006653\n",
      "Epoch 60/100, Step 41, Loss: 0.24279433488845825\n",
      "Epoch 60/100, Step 51, Loss: 0.1557539999485016\n",
      "Epoch 60/100, Step 61, Loss: 0.22523066401481628\n",
      "Epoch 60/100, Step 71, Loss: 0.26863226294517517\n",
      "Epoch 60/100, Step 81, Loss: 0.15975654125213623\n",
      "Epoch 60/100, Step 91, Loss: 0.4853057265281677\n",
      "Epoch 60/100, Step 101, Loss: 0.3328341245651245\n",
      "Epoch 60/100, Step 111, Loss: 0.24243652820587158\n",
      "Epoch 60/100, Step 121, Loss: 0.36227652430534363\n",
      "Epoch 60/100, Step 131, Loss: 0.21219921112060547\n",
      "Epoch 60/100, Step 141, Loss: 0.21895235776901245\n",
      "Epoch 60/100, Step 151, Loss: 0.28280481696128845\n",
      "Epoch 60/100, Step 161, Loss: 0.16979855298995972\n",
      "Epoch 60/100, Step 171, Loss: 0.37420690059661865\n",
      "Epoch 60/100, Step 181, Loss: 0.3729192018508911\n",
      "Epoch 60/100, Step 191, Loss: 0.5233666300773621\n",
      "Epoch 60/100, Step 201, Loss: 0.4743897616863251\n",
      "Epoch 60/100, Step 211, Loss: 0.23107078671455383\n",
      "Epoch 60/100, Step 221, Loss: 0.512370228767395\n",
      "Epoch 60/100, Step 231, Loss: 0.2718138098716736\n",
      "Epoch 60/100, Step 241, Loss: 0.4239530861377716\n",
      "Epoch 60/100, Step 251, Loss: 0.20791956782341003\n",
      "Epoch 60/100, Step 261, Loss: 0.3601101040840149\n",
      "Epoch 60/100, Step 271, Loss: 0.10674632340669632\n",
      "Epoch 60/100, Step 281, Loss: 0.14549408853054047\n",
      "Epoch 60/100, Step 291, Loss: 0.18877919018268585\n",
      "Epoch 60/100, Step 301, Loss: 0.23712711036205292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100, Step 311, Loss: 0.15764130651950836\n",
      "Epoch 60/100, Step 321, Loss: 0.13333937525749207\n",
      "Epoch 60/100, Step 331, Loss: 0.3378867506980896\n",
      "Epoch 60/100, Step 341, Loss: 0.17179448902606964\n",
      "Epoch 60/100, Step 351, Loss: 0.12750151753425598\n",
      "Epoch 60/100, Step 361, Loss: 0.28891581296920776\n",
      "Epoch 61/100, Step 1, Loss: 0.2508259415626526\n",
      "Epoch 61/100, Step 11, Loss: 0.28905361890792847\n",
      "Epoch 61/100, Step 21, Loss: 0.21661560237407684\n",
      "Epoch 61/100, Step 31, Loss: 0.19029274582862854\n",
      "Epoch 61/100, Step 41, Loss: 0.2475425899028778\n",
      "Epoch 61/100, Step 51, Loss: 0.2881168723106384\n",
      "Epoch 61/100, Step 61, Loss: 0.17302559316158295\n",
      "Epoch 61/100, Step 71, Loss: 0.12496200948953629\n",
      "Epoch 61/100, Step 81, Loss: 0.2237272709608078\n",
      "Epoch 61/100, Step 91, Loss: 0.26754653453826904\n",
      "Epoch 61/100, Step 101, Loss: 0.2916221022605896\n",
      "Epoch 61/100, Step 111, Loss: 0.3532579839229584\n",
      "Epoch 61/100, Step 121, Loss: 0.33733320236206055\n",
      "Epoch 61/100, Step 131, Loss: 0.3171541094779968\n",
      "Epoch 61/100, Step 141, Loss: 0.160721093416214\n",
      "Epoch 61/100, Step 151, Loss: 0.32407620549201965\n",
      "Epoch 61/100, Step 161, Loss: 0.13015906512737274\n",
      "Epoch 61/100, Step 171, Loss: 0.4080081284046173\n",
      "Epoch 61/100, Step 181, Loss: 0.20224767923355103\n",
      "Epoch 61/100, Step 191, Loss: 0.6469427347183228\n",
      "Epoch 61/100, Step 201, Loss: 0.1805407851934433\n",
      "Epoch 61/100, Step 211, Loss: 0.23657116293907166\n",
      "Epoch 61/100, Step 221, Loss: 0.23387831449508667\n",
      "Epoch 61/100, Step 231, Loss: 0.3895801901817322\n",
      "Epoch 61/100, Step 241, Loss: 0.2852824628353119\n",
      "Epoch 61/100, Step 251, Loss: 0.1276816725730896\n",
      "Epoch 61/100, Step 261, Loss: 0.15982425212860107\n",
      "Epoch 61/100, Step 271, Loss: 0.24535349011421204\n",
      "Epoch 61/100, Step 281, Loss: 0.10511905699968338\n",
      "Epoch 61/100, Step 291, Loss: 0.2317732572555542\n",
      "Epoch 61/100, Step 301, Loss: 0.4581398665904999\n",
      "Epoch 61/100, Step 311, Loss: 0.27688416838645935\n",
      "Epoch 61/100, Step 321, Loss: 0.1780773103237152\n",
      "Epoch 61/100, Step 331, Loss: 0.17740805447101593\n",
      "Epoch 61/100, Step 341, Loss: 0.15807755291461945\n",
      "Epoch 61/100, Step 351, Loss: 0.4427865445613861\n",
      "Epoch 61/100, Step 361, Loss: 0.3538338840007782\n",
      "Epoch 62/100, Step 1, Loss: 0.3076653480529785\n",
      "Epoch 62/100, Step 11, Loss: 0.22973491251468658\n",
      "Epoch 62/100, Step 21, Loss: 0.3109307587146759\n",
      "Epoch 62/100, Step 31, Loss: 0.39580103754997253\n",
      "Epoch 62/100, Step 41, Loss: 0.3047230541706085\n",
      "Epoch 62/100, Step 51, Loss: 0.2271742969751358\n",
      "Epoch 62/100, Step 61, Loss: 0.3347282111644745\n",
      "Epoch 62/100, Step 71, Loss: 0.2619776129722595\n",
      "Epoch 62/100, Step 81, Loss: 0.2869209349155426\n",
      "Epoch 62/100, Step 91, Loss: 0.26850616931915283\n",
      "Epoch 62/100, Step 101, Loss: 0.2698141932487488\n",
      "Epoch 62/100, Step 111, Loss: 0.22811149060726166\n",
      "Epoch 62/100, Step 121, Loss: 0.18298272788524628\n",
      "Epoch 62/100, Step 131, Loss: 0.2040025144815445\n",
      "Epoch 62/100, Step 141, Loss: 0.36960381269454956\n",
      "Epoch 62/100, Step 151, Loss: 0.1571635901927948\n",
      "Epoch 62/100, Step 161, Loss: 0.15830866992473602\n",
      "Epoch 62/100, Step 171, Loss: 0.2765480875968933\n",
      "Epoch 62/100, Step 181, Loss: 0.2128300666809082\n",
      "Epoch 62/100, Step 191, Loss: 0.25369980931282043\n",
      "Epoch 62/100, Step 201, Loss: 0.45329490303993225\n",
      "Epoch 62/100, Step 211, Loss: 0.15117599070072174\n",
      "Epoch 62/100, Step 221, Loss: 0.3217483460903168\n",
      "Epoch 62/100, Step 231, Loss: 0.09760034829378128\n",
      "Epoch 62/100, Step 241, Loss: 0.29269295930862427\n",
      "Epoch 62/100, Step 251, Loss: 0.2494228631258011\n",
      "Epoch 62/100, Step 261, Loss: 0.17773117125034332\n",
      "Epoch 62/100, Step 271, Loss: 0.44879478216171265\n",
      "Epoch 62/100, Step 281, Loss: 0.14275307953357697\n",
      "Epoch 62/100, Step 291, Loss: 0.2014809399843216\n",
      "Epoch 62/100, Step 301, Loss: 0.20707319676876068\n",
      "Epoch 62/100, Step 311, Loss: 0.14152312278747559\n",
      "Epoch 62/100, Step 321, Loss: 0.3423894941806793\n",
      "Epoch 62/100, Step 331, Loss: 0.3695962131023407\n",
      "Epoch 62/100, Step 341, Loss: 0.45674705505371094\n",
      "Epoch 62/100, Step 351, Loss: 0.3434341847896576\n",
      "Epoch 62/100, Step 361, Loss: 0.4701668918132782\n",
      "Epoch 63/100, Step 1, Loss: 0.3309032917022705\n",
      "Epoch 63/100, Step 11, Loss: 0.11141949146986008\n",
      "Epoch 63/100, Step 21, Loss: 0.31116098165512085\n",
      "Epoch 63/100, Step 31, Loss: 0.3862280249595642\n",
      "Epoch 63/100, Step 41, Loss: 0.1755959689617157\n",
      "Epoch 63/100, Step 51, Loss: 0.2758892774581909\n",
      "Epoch 63/100, Step 61, Loss: 0.2455565482378006\n",
      "Epoch 63/100, Step 71, Loss: 0.37749534845352173\n",
      "Epoch 63/100, Step 81, Loss: 0.21932007372379303\n",
      "Epoch 63/100, Step 91, Loss: 0.17066848278045654\n",
      "Epoch 63/100, Step 101, Loss: 0.5664292573928833\n",
      "Epoch 63/100, Step 111, Loss: 0.13870438933372498\n",
      "Epoch 63/100, Step 121, Loss: 0.32819241285324097\n",
      "Epoch 63/100, Step 131, Loss: 0.20221038162708282\n",
      "Epoch 63/100, Step 141, Loss: 0.1364261358976364\n",
      "Epoch 63/100, Step 151, Loss: 0.1271817833185196\n",
      "Epoch 63/100, Step 161, Loss: 0.2608526349067688\n",
      "Epoch 63/100, Step 171, Loss: 0.2720009684562683\n",
      "Epoch 63/100, Step 181, Loss: 0.3589343726634979\n",
      "Epoch 63/100, Step 191, Loss: 0.16651220619678497\n",
      "Epoch 63/100, Step 201, Loss: 0.28740811347961426\n",
      "Epoch 63/100, Step 211, Loss: 0.5004808902740479\n",
      "Epoch 63/100, Step 221, Loss: 0.3197278380393982\n",
      "Epoch 63/100, Step 231, Loss: 0.3501127362251282\n",
      "Epoch 63/100, Step 241, Loss: 0.29032307863235474\n",
      "Epoch 63/100, Step 251, Loss: 0.3107635974884033\n",
      "Epoch 63/100, Step 261, Loss: 0.7697808146476746\n",
      "Epoch 63/100, Step 271, Loss: 0.20122356712818146\n",
      "Epoch 63/100, Step 281, Loss: 0.29653891921043396\n",
      "Epoch 63/100, Step 291, Loss: 0.20844246447086334\n",
      "Epoch 63/100, Step 301, Loss: 0.26719358563423157\n",
      "Epoch 63/100, Step 311, Loss: 0.14644630253314972\n",
      "Epoch 63/100, Step 321, Loss: 0.2845783233642578\n",
      "Epoch 63/100, Step 331, Loss: 0.3019235134124756\n",
      "Epoch 63/100, Step 341, Loss: 0.2630458474159241\n",
      "Epoch 63/100, Step 351, Loss: 0.23783469200134277\n",
      "Epoch 63/100, Step 361, Loss: 0.1637359857559204\n",
      "Epoch 64/100, Step 1, Loss: 0.2524338960647583\n",
      "Epoch 64/100, Step 11, Loss: 0.29488876461982727\n",
      "Epoch 64/100, Step 21, Loss: 0.3355746269226074\n",
      "Epoch 64/100, Step 31, Loss: 0.2159903347492218\n",
      "Epoch 64/100, Step 41, Loss: 0.22556111216545105\n",
      "Epoch 64/100, Step 51, Loss: 0.3347030580043793\n",
      "Epoch 64/100, Step 61, Loss: 0.4429456293582916\n",
      "Epoch 64/100, Step 71, Loss: 0.3136626183986664\n",
      "Epoch 64/100, Step 81, Loss: 0.30545613169670105\n",
      "Epoch 64/100, Step 91, Loss: 0.13029330968856812\n",
      "Epoch 64/100, Step 101, Loss: 0.25560253858566284\n",
      "Epoch 64/100, Step 111, Loss: 0.220600888133049\n",
      "Epoch 64/100, Step 121, Loss: 0.3420610725879669\n",
      "Epoch 64/100, Step 131, Loss: 0.30981534719467163\n",
      "Epoch 64/100, Step 141, Loss: 0.17519156634807587\n",
      "Epoch 64/100, Step 151, Loss: 0.13755476474761963\n",
      "Epoch 64/100, Step 161, Loss: 0.2005731463432312\n",
      "Epoch 64/100, Step 171, Loss: 0.21042829751968384\n",
      "Epoch 64/100, Step 181, Loss: 0.2873116433620453\n",
      "Epoch 64/100, Step 191, Loss: 0.2985531687736511\n",
      "Epoch 64/100, Step 201, Loss: 0.14269164204597473\n",
      "Epoch 64/100, Step 211, Loss: 0.1297275274991989\n",
      "Epoch 64/100, Step 221, Loss: 0.26621314883232117\n",
      "Epoch 64/100, Step 231, Loss: 0.2971106767654419\n",
      "Epoch 64/100, Step 241, Loss: 0.45373278856277466\n",
      "Epoch 64/100, Step 251, Loss: 0.26305773854255676\n",
      "Epoch 64/100, Step 261, Loss: 0.5265465974807739\n",
      "Epoch 64/100, Step 271, Loss: 0.2669696509838104\n",
      "Epoch 64/100, Step 281, Loss: 0.14214909076690674\n",
      "Epoch 64/100, Step 291, Loss: 0.31924960017204285\n",
      "Epoch 64/100, Step 301, Loss: 0.32172420620918274\n",
      "Epoch 64/100, Step 311, Loss: 0.3271734118461609\n",
      "Epoch 64/100, Step 321, Loss: 0.21672013401985168\n",
      "Epoch 64/100, Step 331, Loss: 0.21092738211154938\n",
      "Epoch 64/100, Step 341, Loss: 0.311698853969574\n",
      "Epoch 64/100, Step 351, Loss: 0.23186102509498596\n",
      "Epoch 64/100, Step 361, Loss: 0.3541529178619385\n",
      "Epoch 65/100, Step 1, Loss: 0.11875906586647034\n",
      "Epoch 65/100, Step 11, Loss: 0.11801627278327942\n",
      "Epoch 65/100, Step 21, Loss: 0.1044296994805336\n",
      "Epoch 65/100, Step 31, Loss: 0.18242135643959045\n",
      "Epoch 65/100, Step 41, Loss: 0.2441578209400177\n",
      "Epoch 65/100, Step 51, Loss: 0.2144685536623001\n",
      "Epoch 65/100, Step 61, Loss: 0.11225289851427078\n",
      "Epoch 65/100, Step 71, Loss: 0.27291354537010193\n",
      "Epoch 65/100, Step 81, Loss: 0.2114727944135666\n",
      "Epoch 65/100, Step 91, Loss: 0.2669048607349396\n",
      "Epoch 65/100, Step 101, Loss: 0.14081121981143951\n",
      "Epoch 65/100, Step 111, Loss: 0.14080403745174408\n",
      "Epoch 65/100, Step 121, Loss: 0.17878848314285278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100, Step 131, Loss: 0.15688428282737732\n",
      "Epoch 65/100, Step 141, Loss: 0.26785901188850403\n",
      "Epoch 65/100, Step 151, Loss: 0.1884002983570099\n",
      "Epoch 65/100, Step 161, Loss: 0.18410560488700867\n",
      "Epoch 65/100, Step 171, Loss: 0.21584011614322662\n",
      "Epoch 65/100, Step 181, Loss: 0.23060324788093567\n",
      "Epoch 65/100, Step 191, Loss: 0.23272904753684998\n",
      "Epoch 65/100, Step 201, Loss: 0.22766511142253876\n",
      "Epoch 65/100, Step 211, Loss: 0.1674998551607132\n",
      "Epoch 65/100, Step 221, Loss: 0.11202456802129745\n",
      "Epoch 65/100, Step 231, Loss: 0.1476382166147232\n",
      "Epoch 65/100, Step 241, Loss: 0.0939263105392456\n",
      "Epoch 65/100, Step 251, Loss: 0.21711963415145874\n",
      "Epoch 65/100, Step 261, Loss: 0.2820400595664978\n",
      "Epoch 65/100, Step 271, Loss: 0.12878896296024323\n",
      "Epoch 65/100, Step 281, Loss: 0.12350282073020935\n",
      "Epoch 65/100, Step 291, Loss: 0.1313333809375763\n",
      "Epoch 65/100, Step 301, Loss: 0.233578622341156\n",
      "Epoch 65/100, Step 311, Loss: 0.21239805221557617\n",
      "Epoch 65/100, Step 321, Loss: 0.3116298317909241\n",
      "Epoch 65/100, Step 331, Loss: 0.1275864541530609\n",
      "Epoch 65/100, Step 341, Loss: 0.22875641286373138\n",
      "Epoch 65/100, Step 351, Loss: 0.3219466805458069\n",
      "Epoch 65/100, Step 361, Loss: 0.19782745838165283\n",
      "Epoch 66/100, Step 1, Loss: 0.29204487800598145\n",
      "Epoch 66/100, Step 11, Loss: 0.23398756980895996\n",
      "Epoch 66/100, Step 21, Loss: 0.21643520891666412\n",
      "Epoch 66/100, Step 31, Loss: 0.14638331532478333\n",
      "Epoch 66/100, Step 41, Loss: 0.12920650839805603\n",
      "Epoch 66/100, Step 51, Loss: 0.1733582466840744\n",
      "Epoch 66/100, Step 61, Loss: 0.15358832478523254\n",
      "Epoch 66/100, Step 71, Loss: 0.08580282330513\n",
      "Epoch 66/100, Step 81, Loss: 0.09859155863523483\n",
      "Epoch 66/100, Step 91, Loss: 0.1700889617204666\n",
      "Epoch 66/100, Step 101, Loss: 0.21469108760356903\n",
      "Epoch 66/100, Step 111, Loss: 0.08199138194322586\n",
      "Epoch 66/100, Step 121, Loss: 0.17368048429489136\n",
      "Epoch 66/100, Step 131, Loss: 0.1583731323480606\n",
      "Epoch 66/100, Step 141, Loss: 0.264447420835495\n",
      "Epoch 66/100, Step 151, Loss: 0.19278374314308167\n",
      "Epoch 66/100, Step 161, Loss: 0.4071837067604065\n",
      "Epoch 66/100, Step 171, Loss: 0.5233089327812195\n",
      "Epoch 66/100, Step 181, Loss: 0.2327640950679779\n",
      "Epoch 66/100, Step 191, Loss: 0.24269984662532806\n",
      "Epoch 66/100, Step 201, Loss: 0.2542739808559418\n",
      "Epoch 66/100, Step 211, Loss: 0.17214664816856384\n",
      "Epoch 66/100, Step 221, Loss: 0.3210567533969879\n",
      "Epoch 66/100, Step 231, Loss: 0.29768964648246765\n",
      "Epoch 66/100, Step 241, Loss: 0.17173311114311218\n",
      "Epoch 66/100, Step 251, Loss: 0.24504192173480988\n",
      "Epoch 66/100, Step 261, Loss: 0.07282237708568573\n",
      "Epoch 66/100, Step 271, Loss: 0.309469610452652\n",
      "Epoch 66/100, Step 281, Loss: 0.2766820192337036\n",
      "Epoch 66/100, Step 291, Loss: 0.3024495542049408\n",
      "Epoch 66/100, Step 301, Loss: 0.09834769368171692\n",
      "Epoch 66/100, Step 311, Loss: 0.1067173108458519\n",
      "Epoch 66/100, Step 321, Loss: 0.19870489835739136\n",
      "Epoch 66/100, Step 331, Loss: 0.2100430727005005\n",
      "Epoch 66/100, Step 341, Loss: 0.20893044769763947\n",
      "Epoch 66/100, Step 351, Loss: 0.19751325249671936\n",
      "Epoch 66/100, Step 361, Loss: 0.25223976373672485\n",
      "Epoch 67/100, Step 1, Loss: 0.08821006864309311\n",
      "Epoch 67/100, Step 11, Loss: 0.10424945503473282\n",
      "Epoch 67/100, Step 21, Loss: 0.17814061045646667\n",
      "Epoch 67/100, Step 31, Loss: 0.14109326899051666\n",
      "Epoch 67/100, Step 41, Loss: 0.11868172883987427\n",
      "Epoch 67/100, Step 51, Loss: 0.19954946637153625\n",
      "Epoch 67/100, Step 61, Loss: 0.24121268093585968\n",
      "Epoch 67/100, Step 71, Loss: 0.2370646893978119\n",
      "Epoch 67/100, Step 81, Loss: 0.2672787010669708\n",
      "Epoch 67/100, Step 91, Loss: 0.16913652420043945\n",
      "Epoch 67/100, Step 101, Loss: 0.2682478129863739\n",
      "Epoch 67/100, Step 111, Loss: 0.12099291384220123\n",
      "Epoch 67/100, Step 121, Loss: 0.16606658697128296\n",
      "Epoch 67/100, Step 131, Loss: 0.12408657371997833\n",
      "Epoch 67/100, Step 141, Loss: 0.29724985361099243\n",
      "Epoch 67/100, Step 151, Loss: 0.5302690863609314\n",
      "Epoch 67/100, Step 161, Loss: 0.15268577635288239\n",
      "Epoch 67/100, Step 171, Loss: 0.16558533906936646\n",
      "Epoch 67/100, Step 181, Loss: 0.08880041539669037\n",
      "Epoch 67/100, Step 191, Loss: 0.17142599821090698\n",
      "Epoch 67/100, Step 201, Loss: 0.16303928196430206\n",
      "Epoch 67/100, Step 211, Loss: 0.20272469520568848\n",
      "Epoch 67/100, Step 221, Loss: 0.289172500371933\n",
      "Epoch 67/100, Step 231, Loss: 0.08083628118038177\n",
      "Epoch 67/100, Step 241, Loss: 0.18124549090862274\n",
      "Epoch 67/100, Step 251, Loss: 0.18635593354701996\n",
      "Epoch 67/100, Step 261, Loss: 0.2303323894739151\n",
      "Epoch 67/100, Step 271, Loss: 0.21676497161388397\n",
      "Epoch 67/100, Step 281, Loss: 0.2077951580286026\n",
      "Epoch 67/100, Step 291, Loss: 0.21048536896705627\n",
      "Epoch 67/100, Step 301, Loss: 0.09224054962396622\n",
      "Epoch 67/100, Step 311, Loss: 0.2677065134048462\n",
      "Epoch 67/100, Step 321, Loss: 0.19963057339191437\n",
      "Epoch 67/100, Step 331, Loss: 0.23322950303554535\n",
      "Epoch 67/100, Step 341, Loss: 0.16476833820343018\n",
      "Epoch 67/100, Step 351, Loss: 0.21942009031772614\n",
      "Epoch 67/100, Step 361, Loss: 0.15003563463687897\n",
      "Epoch 68/100, Step 1, Loss: 0.15185610949993134\n",
      "Epoch 68/100, Step 11, Loss: 0.16159506142139435\n",
      "Epoch 68/100, Step 21, Loss: 0.19103029370307922\n",
      "Epoch 68/100, Step 31, Loss: 0.2921181321144104\n",
      "Epoch 68/100, Step 41, Loss: 0.09371968358755112\n",
      "Epoch 68/100, Step 51, Loss: 0.35473355650901794\n",
      "Epoch 68/100, Step 61, Loss: 0.15188385546207428\n",
      "Epoch 68/100, Step 71, Loss: 0.16192461550235748\n",
      "Epoch 68/100, Step 81, Loss: 0.13343766331672668\n",
      "Epoch 68/100, Step 91, Loss: 0.10847340524196625\n",
      "Epoch 68/100, Step 101, Loss: 0.09746460616588593\n",
      "Epoch 68/100, Step 111, Loss: 0.2989812195301056\n",
      "Epoch 68/100, Step 121, Loss: 0.3066304326057434\n",
      "Epoch 68/100, Step 131, Loss: 0.19900862872600555\n",
      "Epoch 68/100, Step 141, Loss: 0.2395026981830597\n",
      "Epoch 68/100, Step 151, Loss: 0.15164339542388916\n",
      "Epoch 68/100, Step 161, Loss: 0.2240254282951355\n",
      "Epoch 68/100, Step 171, Loss: 0.2445007711648941\n",
      "Epoch 68/100, Step 181, Loss: 0.16912175714969635\n",
      "Epoch 68/100, Step 191, Loss: 0.20658212900161743\n",
      "Epoch 68/100, Step 201, Loss: 0.2147798091173172\n",
      "Epoch 68/100, Step 211, Loss: 0.10386151075363159\n",
      "Epoch 68/100, Step 221, Loss: 0.1354217380285263\n",
      "Epoch 68/100, Step 231, Loss: 0.1663481593132019\n",
      "Epoch 68/100, Step 241, Loss: 0.335612416267395\n",
      "Epoch 68/100, Step 251, Loss: 0.175801083445549\n",
      "Epoch 68/100, Step 261, Loss: 0.11454279720783234\n",
      "Epoch 68/100, Step 271, Loss: 0.09924671053886414\n",
      "Epoch 68/100, Step 281, Loss: 0.16337420046329498\n",
      "Epoch 68/100, Step 291, Loss: 0.2248699814081192\n",
      "Epoch 68/100, Step 301, Loss: 0.07642021030187607\n",
      "Epoch 68/100, Step 311, Loss: 0.43332186341285706\n",
      "Epoch 68/100, Step 321, Loss: 0.3081384301185608\n",
      "Epoch 68/100, Step 331, Loss: 0.4104236960411072\n",
      "Epoch 68/100, Step 341, Loss: 0.3208540678024292\n",
      "Epoch 68/100, Step 351, Loss: 0.1331653743982315\n",
      "Epoch 68/100, Step 361, Loss: 0.14199334383010864\n",
      "Epoch 69/100, Step 1, Loss: 0.12341687083244324\n",
      "Epoch 69/100, Step 11, Loss: 0.17773988842964172\n",
      "Epoch 69/100, Step 21, Loss: 0.07614708691835403\n",
      "Epoch 69/100, Step 31, Loss: 0.11479117721319199\n",
      "Epoch 69/100, Step 41, Loss: 0.24441039562225342\n",
      "Epoch 69/100, Step 51, Loss: 0.08109915256500244\n",
      "Epoch 69/100, Step 61, Loss: 0.0795329287648201\n",
      "Epoch 69/100, Step 71, Loss: 0.20600193738937378\n",
      "Epoch 69/100, Step 81, Loss: 0.09708044677972794\n",
      "Epoch 69/100, Step 91, Loss: 0.19686366617679596\n",
      "Epoch 69/100, Step 101, Loss: 0.07139910012483597\n",
      "Epoch 69/100, Step 111, Loss: 0.22805409133434296\n",
      "Epoch 69/100, Step 121, Loss: 0.10912174731492996\n",
      "Epoch 69/100, Step 131, Loss: 0.21333561837673187\n",
      "Epoch 69/100, Step 141, Loss: 0.1696513444185257\n",
      "Epoch 69/100, Step 151, Loss: 0.13406948745250702\n",
      "Epoch 69/100, Step 161, Loss: 0.10567516088485718\n",
      "Epoch 69/100, Step 171, Loss: 0.18091799318790436\n",
      "Epoch 69/100, Step 181, Loss: 0.10950745642185211\n",
      "Epoch 69/100, Step 191, Loss: 0.1270325928926468\n",
      "Epoch 69/100, Step 201, Loss: 0.09011664241552353\n",
      "Epoch 69/100, Step 211, Loss: 0.11907085031270981\n",
      "Epoch 69/100, Step 221, Loss: 0.11793491989374161\n",
      "Epoch 69/100, Step 231, Loss: 0.1886170506477356\n",
      "Epoch 69/100, Step 241, Loss: 0.17479580640792847\n",
      "Epoch 69/100, Step 251, Loss: 0.09025711566209793\n",
      "Epoch 69/100, Step 261, Loss: 0.11305977404117584\n",
      "Epoch 69/100, Step 271, Loss: 0.09243223071098328\n",
      "Epoch 69/100, Step 281, Loss: 0.07989172637462616\n",
      "Epoch 69/100, Step 291, Loss: 0.12233072519302368\n",
      "Epoch 69/100, Step 301, Loss: 0.17380976676940918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100, Step 311, Loss: 0.07948959618806839\n",
      "Epoch 69/100, Step 321, Loss: 0.10797745734453201\n",
      "Epoch 69/100, Step 331, Loss: 0.17262160778045654\n",
      "Epoch 69/100, Step 341, Loss: 0.20311005413532257\n",
      "Epoch 69/100, Step 351, Loss: 0.07859025150537491\n",
      "Epoch 69/100, Step 361, Loss: 0.1315818428993225\n",
      "Epoch 70/100, Step 1, Loss: 0.09851064532995224\n",
      "Epoch 70/100, Step 11, Loss: 0.10421687364578247\n",
      "Epoch 70/100, Step 21, Loss: 0.11501353234052658\n",
      "Epoch 70/100, Step 31, Loss: 0.13823561370372772\n",
      "Epoch 70/100, Step 41, Loss: 0.26747092604637146\n",
      "Epoch 70/100, Step 51, Loss: 0.10162068903446198\n",
      "Epoch 70/100, Step 61, Loss: 0.23457352817058563\n",
      "Epoch 70/100, Step 71, Loss: 0.14315521717071533\n",
      "Epoch 70/100, Step 81, Loss: 0.27578863501548767\n",
      "Epoch 70/100, Step 91, Loss: 0.13284005224704742\n",
      "Epoch 70/100, Step 101, Loss: 0.08119923621416092\n",
      "Epoch 70/100, Step 111, Loss: 0.24677924811840057\n",
      "Epoch 70/100, Step 121, Loss: 0.10103500634431839\n",
      "Epoch 70/100, Step 131, Loss: 0.09196673333644867\n",
      "Epoch 70/100, Step 141, Loss: 0.2237342745065689\n",
      "Epoch 70/100, Step 151, Loss: 0.13382366299629211\n",
      "Epoch 70/100, Step 161, Loss: 0.15378840267658234\n",
      "Epoch 70/100, Step 171, Loss: 0.30035635828971863\n",
      "Epoch 70/100, Step 181, Loss: 0.26098671555519104\n",
      "Epoch 70/100, Step 191, Loss: 0.20181308686733246\n",
      "Epoch 70/100, Step 201, Loss: 0.22010377049446106\n",
      "Epoch 70/100, Step 211, Loss: 0.19336679577827454\n",
      "Epoch 70/100, Step 221, Loss: 0.2502601444721222\n",
      "Epoch 70/100, Step 231, Loss: 0.13006672263145447\n",
      "Epoch 70/100, Step 241, Loss: 0.2226533144712448\n",
      "Epoch 70/100, Step 251, Loss: 0.11323730647563934\n",
      "Epoch 70/100, Step 261, Loss: 0.10955484956502914\n",
      "Epoch 70/100, Step 271, Loss: 0.18920451402664185\n",
      "Epoch 70/100, Step 281, Loss: 0.06650222092866898\n",
      "Epoch 70/100, Step 291, Loss: 0.3116982877254486\n",
      "Epoch 70/100, Step 301, Loss: 0.19356399774551392\n",
      "Epoch 70/100, Step 311, Loss: 0.14627639949321747\n",
      "Epoch 70/100, Step 321, Loss: 0.2097502201795578\n",
      "Epoch 70/100, Step 331, Loss: 0.2261326164007187\n",
      "Epoch 70/100, Step 341, Loss: 0.2001735121011734\n",
      "Epoch 70/100, Step 351, Loss: 0.1651148945093155\n",
      "Epoch 70/100, Step 361, Loss: 0.22459301352500916\n",
      "Epoch 71/100, Step 1, Loss: 0.23701539635658264\n",
      "Epoch 71/100, Step 11, Loss: 0.3071831464767456\n",
      "Epoch 71/100, Step 21, Loss: 0.17072302103042603\n",
      "Epoch 71/100, Step 31, Loss: 0.17644481360912323\n",
      "Epoch 71/100, Step 41, Loss: 0.11297079175710678\n",
      "Epoch 71/100, Step 51, Loss: 0.14649598300457\n",
      "Epoch 71/100, Step 61, Loss: 0.12750110030174255\n",
      "Epoch 71/100, Step 71, Loss: 0.0949433296918869\n",
      "Epoch 71/100, Step 81, Loss: 0.13698813319206238\n",
      "Epoch 71/100, Step 91, Loss: 0.338968962430954\n",
      "Epoch 71/100, Step 101, Loss: 0.16507478058338165\n",
      "Epoch 71/100, Step 111, Loss: 0.15909388661384583\n",
      "Epoch 71/100, Step 121, Loss: 0.1522897183895111\n",
      "Epoch 71/100, Step 131, Loss: 0.2363881766796112\n",
      "Epoch 71/100, Step 141, Loss: 0.0620855987071991\n",
      "Epoch 71/100, Step 151, Loss: 0.07651200890541077\n",
      "Epoch 71/100, Step 161, Loss: 0.16314569115638733\n",
      "Epoch 71/100, Step 171, Loss: 0.14569075405597687\n",
      "Epoch 71/100, Step 181, Loss: 0.1632545292377472\n",
      "Epoch 71/100, Step 191, Loss: 0.2414955347776413\n",
      "Epoch 71/100, Step 201, Loss: 0.12057536095380783\n",
      "Epoch 71/100, Step 211, Loss: 0.10170868784189224\n",
      "Epoch 71/100, Step 221, Loss: 0.08038484305143356\n",
      "Epoch 71/100, Step 231, Loss: 0.1278347373008728\n",
      "Epoch 71/100, Step 241, Loss: 0.11783499270677567\n",
      "Epoch 71/100, Step 251, Loss: 0.07912231981754303\n",
      "Epoch 71/100, Step 261, Loss: 0.16181440651416779\n",
      "Epoch 71/100, Step 271, Loss: 0.1581582874059677\n",
      "Epoch 71/100, Step 281, Loss: 0.12670916318893433\n",
      "Epoch 71/100, Step 291, Loss: 0.12810973823070526\n",
      "Epoch 71/100, Step 301, Loss: 0.1754169911146164\n",
      "Epoch 71/100, Step 311, Loss: 0.20864449441432953\n",
      "Epoch 71/100, Step 321, Loss: 0.17540326714515686\n",
      "Epoch 71/100, Step 331, Loss: 0.14754530787467957\n",
      "Epoch 71/100, Step 341, Loss: 0.11190132051706314\n",
      "Epoch 71/100, Step 351, Loss: 0.12779326736927032\n",
      "Epoch 71/100, Step 361, Loss: 0.21910306811332703\n",
      "Epoch 72/100, Step 1, Loss: 0.14903151988983154\n",
      "Epoch 72/100, Step 11, Loss: 0.132051482796669\n",
      "Epoch 72/100, Step 21, Loss: 0.08581256121397018\n",
      "Epoch 72/100, Step 31, Loss: 0.11015559732913971\n",
      "Epoch 72/100, Step 41, Loss: 0.12226200103759766\n",
      "Epoch 72/100, Step 51, Loss: 0.1385432481765747\n",
      "Epoch 72/100, Step 61, Loss: 0.06241181865334511\n",
      "Epoch 72/100, Step 71, Loss: 0.06332826614379883\n",
      "Epoch 72/100, Step 81, Loss: 0.2463403344154358\n",
      "Epoch 72/100, Step 91, Loss: 0.30270060896873474\n",
      "Epoch 72/100, Step 101, Loss: 0.3411691188812256\n",
      "Epoch 72/100, Step 111, Loss: 0.5303170084953308\n",
      "Epoch 72/100, Step 121, Loss: 0.2921687960624695\n",
      "Epoch 72/100, Step 131, Loss: 0.18859434127807617\n",
      "Epoch 72/100, Step 141, Loss: 0.4031042456626892\n",
      "Epoch 72/100, Step 151, Loss: 0.1519262194633484\n",
      "Epoch 72/100, Step 161, Loss: 0.24099886417388916\n",
      "Epoch 72/100, Step 171, Loss: 0.24486878514289856\n",
      "Epoch 72/100, Step 181, Loss: 0.5195810198783875\n",
      "Epoch 72/100, Step 191, Loss: 0.2965884208679199\n",
      "Epoch 72/100, Step 201, Loss: 0.34755590558052063\n",
      "Epoch 72/100, Step 211, Loss: 0.24887607991695404\n",
      "Epoch 72/100, Step 221, Loss: 0.16213999688625336\n",
      "Epoch 72/100, Step 231, Loss: 0.20383436977863312\n",
      "Epoch 72/100, Step 241, Loss: 0.19065025448799133\n",
      "Epoch 72/100, Step 251, Loss: 0.15581290423870087\n",
      "Epoch 72/100, Step 261, Loss: 0.3774122893810272\n",
      "Epoch 72/100, Step 271, Loss: 0.15432004630565643\n",
      "Epoch 72/100, Step 281, Loss: 0.15710844099521637\n",
      "Epoch 72/100, Step 291, Loss: 0.12793833017349243\n",
      "Epoch 72/100, Step 301, Loss: 0.18891781568527222\n",
      "Epoch 72/100, Step 311, Loss: 0.1876859962940216\n",
      "Epoch 72/100, Step 321, Loss: 0.2988046705722809\n",
      "Epoch 72/100, Step 331, Loss: 0.2032422125339508\n",
      "Epoch 72/100, Step 341, Loss: 0.15010438859462738\n",
      "Epoch 72/100, Step 351, Loss: 0.11385095864534378\n",
      "Epoch 72/100, Step 361, Loss: 0.20127582550048828\n",
      "Epoch 73/100, Step 1, Loss: 0.06943503022193909\n",
      "Epoch 73/100, Step 11, Loss: 0.17878511548042297\n",
      "Epoch 73/100, Step 21, Loss: 0.14120891690254211\n",
      "Epoch 73/100, Step 31, Loss: 0.12314014136791229\n",
      "Epoch 73/100, Step 41, Loss: 0.16699405014514923\n",
      "Epoch 73/100, Step 51, Loss: 0.1468634307384491\n",
      "Epoch 73/100, Step 61, Loss: 0.15413829684257507\n",
      "Epoch 73/100, Step 71, Loss: 0.1719413548707962\n",
      "Epoch 73/100, Step 81, Loss: 0.09164319932460785\n",
      "Epoch 73/100, Step 91, Loss: 0.17198991775512695\n",
      "Epoch 73/100, Step 101, Loss: 0.1838848739862442\n",
      "Epoch 73/100, Step 111, Loss: 0.09994643181562424\n",
      "Epoch 73/100, Step 121, Loss: 0.11969100683927536\n",
      "Epoch 73/100, Step 131, Loss: 0.18130336701869965\n",
      "Epoch 73/100, Step 141, Loss: 0.1111074835062027\n",
      "Epoch 73/100, Step 151, Loss: 0.2153693586587906\n",
      "Epoch 73/100, Step 161, Loss: 0.21261368691921234\n",
      "Epoch 73/100, Step 171, Loss: 0.12240927666425705\n",
      "Epoch 73/100, Step 181, Loss: 0.10416126996278763\n",
      "Epoch 73/100, Step 191, Loss: 0.30838242173194885\n",
      "Epoch 73/100, Step 201, Loss: 0.17051716148853302\n",
      "Epoch 73/100, Step 211, Loss: 0.2152191549539566\n",
      "Epoch 73/100, Step 221, Loss: 0.21640899777412415\n",
      "Epoch 73/100, Step 231, Loss: 0.17385996878147125\n",
      "Epoch 73/100, Step 241, Loss: 0.21125705540180206\n",
      "Epoch 73/100, Step 251, Loss: 0.1671391725540161\n",
      "Epoch 73/100, Step 261, Loss: 0.233368918299675\n",
      "Epoch 73/100, Step 271, Loss: 0.11393927037715912\n",
      "Epoch 73/100, Step 281, Loss: 0.1084945872426033\n",
      "Epoch 73/100, Step 291, Loss: 0.07287589460611343\n",
      "Epoch 73/100, Step 301, Loss: 0.22086991369724274\n",
      "Epoch 73/100, Step 311, Loss: 0.13166677951812744\n",
      "Epoch 73/100, Step 321, Loss: 0.14064645767211914\n",
      "Epoch 73/100, Step 331, Loss: 0.13476768136024475\n",
      "Epoch 73/100, Step 341, Loss: 0.2891441583633423\n",
      "Epoch 73/100, Step 351, Loss: 0.1742636263370514\n",
      "Epoch 73/100, Step 361, Loss: 0.13608291745185852\n",
      "Epoch 74/100, Step 1, Loss: 0.10708682984113693\n",
      "Epoch 74/100, Step 11, Loss: 0.1844361126422882\n",
      "Epoch 74/100, Step 21, Loss: 0.09764000028371811\n",
      "Epoch 74/100, Step 31, Loss: 0.1527276337146759\n",
      "Epoch 74/100, Step 41, Loss: 0.08164604753255844\n",
      "Epoch 74/100, Step 51, Loss: 0.05439150333404541\n",
      "Epoch 74/100, Step 61, Loss: 0.13627347350120544\n",
      "Epoch 74/100, Step 71, Loss: 0.14609774947166443\n",
      "Epoch 74/100, Step 81, Loss: 0.11522658169269562\n",
      "Epoch 74/100, Step 91, Loss: 0.11227160692214966\n",
      "Epoch 74/100, Step 101, Loss: 0.19576393067836761\n",
      "Epoch 74/100, Step 111, Loss: 0.08143309503793716\n",
      "Epoch 74/100, Step 121, Loss: 0.11941765248775482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100, Step 131, Loss: 0.11835917830467224\n",
      "Epoch 74/100, Step 141, Loss: 0.17400135099887848\n",
      "Epoch 74/100, Step 151, Loss: 0.1196489930152893\n",
      "Epoch 74/100, Step 161, Loss: 0.08591332286596298\n",
      "Epoch 74/100, Step 171, Loss: 0.08058497309684753\n",
      "Epoch 74/100, Step 181, Loss: 0.2843994200229645\n",
      "Epoch 74/100, Step 191, Loss: 0.09727000445127487\n",
      "Epoch 74/100, Step 201, Loss: 0.052719686180353165\n",
      "Epoch 74/100, Step 211, Loss: 0.07099110633134842\n",
      "Epoch 74/100, Step 221, Loss: 0.07119055837392807\n",
      "Epoch 74/100, Step 231, Loss: 0.0668235495686531\n",
      "Epoch 74/100, Step 241, Loss: 0.16568918526172638\n",
      "Epoch 74/100, Step 251, Loss: 0.2896624803543091\n",
      "Epoch 74/100, Step 261, Loss: 0.14789904654026031\n",
      "Epoch 74/100, Step 271, Loss: 0.3090450167655945\n",
      "Epoch 74/100, Step 281, Loss: 0.18405230343341827\n",
      "Epoch 74/100, Step 291, Loss: 0.08928730338811874\n",
      "Epoch 74/100, Step 301, Loss: 0.16824761033058167\n",
      "Epoch 74/100, Step 311, Loss: 0.22134734690189362\n",
      "Epoch 74/100, Step 321, Loss: 0.06810959428548813\n",
      "Epoch 74/100, Step 331, Loss: 0.12590031325817108\n",
      "Epoch 74/100, Step 341, Loss: 0.14442399144172668\n",
      "Epoch 74/100, Step 351, Loss: 0.04965291544795036\n",
      "Epoch 74/100, Step 361, Loss: 0.09398844838142395\n",
      "Epoch 75/100, Step 1, Loss: 0.07840561121702194\n",
      "Epoch 75/100, Step 11, Loss: 0.0740492045879364\n",
      "Epoch 75/100, Step 21, Loss: 0.09200447052717209\n",
      "Epoch 75/100, Step 31, Loss: 0.12661762535572052\n",
      "Epoch 75/100, Step 41, Loss: 0.0626809224486351\n",
      "Epoch 75/100, Step 51, Loss: 0.12649653851985931\n",
      "Epoch 75/100, Step 61, Loss: 0.10392668098211288\n",
      "Epoch 75/100, Step 71, Loss: 0.07232192158699036\n",
      "Epoch 75/100, Step 81, Loss: 0.08897724002599716\n",
      "Epoch 75/100, Step 91, Loss: 0.1708684265613556\n",
      "Epoch 75/100, Step 101, Loss: 0.06421241909265518\n",
      "Epoch 75/100, Step 111, Loss: 0.12238021194934845\n",
      "Epoch 75/100, Step 121, Loss: 0.09248451143503189\n",
      "Epoch 75/100, Step 131, Loss: 0.06624075770378113\n",
      "Epoch 75/100, Step 141, Loss: 0.10689009726047516\n",
      "Epoch 75/100, Step 151, Loss: 0.08008680492639542\n",
      "Epoch 75/100, Step 161, Loss: 0.12351345270872116\n",
      "Epoch 75/100, Step 171, Loss: 0.049231864511966705\n",
      "Epoch 75/100, Step 181, Loss: 0.0729830339550972\n",
      "Epoch 75/100, Step 191, Loss: 0.08476853370666504\n",
      "Epoch 75/100, Step 201, Loss: 0.1212368905544281\n",
      "Epoch 75/100, Step 211, Loss: 0.20904304087162018\n",
      "Epoch 75/100, Step 221, Loss: 0.05721621587872505\n",
      "Epoch 75/100, Step 231, Loss: 0.10599876195192337\n",
      "Epoch 75/100, Step 241, Loss: 0.23780767619609833\n",
      "Epoch 75/100, Step 251, Loss: 0.08363725990056992\n",
      "Epoch 75/100, Step 261, Loss: 0.1285029798746109\n",
      "Epoch 75/100, Step 271, Loss: 0.08681323379278183\n",
      "Epoch 75/100, Step 281, Loss: 0.0966026708483696\n",
      "Epoch 75/100, Step 291, Loss: 0.0830237865447998\n",
      "Epoch 75/100, Step 301, Loss: 0.15989136695861816\n",
      "Epoch 75/100, Step 311, Loss: 0.07296252250671387\n",
      "Epoch 75/100, Step 321, Loss: 0.0997810885310173\n",
      "Epoch 75/100, Step 331, Loss: 0.08228925615549088\n",
      "Epoch 75/100, Step 341, Loss: 0.1194252073764801\n",
      "Epoch 75/100, Step 351, Loss: 0.06210976466536522\n",
      "Epoch 75/100, Step 361, Loss: 0.070968396961689\n",
      "Epoch 76/100, Step 1, Loss: 0.09162019193172455\n",
      "Epoch 76/100, Step 11, Loss: 0.10881819576025009\n",
      "Epoch 76/100, Step 21, Loss: 0.07894367724657059\n",
      "Epoch 76/100, Step 31, Loss: 0.04342372342944145\n",
      "Epoch 76/100, Step 41, Loss: 0.13044235110282898\n",
      "Epoch 76/100, Step 51, Loss: 0.0984889343380928\n",
      "Epoch 76/100, Step 61, Loss: 0.2344399094581604\n",
      "Epoch 76/100, Step 71, Loss: 0.07664165645837784\n",
      "Epoch 76/100, Step 81, Loss: 0.08827295154333115\n",
      "Epoch 76/100, Step 91, Loss: 0.08723178505897522\n",
      "Epoch 76/100, Step 101, Loss: 0.056879591196775436\n",
      "Epoch 76/100, Step 111, Loss: 0.06639328598976135\n",
      "Epoch 76/100, Step 121, Loss: 0.06950484961271286\n",
      "Epoch 76/100, Step 131, Loss: 0.09163476526737213\n",
      "Epoch 76/100, Step 141, Loss: 0.13991737365722656\n",
      "Epoch 76/100, Step 151, Loss: 0.10226410627365112\n",
      "Epoch 76/100, Step 161, Loss: 0.0966610237956047\n",
      "Epoch 76/100, Step 171, Loss: 0.09289660304784775\n",
      "Epoch 76/100, Step 181, Loss: 0.09237996488809586\n",
      "Epoch 76/100, Step 191, Loss: 0.048888515681028366\n",
      "Epoch 76/100, Step 201, Loss: 0.04858042299747467\n",
      "Epoch 76/100, Step 211, Loss: 0.07652314752340317\n",
      "Epoch 76/100, Step 221, Loss: 0.12289340794086456\n",
      "Epoch 76/100, Step 231, Loss: 0.07332298159599304\n",
      "Epoch 76/100, Step 241, Loss: 0.14270034432411194\n",
      "Epoch 76/100, Step 251, Loss: 0.07211129367351532\n",
      "Epoch 76/100, Step 261, Loss: 0.10446768254041672\n",
      "Epoch 76/100, Step 271, Loss: 0.09571822732686996\n",
      "Epoch 76/100, Step 281, Loss: 0.06715082377195358\n",
      "Epoch 76/100, Step 291, Loss: 0.05990338698029518\n",
      "Epoch 76/100, Step 301, Loss: 0.03786803036928177\n",
      "Epoch 76/100, Step 311, Loss: 0.05131238326430321\n",
      "Epoch 76/100, Step 321, Loss: 0.08774402737617493\n",
      "Epoch 76/100, Step 331, Loss: 0.20797385275363922\n",
      "Epoch 76/100, Step 341, Loss: 0.20944775640964508\n",
      "Epoch 76/100, Step 351, Loss: 0.12237043678760529\n",
      "Epoch 76/100, Step 361, Loss: 0.09166626632213593\n",
      "Epoch 77/100, Step 1, Loss: 0.08829569071531296\n",
      "Epoch 77/100, Step 11, Loss: 0.1754503697156906\n",
      "Epoch 77/100, Step 21, Loss: 0.05961403250694275\n",
      "Epoch 77/100, Step 31, Loss: 0.08732058852910995\n",
      "Epoch 77/100, Step 41, Loss: 0.07954782247543335\n",
      "Epoch 77/100, Step 51, Loss: 0.08984625339508057\n",
      "Epoch 77/100, Step 61, Loss: 0.09779888391494751\n",
      "Epoch 77/100, Step 71, Loss: 0.10370703786611557\n",
      "Epoch 77/100, Step 81, Loss: 0.09535735100507736\n",
      "Epoch 77/100, Step 91, Loss: 0.04839354380965233\n",
      "Epoch 77/100, Step 101, Loss: 0.043445806950330734\n",
      "Epoch 77/100, Step 111, Loss: 0.05501198768615723\n",
      "Epoch 77/100, Step 121, Loss: 0.05219438672065735\n",
      "Epoch 77/100, Step 131, Loss: 0.06843439489603043\n",
      "Epoch 77/100, Step 141, Loss: 0.07662598788738251\n",
      "Epoch 77/100, Step 151, Loss: 0.06415928155183792\n",
      "Epoch 77/100, Step 161, Loss: 0.11526403576135635\n",
      "Epoch 77/100, Step 171, Loss: 0.061364877969026566\n",
      "Epoch 77/100, Step 181, Loss: 0.06907892227172852\n",
      "Epoch 77/100, Step 191, Loss: 0.0729675441980362\n",
      "Epoch 77/100, Step 201, Loss: 0.09316042810678482\n",
      "Epoch 77/100, Step 211, Loss: 0.13200797140598297\n",
      "Epoch 77/100, Step 221, Loss: 0.12824542820453644\n",
      "Epoch 77/100, Step 231, Loss: 0.08819580078125\n",
      "Epoch 77/100, Step 241, Loss: 0.09032756835222244\n",
      "Epoch 77/100, Step 251, Loss: 0.04932314530014992\n",
      "Epoch 77/100, Step 261, Loss: 0.1006145253777504\n",
      "Epoch 77/100, Step 271, Loss: 0.10006693005561829\n",
      "Epoch 77/100, Step 281, Loss: 0.0635518953204155\n",
      "Epoch 77/100, Step 291, Loss: 0.035222139209508896\n",
      "Epoch 77/100, Step 301, Loss: 0.1051270067691803\n",
      "Epoch 77/100, Step 311, Loss: 0.053417790681123734\n",
      "Epoch 77/100, Step 321, Loss: 0.11305193603038788\n",
      "Epoch 77/100, Step 331, Loss: 0.13175155222415924\n",
      "Epoch 77/100, Step 341, Loss: 0.04986504465341568\n",
      "Epoch 77/100, Step 351, Loss: 0.05236915126442909\n",
      "Epoch 77/100, Step 361, Loss: 0.15673409402370453\n",
      "Epoch 78/100, Step 1, Loss: 0.08280084282159805\n",
      "Epoch 78/100, Step 11, Loss: 0.03062329813838005\n",
      "Epoch 78/100, Step 21, Loss: 0.09622207283973694\n",
      "Epoch 78/100, Step 31, Loss: 0.05760520324110985\n",
      "Epoch 78/100, Step 41, Loss: 0.05695614963769913\n",
      "Epoch 78/100, Step 51, Loss: 0.13123278319835663\n",
      "Epoch 78/100, Step 61, Loss: 0.06429795920848846\n",
      "Epoch 78/100, Step 71, Loss: 0.0570116750895977\n",
      "Epoch 78/100, Step 81, Loss: 0.08159778267145157\n",
      "Epoch 78/100, Step 91, Loss: 0.2642013132572174\n",
      "Epoch 78/100, Step 101, Loss: 0.11923454701900482\n",
      "Epoch 78/100, Step 111, Loss: 0.059239864349365234\n",
      "Epoch 78/100, Step 121, Loss: 0.12884706258773804\n",
      "Epoch 78/100, Step 131, Loss: 0.1922881305217743\n",
      "Epoch 78/100, Step 141, Loss: 0.042904358357191086\n",
      "Epoch 78/100, Step 151, Loss: 0.12214586138725281\n",
      "Epoch 78/100, Step 161, Loss: 0.0702793076634407\n",
      "Epoch 78/100, Step 171, Loss: 0.06742816418409348\n",
      "Epoch 78/100, Step 181, Loss: 0.061838772147893906\n",
      "Epoch 78/100, Step 191, Loss: 0.05239889770746231\n",
      "Epoch 78/100, Step 201, Loss: 0.03371764346957207\n",
      "Epoch 78/100, Step 211, Loss: 0.10678411275148392\n",
      "Epoch 78/100, Step 221, Loss: 0.16498032212257385\n",
      "Epoch 78/100, Step 231, Loss: 0.09096812456846237\n",
      "Epoch 78/100, Step 241, Loss: 0.12547437846660614\n",
      "Epoch 78/100, Step 251, Loss: 0.06616611778736115\n",
      "Epoch 78/100, Step 261, Loss: 0.05943385511636734\n",
      "Epoch 78/100, Step 271, Loss: 0.08291979134082794\n",
      "Epoch 78/100, Step 281, Loss: 0.07939361780881882\n",
      "Epoch 78/100, Step 291, Loss: 0.16985230147838593\n",
      "Epoch 78/100, Step 301, Loss: 0.10811224579811096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100, Step 311, Loss: 0.026018353179097176\n",
      "Epoch 78/100, Step 321, Loss: 0.06975358724594116\n",
      "Epoch 78/100, Step 331, Loss: 0.07054958492517471\n",
      "Epoch 78/100, Step 341, Loss: 0.04473228380084038\n",
      "Epoch 78/100, Step 351, Loss: 0.1201004907488823\n",
      "Epoch 78/100, Step 361, Loss: 0.06311196833848953\n",
      "Epoch 79/100, Step 1, Loss: 0.20825116336345673\n",
      "Epoch 79/100, Step 11, Loss: 0.0499516986310482\n",
      "Epoch 79/100, Step 21, Loss: 0.08231004327535629\n",
      "Epoch 79/100, Step 31, Loss: 0.03244840353727341\n",
      "Epoch 79/100, Step 41, Loss: 0.08076972514390945\n",
      "Epoch 79/100, Step 51, Loss: 0.03961808606982231\n",
      "Epoch 79/100, Step 61, Loss: 0.08409851789474487\n",
      "Epoch 79/100, Step 71, Loss: 0.0935150682926178\n",
      "Epoch 79/100, Step 81, Loss: 0.03959199786186218\n",
      "Epoch 79/100, Step 91, Loss: 0.04892711713910103\n",
      "Epoch 79/100, Step 101, Loss: 0.15784940123558044\n",
      "Epoch 79/100, Step 111, Loss: 0.06104040890932083\n",
      "Epoch 79/100, Step 121, Loss: 0.07426733523607254\n",
      "Epoch 79/100, Step 131, Loss: 0.08094759285449982\n",
      "Epoch 79/100, Step 141, Loss: 0.05572785437107086\n",
      "Epoch 79/100, Step 151, Loss: 0.08756833523511887\n",
      "Epoch 79/100, Step 161, Loss: 0.11012883484363556\n",
      "Epoch 79/100, Step 171, Loss: 0.05263170599937439\n",
      "Epoch 79/100, Step 181, Loss: 0.058611638844013214\n",
      "Epoch 79/100, Step 191, Loss: 0.03905653581023216\n",
      "Epoch 79/100, Step 201, Loss: 0.10503741353750229\n",
      "Epoch 79/100, Step 211, Loss: 0.09783072769641876\n",
      "Epoch 79/100, Step 221, Loss: 0.08769740164279938\n",
      "Epoch 79/100, Step 231, Loss: 0.0415443517267704\n",
      "Epoch 79/100, Step 241, Loss: 0.06846043467521667\n",
      "Epoch 79/100, Step 251, Loss: 0.11783768981695175\n",
      "Epoch 79/100, Step 261, Loss: 0.08251781016588211\n",
      "Epoch 79/100, Step 271, Loss: 0.07067917287349701\n",
      "Epoch 79/100, Step 281, Loss: 0.07266533374786377\n",
      "Epoch 79/100, Step 291, Loss: 0.10546259582042694\n",
      "Epoch 79/100, Step 301, Loss: 0.16702795028686523\n",
      "Epoch 79/100, Step 311, Loss: 0.14665181934833527\n",
      "Epoch 79/100, Step 321, Loss: 0.0556584969162941\n",
      "Epoch 79/100, Step 331, Loss: 0.06101885437965393\n",
      "Epoch 79/100, Step 341, Loss: 0.06621208786964417\n",
      "Epoch 79/100, Step 351, Loss: 0.055457703769207\n",
      "Epoch 79/100, Step 361, Loss: 0.14368866384029388\n",
      "Epoch 80/100, Step 1, Loss: 0.0888725072145462\n",
      "Epoch 80/100, Step 11, Loss: 0.07917121797800064\n",
      "Epoch 80/100, Step 21, Loss: 0.0691794604063034\n",
      "Epoch 80/100, Step 31, Loss: 0.11630672216415405\n",
      "Epoch 80/100, Step 41, Loss: 0.06679698824882507\n",
      "Epoch 80/100, Step 51, Loss: 0.11385008692741394\n",
      "Epoch 80/100, Step 61, Loss: 0.06470287591218948\n",
      "Epoch 80/100, Step 71, Loss: 0.0675332173705101\n",
      "Epoch 80/100, Step 81, Loss: 0.20234613120555878\n",
      "Epoch 80/100, Step 91, Loss: 0.03587443009018898\n",
      "Epoch 80/100, Step 101, Loss: 0.11144580692052841\n",
      "Epoch 80/100, Step 111, Loss: 0.0707995593547821\n",
      "Epoch 80/100, Step 121, Loss: 0.15194909274578094\n",
      "Epoch 80/100, Step 131, Loss: 0.07049882411956787\n",
      "Epoch 80/100, Step 141, Loss: 0.07000341266393661\n",
      "Epoch 80/100, Step 151, Loss: 0.09379300475120544\n",
      "Epoch 80/100, Step 161, Loss: 0.054254431277513504\n",
      "Epoch 80/100, Step 171, Loss: 0.08053634315729141\n",
      "Epoch 80/100, Step 181, Loss: 0.09833376109600067\n",
      "Epoch 80/100, Step 191, Loss: 0.07748687267303467\n",
      "Epoch 80/100, Step 201, Loss: 0.05707859992980957\n",
      "Epoch 80/100, Step 211, Loss: 0.06276948004961014\n",
      "Epoch 80/100, Step 221, Loss: 0.08677265048027039\n",
      "Epoch 80/100, Step 231, Loss: 0.06074221059679985\n",
      "Epoch 80/100, Step 241, Loss: 0.06578188389539719\n",
      "Epoch 80/100, Step 251, Loss: 0.054080989211797714\n",
      "Epoch 80/100, Step 261, Loss: 0.10821237415075302\n",
      "Epoch 80/100, Step 271, Loss: 0.052805572748184204\n",
      "Epoch 80/100, Step 281, Loss: 0.11999101936817169\n",
      "Epoch 80/100, Step 291, Loss: 0.041690606623888016\n",
      "Epoch 80/100, Step 301, Loss: 0.09360375255346298\n",
      "Epoch 80/100, Step 311, Loss: 0.15061329305171967\n",
      "Epoch 80/100, Step 321, Loss: 0.030546722933650017\n",
      "Epoch 80/100, Step 331, Loss: 0.06365419924259186\n",
      "Epoch 80/100, Step 341, Loss: 0.09534959495067596\n",
      "Epoch 80/100, Step 351, Loss: 0.07918243110179901\n",
      "Epoch 80/100, Step 361, Loss: 0.05283721908926964\n",
      "Epoch 81/100, Step 1, Loss: 0.052164312452077866\n",
      "Epoch 81/100, Step 11, Loss: 0.12461867928504944\n",
      "Epoch 81/100, Step 21, Loss: 0.09011777490377426\n",
      "Epoch 81/100, Step 31, Loss: 0.12677280604839325\n",
      "Epoch 81/100, Step 41, Loss: 0.0596659816801548\n",
      "Epoch 81/100, Step 51, Loss: 0.09370310604572296\n",
      "Epoch 81/100, Step 61, Loss: 0.06530997157096863\n",
      "Epoch 81/100, Step 71, Loss: 0.08961006999015808\n",
      "Epoch 81/100, Step 81, Loss: 0.06034063920378685\n",
      "Epoch 81/100, Step 91, Loss: 0.05327847972512245\n",
      "Epoch 81/100, Step 101, Loss: 0.06467334926128387\n",
      "Epoch 81/100, Step 111, Loss: 0.04715732857584953\n",
      "Epoch 81/100, Step 121, Loss: 0.1256883442401886\n",
      "Epoch 81/100, Step 131, Loss: 0.041461456567049026\n",
      "Epoch 81/100, Step 141, Loss: 0.07252015173435211\n",
      "Epoch 81/100, Step 151, Loss: 0.07352933287620544\n",
      "Epoch 81/100, Step 161, Loss: 0.05491333827376366\n",
      "Epoch 81/100, Step 171, Loss: 0.2534432113170624\n",
      "Epoch 81/100, Step 181, Loss: 0.1933533102273941\n",
      "Epoch 81/100, Step 191, Loss: 0.04381481930613518\n",
      "Epoch 81/100, Step 201, Loss: 0.09152276813983917\n",
      "Epoch 81/100, Step 211, Loss: 0.08287784457206726\n",
      "Epoch 81/100, Step 221, Loss: 0.1659756451845169\n",
      "Epoch 81/100, Step 231, Loss: 0.0805501788854599\n",
      "Epoch 81/100, Step 241, Loss: 0.07434507459402084\n",
      "Epoch 81/100, Step 251, Loss: 0.1264989674091339\n",
      "Epoch 81/100, Step 261, Loss: 0.07631580531597137\n",
      "Epoch 81/100, Step 271, Loss: 0.04183557257056236\n",
      "Epoch 81/100, Step 281, Loss: 0.04394175484776497\n",
      "Epoch 81/100, Step 291, Loss: 0.06254842877388\n",
      "Epoch 81/100, Step 301, Loss: 0.029606878757476807\n",
      "Epoch 81/100, Step 311, Loss: 0.0569160096347332\n",
      "Epoch 81/100, Step 321, Loss: 0.12276288866996765\n",
      "Epoch 81/100, Step 331, Loss: 0.03882366046309471\n",
      "Epoch 81/100, Step 341, Loss: 0.054697517305612564\n",
      "Epoch 81/100, Step 351, Loss: 0.17022167146205902\n",
      "Epoch 81/100, Step 361, Loss: 0.20543935894966125\n",
      "Epoch 82/100, Step 1, Loss: 0.07324876636266708\n",
      "Epoch 82/100, Step 11, Loss: 0.07152605801820755\n",
      "Epoch 82/100, Step 21, Loss: 0.039628323167562485\n",
      "Epoch 82/100, Step 31, Loss: 0.04807247966527939\n",
      "Epoch 82/100, Step 41, Loss: 0.13521689176559448\n",
      "Epoch 82/100, Step 51, Loss: 0.07253848761320114\n",
      "Epoch 82/100, Step 61, Loss: 0.09211330860853195\n",
      "Epoch 82/100, Step 71, Loss: 0.10836327075958252\n",
      "Epoch 82/100, Step 81, Loss: 0.14984995126724243\n",
      "Epoch 82/100, Step 91, Loss: 0.0815441906452179\n",
      "Epoch 82/100, Step 101, Loss: 0.05674765631556511\n",
      "Epoch 82/100, Step 111, Loss: 0.04417989403009415\n",
      "Epoch 82/100, Step 121, Loss: 0.04664132744073868\n",
      "Epoch 82/100, Step 131, Loss: 0.02706059254705906\n",
      "Epoch 82/100, Step 141, Loss: 0.17451637983322144\n",
      "Epoch 82/100, Step 151, Loss: 0.06422917544841766\n",
      "Epoch 82/100, Step 161, Loss: 0.09720797091722488\n",
      "Epoch 82/100, Step 171, Loss: 0.08875948935747147\n",
      "Epoch 82/100, Step 181, Loss: 0.07381298393011093\n",
      "Epoch 82/100, Step 191, Loss: 0.12426578998565674\n",
      "Epoch 82/100, Step 201, Loss: 0.11125751584768295\n",
      "Epoch 82/100, Step 211, Loss: 0.10172399133443832\n",
      "Epoch 82/100, Step 221, Loss: 0.14295805990695953\n",
      "Epoch 82/100, Step 231, Loss: 0.09441141784191132\n",
      "Epoch 82/100, Step 241, Loss: 0.15372347831726074\n",
      "Epoch 82/100, Step 251, Loss: 0.06417959183454514\n",
      "Epoch 82/100, Step 261, Loss: 0.06883670389652252\n",
      "Epoch 82/100, Step 271, Loss: 0.03705940768122673\n",
      "Epoch 82/100, Step 281, Loss: 0.08923190832138062\n",
      "Epoch 82/100, Step 291, Loss: 0.07648194581270218\n",
      "Epoch 82/100, Step 301, Loss: 0.1504945009946823\n",
      "Epoch 82/100, Step 311, Loss: 0.0407586395740509\n",
      "Epoch 82/100, Step 321, Loss: 0.07592899352312088\n",
      "Epoch 82/100, Step 331, Loss: 0.1393301635980606\n",
      "Epoch 82/100, Step 341, Loss: 0.10684697329998016\n",
      "Epoch 82/100, Step 351, Loss: 0.1798029989004135\n",
      "Epoch 82/100, Step 361, Loss: 0.32363948225975037\n",
      "Epoch 83/100, Step 1, Loss: 0.3956880569458008\n",
      "Epoch 83/100, Step 11, Loss: 0.4332695007324219\n",
      "Epoch 83/100, Step 21, Loss: 0.5557624101638794\n",
      "Epoch 83/100, Step 31, Loss: 0.5358643531799316\n",
      "Epoch 83/100, Step 41, Loss: 0.15026681125164032\n",
      "Epoch 83/100, Step 51, Loss: 0.12627504765987396\n",
      "Epoch 83/100, Step 61, Loss: 0.13120271265506744\n",
      "Epoch 83/100, Step 71, Loss: 0.2227657437324524\n",
      "Epoch 83/100, Step 81, Loss: 0.14200979471206665\n",
      "Epoch 83/100, Step 91, Loss: 0.16564439237117767\n",
      "Epoch 83/100, Step 101, Loss: 0.12187940627336502\n",
      "Epoch 83/100, Step 111, Loss: 0.08926738053560257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100, Step 121, Loss: 0.5265539288520813\n",
      "Epoch 83/100, Step 131, Loss: 0.31713318824768066\n",
      "Epoch 83/100, Step 141, Loss: 0.09790930151939392\n",
      "Epoch 83/100, Step 151, Loss: 0.6452054381370544\n",
      "Epoch 83/100, Step 161, Loss: 0.29539018869400024\n",
      "Epoch 83/100, Step 171, Loss: 0.33747467398643494\n",
      "Epoch 83/100, Step 181, Loss: 0.321271151304245\n",
      "Epoch 83/100, Step 191, Loss: 0.13685068488121033\n",
      "Epoch 83/100, Step 201, Loss: 0.11547006666660309\n",
      "Epoch 83/100, Step 211, Loss: 0.2233303338289261\n",
      "Epoch 83/100, Step 221, Loss: 0.14867988228797913\n",
      "Epoch 83/100, Step 231, Loss: 0.13568419218063354\n",
      "Epoch 83/100, Step 241, Loss: 0.07809022814035416\n",
      "Epoch 83/100, Step 251, Loss: 0.09117498248815536\n",
      "Epoch 83/100, Step 261, Loss: 0.2728705108165741\n",
      "Epoch 83/100, Step 271, Loss: 0.19388408958911896\n",
      "Epoch 83/100, Step 281, Loss: 0.19730278849601746\n",
      "Epoch 83/100, Step 291, Loss: 0.12254873663187027\n",
      "Epoch 83/100, Step 301, Loss: 0.11566881090402603\n",
      "Epoch 83/100, Step 311, Loss: 0.2602601647377014\n",
      "Epoch 83/100, Step 321, Loss: 0.12004448473453522\n",
      "Epoch 83/100, Step 331, Loss: 0.21100689470767975\n",
      "Epoch 83/100, Step 341, Loss: 0.5876200199127197\n",
      "Epoch 83/100, Step 351, Loss: 0.31653180718421936\n",
      "Epoch 83/100, Step 361, Loss: 0.278263121843338\n",
      "Epoch 84/100, Step 1, Loss: 0.2386835217475891\n",
      "Epoch 84/100, Step 11, Loss: 0.14433792233467102\n",
      "Epoch 84/100, Step 21, Loss: 0.04576198011636734\n",
      "Epoch 84/100, Step 31, Loss: 0.12743708491325378\n",
      "Epoch 84/100, Step 41, Loss: 0.09493382275104523\n",
      "Epoch 84/100, Step 51, Loss: 0.25999754667282104\n",
      "Epoch 84/100, Step 61, Loss: 0.07903651148080826\n",
      "Epoch 84/100, Step 71, Loss: 0.17613206803798676\n",
      "Epoch 84/100, Step 81, Loss: 0.1789257675409317\n",
      "Epoch 84/100, Step 91, Loss: 0.25599220395088196\n",
      "Epoch 84/100, Step 101, Loss: 0.07344310730695724\n",
      "Epoch 84/100, Step 111, Loss: 0.21404124796390533\n",
      "Epoch 84/100, Step 121, Loss: 0.16494517028331757\n",
      "Epoch 84/100, Step 131, Loss: 0.20472431182861328\n",
      "Epoch 84/100, Step 141, Loss: 0.12061388045549393\n",
      "Epoch 84/100, Step 151, Loss: 0.03709574416279793\n",
      "Epoch 84/100, Step 161, Loss: 0.09007058292627335\n",
      "Epoch 84/100, Step 171, Loss: 0.08119413256645203\n",
      "Epoch 84/100, Step 181, Loss: 0.13504719734191895\n",
      "Epoch 84/100, Step 191, Loss: 0.15697930753231049\n",
      "Epoch 84/100, Step 201, Loss: 0.06559348851442337\n",
      "Epoch 84/100, Step 211, Loss: 0.0426667295396328\n",
      "Epoch 84/100, Step 221, Loss: 0.11184316873550415\n",
      "Epoch 84/100, Step 231, Loss: 0.15058022737503052\n",
      "Epoch 84/100, Step 241, Loss: 0.08851916342973709\n",
      "Epoch 84/100, Step 251, Loss: 0.0464099682867527\n",
      "Epoch 84/100, Step 261, Loss: 0.06134198606014252\n",
      "Epoch 84/100, Step 271, Loss: 0.08767741918563843\n",
      "Epoch 84/100, Step 281, Loss: 0.05409947782754898\n",
      "Epoch 84/100, Step 291, Loss: 0.06934287399053574\n",
      "Epoch 84/100, Step 301, Loss: 0.05525892227888107\n",
      "Epoch 84/100, Step 311, Loss: 0.03287820145487785\n",
      "Epoch 84/100, Step 321, Loss: 0.08606354147195816\n",
      "Epoch 84/100, Step 331, Loss: 0.11960743367671967\n",
      "Epoch 84/100, Step 341, Loss: 0.1193300411105156\n",
      "Epoch 84/100, Step 351, Loss: 0.13037808239459991\n",
      "Epoch 84/100, Step 361, Loss: 0.08314596116542816\n",
      "Epoch 85/100, Step 1, Loss: 0.11321151256561279\n",
      "Epoch 85/100, Step 11, Loss: 0.07932209223508835\n",
      "Epoch 85/100, Step 21, Loss: 0.0737934336066246\n",
      "Epoch 85/100, Step 31, Loss: 0.07545092701911926\n",
      "Epoch 85/100, Step 41, Loss: 0.2726062834262848\n",
      "Epoch 85/100, Step 51, Loss: 0.12313175201416016\n",
      "Epoch 85/100, Step 61, Loss: 0.116709403693676\n",
      "Epoch 85/100, Step 71, Loss: 0.18064096570014954\n",
      "Epoch 85/100, Step 81, Loss: 0.12301554530858994\n",
      "Epoch 85/100, Step 91, Loss: 0.2612164616584778\n",
      "Epoch 85/100, Step 101, Loss: 0.2567901611328125\n",
      "Epoch 85/100, Step 111, Loss: 0.17277845740318298\n",
      "Epoch 85/100, Step 121, Loss: 0.11431208997964859\n",
      "Epoch 85/100, Step 131, Loss: 0.055327244102954865\n",
      "Epoch 85/100, Step 141, Loss: 0.250068336725235\n",
      "Epoch 85/100, Step 151, Loss: 0.2312714159488678\n",
      "Epoch 85/100, Step 161, Loss: 0.2109905481338501\n",
      "Epoch 85/100, Step 171, Loss: 0.11414452642202377\n",
      "Epoch 85/100, Step 181, Loss: 0.12373978644609451\n",
      "Epoch 85/100, Step 191, Loss: 0.20729461312294006\n",
      "Epoch 85/100, Step 201, Loss: 0.10495553910732269\n",
      "Epoch 85/100, Step 211, Loss: 0.051199670881032944\n",
      "Epoch 85/100, Step 221, Loss: 0.08612055331468582\n",
      "Epoch 85/100, Step 231, Loss: 0.1948157250881195\n",
      "Epoch 85/100, Step 241, Loss: 0.32483091950416565\n",
      "Epoch 85/100, Step 251, Loss: 0.05852338671684265\n",
      "Epoch 85/100, Step 261, Loss: 0.09390793740749359\n",
      "Epoch 85/100, Step 271, Loss: 0.13681358098983765\n",
      "Epoch 85/100, Step 281, Loss: 0.08871883898973465\n",
      "Epoch 85/100, Step 291, Loss: 0.06799720227718353\n",
      "Epoch 85/100, Step 301, Loss: 0.1484016478061676\n",
      "Epoch 85/100, Step 311, Loss: 0.05840349942445755\n",
      "Epoch 85/100, Step 321, Loss: 0.10868793725967407\n",
      "Epoch 85/100, Step 331, Loss: 0.14469751715660095\n",
      "Epoch 85/100, Step 341, Loss: 0.5885376334190369\n",
      "Epoch 85/100, Step 351, Loss: 0.21833834052085876\n",
      "Epoch 85/100, Step 361, Loss: 0.3103349506855011\n",
      "Epoch 86/100, Step 1, Loss: 0.33009201288223267\n",
      "Epoch 86/100, Step 11, Loss: 0.17273551225662231\n",
      "Epoch 86/100, Step 21, Loss: 0.14130473136901855\n",
      "Epoch 86/100, Step 31, Loss: 0.327129602432251\n",
      "Epoch 86/100, Step 41, Loss: 0.20205584168434143\n",
      "Epoch 86/100, Step 51, Loss: 0.22015491127967834\n",
      "Epoch 86/100, Step 61, Loss: 0.2620753049850464\n",
      "Epoch 86/100, Step 71, Loss: 0.19799920916557312\n",
      "Epoch 86/100, Step 81, Loss: 0.14772503077983856\n",
      "Epoch 86/100, Step 91, Loss: 0.3611046373844147\n",
      "Epoch 86/100, Step 101, Loss: 0.3563552498817444\n",
      "Epoch 86/100, Step 111, Loss: 0.10151515901088715\n",
      "Epoch 86/100, Step 121, Loss: 0.1428535431623459\n",
      "Epoch 86/100, Step 131, Loss: 0.12802641093730927\n",
      "Epoch 86/100, Step 141, Loss: 0.2116970270872116\n",
      "Epoch 86/100, Step 151, Loss: 0.15539966523647308\n",
      "Epoch 86/100, Step 161, Loss: 0.25574612617492676\n",
      "Epoch 86/100, Step 171, Loss: 0.09674780815839767\n",
      "Epoch 86/100, Step 181, Loss: 0.24143697321414948\n",
      "Epoch 86/100, Step 191, Loss: 0.08385339379310608\n",
      "Epoch 86/100, Step 201, Loss: 0.08934459835290909\n",
      "Epoch 86/100, Step 211, Loss: 0.07701950520277023\n",
      "Epoch 86/100, Step 221, Loss: 0.10623815655708313\n",
      "Epoch 86/100, Step 231, Loss: 0.058210764080286026\n",
      "Epoch 86/100, Step 241, Loss: 0.07932176440954208\n",
      "Epoch 86/100, Step 251, Loss: 0.06354210525751114\n",
      "Epoch 86/100, Step 261, Loss: 0.21799135208129883\n",
      "Epoch 86/100, Step 271, Loss: 0.13225862383842468\n",
      "Epoch 86/100, Step 281, Loss: 0.1266707181930542\n",
      "Epoch 86/100, Step 291, Loss: 0.08759734034538269\n",
      "Epoch 86/100, Step 301, Loss: 0.09822236746549606\n",
      "Epoch 86/100, Step 311, Loss: 0.14026513695716858\n",
      "Epoch 86/100, Step 321, Loss: 0.17204177379608154\n",
      "Epoch 86/100, Step 331, Loss: 0.19552293419837952\n",
      "Epoch 86/100, Step 341, Loss: 0.2856697142124176\n",
      "Epoch 86/100, Step 351, Loss: 0.1086350530385971\n",
      "Epoch 86/100, Step 361, Loss: 0.18849235773086548\n",
      "Epoch 87/100, Step 1, Loss: 0.12204089760780334\n",
      "Epoch 87/100, Step 11, Loss: 0.13428626954555511\n",
      "Epoch 87/100, Step 21, Loss: 0.13115155696868896\n",
      "Epoch 87/100, Step 31, Loss: 0.09782159328460693\n",
      "Epoch 87/100, Step 41, Loss: 0.09571851789951324\n",
      "Epoch 87/100, Step 51, Loss: 0.1063394546508789\n",
      "Epoch 87/100, Step 61, Loss: 0.07354138791561127\n",
      "Epoch 87/100, Step 71, Loss: 0.09481555968523026\n",
      "Epoch 87/100, Step 81, Loss: 0.07355999946594238\n",
      "Epoch 87/100, Step 91, Loss: 0.12061186879873276\n",
      "Epoch 87/100, Step 101, Loss: 0.27219951152801514\n",
      "Epoch 87/100, Step 111, Loss: 0.06442857533693314\n",
      "Epoch 87/100, Step 121, Loss: 0.05098746716976166\n",
      "Epoch 87/100, Step 131, Loss: 0.057415418326854706\n",
      "Epoch 87/100, Step 141, Loss: 0.0649683028459549\n",
      "Epoch 87/100, Step 151, Loss: 0.0805053859949112\n",
      "Epoch 87/100, Step 161, Loss: 0.04955291375517845\n",
      "Epoch 87/100, Step 171, Loss: 0.08381224423646927\n",
      "Epoch 87/100, Step 181, Loss: 0.12535680830478668\n",
      "Epoch 87/100, Step 191, Loss: 0.08186984807252884\n",
      "Epoch 87/100, Step 201, Loss: 0.115529365837574\n",
      "Epoch 87/100, Step 211, Loss: 0.11902961134910583\n",
      "Epoch 87/100, Step 221, Loss: 0.09426295012235641\n",
      "Epoch 87/100, Step 231, Loss: 0.08939465880393982\n",
      "Epoch 87/100, Step 241, Loss: 0.08226300776004791\n",
      "Epoch 87/100, Step 251, Loss: 0.104521743953228\n",
      "Epoch 87/100, Step 261, Loss: 0.05144617334008217\n",
      "Epoch 87/100, Step 271, Loss: 0.055040501058101654\n",
      "Epoch 87/100, Step 281, Loss: 0.04993502423167229\n",
      "Epoch 87/100, Step 291, Loss: 0.10202296078205109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100, Step 301, Loss: 0.1260048896074295\n",
      "Epoch 87/100, Step 311, Loss: 0.15387536585330963\n",
      "Epoch 87/100, Step 321, Loss: 0.08898380398750305\n",
      "Epoch 87/100, Step 331, Loss: 0.0738033801317215\n",
      "Epoch 87/100, Step 341, Loss: 0.09582659602165222\n",
      "Epoch 87/100, Step 351, Loss: 0.0719393938779831\n",
      "Epoch 87/100, Step 361, Loss: 0.11398891359567642\n",
      "Epoch 88/100, Step 1, Loss: 0.07856415212154388\n",
      "Epoch 88/100, Step 11, Loss: 0.07868774235248566\n",
      "Epoch 88/100, Step 21, Loss: 0.06853215396404266\n",
      "Epoch 88/100, Step 31, Loss: 0.06836822628974915\n",
      "Epoch 88/100, Step 41, Loss: 0.061834923923015594\n",
      "Epoch 88/100, Step 51, Loss: 0.07789359241724014\n",
      "Epoch 88/100, Step 61, Loss: 0.19579914212226868\n",
      "Epoch 88/100, Step 71, Loss: 0.02791762910783291\n",
      "Epoch 88/100, Step 81, Loss: 0.0869540125131607\n",
      "Epoch 88/100, Step 91, Loss: 0.06936310976743698\n",
      "Epoch 88/100, Step 101, Loss: 0.04081251472234726\n",
      "Epoch 88/100, Step 111, Loss: 0.0752982571721077\n",
      "Epoch 88/100, Step 121, Loss: 0.06592937558889389\n",
      "Epoch 88/100, Step 131, Loss: 0.10546465963125229\n",
      "Epoch 88/100, Step 141, Loss: 0.0655253604054451\n",
      "Epoch 88/100, Step 151, Loss: 0.06619726121425629\n",
      "Epoch 88/100, Step 161, Loss: 0.03452058508992195\n",
      "Epoch 88/100, Step 171, Loss: 0.042326923459768295\n",
      "Epoch 88/100, Step 181, Loss: 0.13968472182750702\n",
      "Epoch 88/100, Step 191, Loss: 0.08407248556613922\n",
      "Epoch 88/100, Step 201, Loss: 0.0710841715335846\n",
      "Epoch 88/100, Step 211, Loss: 0.06521632522344589\n",
      "Epoch 88/100, Step 221, Loss: 0.0432356595993042\n",
      "Epoch 88/100, Step 231, Loss: 0.133207768201828\n",
      "Epoch 88/100, Step 241, Loss: 0.13597239553928375\n",
      "Epoch 88/100, Step 251, Loss: 0.15744061768054962\n",
      "Epoch 88/100, Step 261, Loss: 0.05713856965303421\n",
      "Epoch 88/100, Step 271, Loss: 0.058631137013435364\n",
      "Epoch 88/100, Step 281, Loss: 0.06284134835004807\n",
      "Epoch 88/100, Step 291, Loss: 0.07731102406978607\n",
      "Epoch 88/100, Step 301, Loss: 0.04527125507593155\n",
      "Epoch 88/100, Step 311, Loss: 0.03586031496524811\n",
      "Epoch 88/100, Step 321, Loss: 0.06317184120416641\n",
      "Epoch 88/100, Step 331, Loss: 0.07772289961576462\n",
      "Epoch 88/100, Step 341, Loss: 0.07575328648090363\n",
      "Epoch 88/100, Step 351, Loss: 0.07875414192676544\n",
      "Epoch 88/100, Step 361, Loss: 0.05553387477993965\n",
      "Epoch 89/100, Step 1, Loss: 0.02910616621375084\n",
      "Epoch 89/100, Step 11, Loss: 0.07382512837648392\n",
      "Epoch 89/100, Step 21, Loss: 0.06336867809295654\n",
      "Epoch 89/100, Step 31, Loss: 0.06462261080741882\n",
      "Epoch 89/100, Step 41, Loss: 0.041495777666568756\n",
      "Epoch 89/100, Step 51, Loss: 0.05930349603295326\n",
      "Epoch 89/100, Step 61, Loss: 0.0448143370449543\n",
      "Epoch 89/100, Step 71, Loss: 0.021199310198426247\n",
      "Epoch 89/100, Step 81, Loss: 0.09161962568759918\n",
      "Epoch 89/100, Step 91, Loss: 0.05660322681069374\n",
      "Epoch 89/100, Step 101, Loss: 0.04384635388851166\n",
      "Epoch 89/100, Step 111, Loss: 0.024526452645659447\n",
      "Epoch 89/100, Step 121, Loss: 0.06840413063764572\n",
      "Epoch 89/100, Step 131, Loss: 0.08591317385435104\n",
      "Epoch 89/100, Step 141, Loss: 0.04744597151875496\n",
      "Epoch 89/100, Step 151, Loss: 0.06772672384977341\n",
      "Epoch 89/100, Step 161, Loss: 0.07352326065301895\n",
      "Epoch 89/100, Step 171, Loss: 0.05804652348160744\n",
      "Epoch 89/100, Step 181, Loss: 0.07055205851793289\n",
      "Epoch 89/100, Step 191, Loss: 0.061764683574438095\n",
      "Epoch 89/100, Step 201, Loss: 0.042860373854637146\n",
      "Epoch 89/100, Step 211, Loss: 0.06077630817890167\n",
      "Epoch 89/100, Step 221, Loss: 0.041025351732969284\n",
      "Epoch 89/100, Step 231, Loss: 0.06041904538869858\n",
      "Epoch 89/100, Step 241, Loss: 0.07362392544746399\n",
      "Epoch 89/100, Step 251, Loss: 0.10527177900075912\n",
      "Epoch 89/100, Step 261, Loss: 0.06836765259504318\n",
      "Epoch 89/100, Step 271, Loss: 0.04110914096236229\n",
      "Epoch 89/100, Step 281, Loss: 0.04802124947309494\n",
      "Epoch 89/100, Step 291, Loss: 0.06439720094203949\n",
      "Epoch 89/100, Step 301, Loss: 0.028407683596014977\n",
      "Epoch 89/100, Step 311, Loss: 0.08250698447227478\n",
      "Epoch 89/100, Step 321, Loss: 0.07132261991500854\n",
      "Epoch 89/100, Step 331, Loss: 0.08188066631555557\n",
      "Epoch 89/100, Step 341, Loss: 0.04150090366601944\n",
      "Epoch 89/100, Step 351, Loss: 0.05168578028678894\n",
      "Epoch 89/100, Step 361, Loss: 0.06115143746137619\n",
      "Epoch 90/100, Step 1, Loss: 0.03249035030603409\n",
      "Epoch 90/100, Step 11, Loss: 0.03460966423153877\n",
      "Epoch 90/100, Step 21, Loss: 0.038505569100379944\n",
      "Epoch 90/100, Step 31, Loss: 0.030772266909480095\n",
      "Epoch 90/100, Step 41, Loss: 0.04753583297133446\n",
      "Epoch 90/100, Step 51, Loss: 0.038553982973098755\n",
      "Epoch 90/100, Step 61, Loss: 0.05911509320139885\n",
      "Epoch 90/100, Step 71, Loss: 0.03285865858197212\n",
      "Epoch 90/100, Step 81, Loss: 0.053472019731998444\n",
      "Epoch 90/100, Step 91, Loss: 0.047773875296115875\n",
      "Epoch 90/100, Step 101, Loss: 0.02505524456501007\n",
      "Epoch 90/100, Step 111, Loss: 0.09021841734647751\n",
      "Epoch 90/100, Step 121, Loss: 0.07630239427089691\n",
      "Epoch 90/100, Step 131, Loss: 0.04403518885374069\n",
      "Epoch 90/100, Step 141, Loss: 0.04751294106245041\n",
      "Epoch 90/100, Step 151, Loss: 0.03190286457538605\n",
      "Epoch 90/100, Step 161, Loss: 0.10946878045797348\n",
      "Epoch 90/100, Step 171, Loss: 0.0775449126958847\n",
      "Epoch 90/100, Step 181, Loss: 0.06530174612998962\n",
      "Epoch 90/100, Step 191, Loss: 0.07733973860740662\n",
      "Epoch 90/100, Step 201, Loss: 0.06888508051633835\n",
      "Epoch 90/100, Step 211, Loss: 0.03375505656003952\n",
      "Epoch 90/100, Step 221, Loss: 0.0551191009581089\n",
      "Epoch 90/100, Step 231, Loss: 0.034501221030950546\n",
      "Epoch 90/100, Step 241, Loss: 0.1011325791478157\n",
      "Epoch 90/100, Step 251, Loss: 0.03870219364762306\n",
      "Epoch 90/100, Step 261, Loss: 0.04225264489650726\n",
      "Epoch 90/100, Step 271, Loss: 0.08488351851701736\n",
      "Epoch 90/100, Step 281, Loss: 0.06376731395721436\n",
      "Epoch 90/100, Step 291, Loss: 0.07335866987705231\n",
      "Epoch 90/100, Step 301, Loss: 0.0395759753882885\n",
      "Epoch 90/100, Step 311, Loss: 0.053583063185214996\n",
      "Epoch 90/100, Step 321, Loss: 0.07215945422649384\n",
      "Epoch 90/100, Step 331, Loss: 0.04755353927612305\n",
      "Epoch 90/100, Step 341, Loss: 0.04077273979783058\n",
      "Epoch 90/100, Step 351, Loss: 0.03802947700023651\n",
      "Epoch 90/100, Step 361, Loss: 0.033374905586242676\n",
      "Epoch 91/100, Step 1, Loss: 0.03200291097164154\n",
      "Epoch 91/100, Step 11, Loss: 0.0333278551697731\n",
      "Epoch 91/100, Step 21, Loss: 0.022257693111896515\n",
      "Epoch 91/100, Step 31, Loss: 0.03877098858356476\n",
      "Epoch 91/100, Step 41, Loss: 0.08866612613201141\n",
      "Epoch 91/100, Step 51, Loss: 0.09862031787633896\n",
      "Epoch 91/100, Step 61, Loss: 0.027125954627990723\n",
      "Epoch 91/100, Step 71, Loss: 0.033149465918540955\n",
      "Epoch 91/100, Step 81, Loss: 0.036640364676713943\n",
      "Epoch 91/100, Step 91, Loss: 0.09171266108751297\n",
      "Epoch 91/100, Step 101, Loss: 0.042227838188409805\n",
      "Epoch 91/100, Step 111, Loss: 0.030678564682602882\n",
      "Epoch 91/100, Step 121, Loss: 0.037444960325956345\n",
      "Epoch 91/100, Step 131, Loss: 0.05203881114721298\n",
      "Epoch 91/100, Step 141, Loss: 0.045745208859443665\n",
      "Epoch 91/100, Step 151, Loss: 0.17147725820541382\n",
      "Epoch 91/100, Step 161, Loss: 0.061159539967775345\n",
      "Epoch 91/100, Step 171, Loss: 0.03261226415634155\n",
      "Epoch 91/100, Step 181, Loss: 0.0476141981780529\n",
      "Epoch 91/100, Step 191, Loss: 0.049505364149808884\n",
      "Epoch 91/100, Step 201, Loss: 0.05708973482251167\n",
      "Epoch 91/100, Step 211, Loss: 0.06388162821531296\n",
      "Epoch 91/100, Step 221, Loss: 0.0510752871632576\n",
      "Epoch 91/100, Step 231, Loss: 0.0337943397462368\n",
      "Epoch 91/100, Step 241, Loss: 0.027643578127026558\n",
      "Epoch 91/100, Step 251, Loss: 0.03744349256157875\n",
      "Epoch 91/100, Step 261, Loss: 0.02192297764122486\n",
      "Epoch 91/100, Step 271, Loss: 0.02718639373779297\n",
      "Epoch 91/100, Step 281, Loss: 0.037888288497924805\n",
      "Epoch 91/100, Step 291, Loss: 0.03881860896945\n",
      "Epoch 91/100, Step 301, Loss: 0.0319071002304554\n",
      "Epoch 91/100, Step 311, Loss: 0.034532710909843445\n",
      "Epoch 91/100, Step 321, Loss: 0.02808091789484024\n",
      "Epoch 91/100, Step 331, Loss: 0.030672047287225723\n",
      "Epoch 91/100, Step 341, Loss: 0.02064291015267372\n",
      "Epoch 91/100, Step 351, Loss: 0.031947601586580276\n",
      "Epoch 91/100, Step 361, Loss: 0.03460157662630081\n",
      "Epoch 92/100, Step 1, Loss: 0.04444638267159462\n",
      "Epoch 92/100, Step 11, Loss: 0.05254495516419411\n",
      "Epoch 92/100, Step 21, Loss: 0.03474503383040428\n",
      "Epoch 92/100, Step 31, Loss: 0.026946332305669785\n",
      "Epoch 92/100, Step 41, Loss: 0.05890240892767906\n",
      "Epoch 92/100, Step 51, Loss: 0.02898714877665043\n",
      "Epoch 92/100, Step 61, Loss: 0.033030591905117035\n",
      "Epoch 92/100, Step 71, Loss: 0.09131692349910736\n",
      "Epoch 92/100, Step 81, Loss: 0.08359970152378082\n",
      "Epoch 92/100, Step 91, Loss: 0.21072450280189514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100, Step 101, Loss: 0.11823468655347824\n",
      "Epoch 92/100, Step 111, Loss: 0.04680758714675903\n",
      "Epoch 92/100, Step 121, Loss: 0.038971640169620514\n",
      "Epoch 92/100, Step 131, Loss: 0.03560321405529976\n",
      "Epoch 92/100, Step 141, Loss: 0.10075259953737259\n",
      "Epoch 92/100, Step 151, Loss: 0.08951966464519501\n",
      "Epoch 92/100, Step 161, Loss: 0.022507142275571823\n",
      "Epoch 92/100, Step 171, Loss: 0.05621959641575813\n",
      "Epoch 92/100, Step 181, Loss: 0.06394900381565094\n",
      "Epoch 92/100, Step 191, Loss: 0.02360684610903263\n",
      "Epoch 92/100, Step 201, Loss: 0.04137718677520752\n",
      "Epoch 92/100, Step 211, Loss: 0.04037009924650192\n",
      "Epoch 92/100, Step 221, Loss: 0.03868776932358742\n",
      "Epoch 92/100, Step 231, Loss: 0.01956910453736782\n",
      "Epoch 92/100, Step 241, Loss: 0.04909563064575195\n",
      "Epoch 92/100, Step 251, Loss: 0.038201697170734406\n",
      "Epoch 92/100, Step 261, Loss: 0.01930229365825653\n",
      "Epoch 92/100, Step 271, Loss: 0.05203394964337349\n",
      "Epoch 92/100, Step 281, Loss: 0.051193807274103165\n",
      "Epoch 92/100, Step 291, Loss: 0.05325635150074959\n",
      "Epoch 92/100, Step 301, Loss: 0.03120865300297737\n",
      "Epoch 92/100, Step 311, Loss: 0.12001746147871017\n",
      "Epoch 92/100, Step 321, Loss: 0.0460905022919178\n",
      "Epoch 92/100, Step 331, Loss: 0.054898012429475784\n",
      "Epoch 92/100, Step 341, Loss: 0.040150512009859085\n",
      "Epoch 92/100, Step 351, Loss: 0.039519086480140686\n",
      "Epoch 92/100, Step 361, Loss: 0.012152028270065784\n",
      "Epoch 93/100, Step 1, Loss: 0.05184471607208252\n",
      "Epoch 93/100, Step 11, Loss: 0.06512925773859024\n",
      "Epoch 93/100, Step 21, Loss: 0.02692534774541855\n",
      "Epoch 93/100, Step 31, Loss: 0.03231430426239967\n",
      "Epoch 93/100, Step 41, Loss: 0.03125424310564995\n",
      "Epoch 93/100, Step 51, Loss: 0.09021147340536118\n",
      "Epoch 93/100, Step 61, Loss: 0.0629538893699646\n",
      "Epoch 93/100, Step 71, Loss: 0.029903337359428406\n",
      "Epoch 93/100, Step 81, Loss: 0.028857365250587463\n",
      "Epoch 93/100, Step 91, Loss: 0.020112259313464165\n",
      "Epoch 93/100, Step 101, Loss: 0.028679046779870987\n",
      "Epoch 93/100, Step 111, Loss: 0.025854021310806274\n",
      "Epoch 93/100, Step 121, Loss: 0.051536742597818375\n",
      "Epoch 93/100, Step 131, Loss: 0.04967846721410751\n",
      "Epoch 93/100, Step 141, Loss: 0.03365803137421608\n",
      "Epoch 93/100, Step 151, Loss: 0.06364071369171143\n",
      "Epoch 93/100, Step 161, Loss: 0.034954339265823364\n",
      "Epoch 93/100, Step 171, Loss: 0.03860916569828987\n",
      "Epoch 93/100, Step 181, Loss: 0.07155288755893707\n",
      "Epoch 93/100, Step 191, Loss: 0.023844687268137932\n",
      "Epoch 93/100, Step 201, Loss: 0.02905437722802162\n",
      "Epoch 93/100, Step 211, Loss: 0.034340061247348785\n",
      "Epoch 93/100, Step 221, Loss: 0.07092659920454025\n",
      "Epoch 93/100, Step 231, Loss: 0.03665202111005783\n",
      "Epoch 93/100, Step 241, Loss: 0.06990469992160797\n",
      "Epoch 93/100, Step 251, Loss: 0.04066646844148636\n",
      "Epoch 93/100, Step 261, Loss: 0.06153284013271332\n",
      "Epoch 93/100, Step 271, Loss: 0.024850714951753616\n",
      "Epoch 93/100, Step 281, Loss: 0.04100852832198143\n",
      "Epoch 93/100, Step 291, Loss: 0.07804425805807114\n",
      "Epoch 93/100, Step 301, Loss: 0.03534155711531639\n",
      "Epoch 93/100, Step 311, Loss: 0.029510708525776863\n",
      "Epoch 93/100, Step 321, Loss: 0.026155326515436172\n",
      "Epoch 93/100, Step 331, Loss: 0.03806626796722412\n",
      "Epoch 93/100, Step 341, Loss: 0.04426896572113037\n",
      "Epoch 93/100, Step 351, Loss: 0.029556257650256157\n",
      "Epoch 93/100, Step 361, Loss: 0.035671934485435486\n",
      "Epoch 94/100, Step 1, Loss: 0.03651614487171173\n",
      "Epoch 94/100, Step 11, Loss: 0.0765710175037384\n",
      "Epoch 94/100, Step 21, Loss: 0.02024378441274166\n",
      "Epoch 94/100, Step 31, Loss: 0.03906850889325142\n",
      "Epoch 94/100, Step 41, Loss: 0.032086994498968124\n",
      "Epoch 94/100, Step 51, Loss: 0.021339567378163338\n",
      "Epoch 94/100, Step 61, Loss: 0.02674548141658306\n",
      "Epoch 94/100, Step 71, Loss: 0.019331052899360657\n",
      "Epoch 94/100, Step 81, Loss: 0.02453959919512272\n",
      "Epoch 94/100, Step 91, Loss: 0.022996626794338226\n",
      "Epoch 94/100, Step 101, Loss: 0.03556864336133003\n",
      "Epoch 94/100, Step 111, Loss: 0.021463152021169662\n",
      "Epoch 94/100, Step 121, Loss: 0.03461475670337677\n",
      "Epoch 94/100, Step 131, Loss: 0.055702388286590576\n",
      "Epoch 94/100, Step 141, Loss: 0.027133356779813766\n",
      "Epoch 94/100, Step 151, Loss: 0.0285626370459795\n",
      "Epoch 94/100, Step 161, Loss: 0.06647402793169022\n",
      "Epoch 94/100, Step 171, Loss: 0.028600158169865608\n",
      "Epoch 94/100, Step 181, Loss: 0.03622930869460106\n",
      "Epoch 94/100, Step 191, Loss: 0.06359358131885529\n",
      "Epoch 94/100, Step 201, Loss: 0.013523122295737267\n",
      "Epoch 94/100, Step 211, Loss: 0.0479174368083477\n",
      "Epoch 94/100, Step 221, Loss: 0.02679610811173916\n",
      "Epoch 94/100, Step 231, Loss: 0.04585392773151398\n",
      "Epoch 94/100, Step 241, Loss: 0.020191729068756104\n",
      "Epoch 94/100, Step 251, Loss: 0.028602279722690582\n",
      "Epoch 94/100, Step 261, Loss: 0.0316767543554306\n",
      "Epoch 94/100, Step 271, Loss: 0.02377443201839924\n",
      "Epoch 94/100, Step 281, Loss: 0.06090021878480911\n",
      "Epoch 94/100, Step 291, Loss: 0.027432357892394066\n",
      "Epoch 94/100, Step 301, Loss: 0.030452728271484375\n",
      "Epoch 94/100, Step 311, Loss: 0.042334266006946564\n",
      "Epoch 94/100, Step 321, Loss: 0.051447559148073196\n",
      "Epoch 94/100, Step 331, Loss: 0.01749357022345066\n",
      "Epoch 94/100, Step 341, Loss: 0.10199375450611115\n",
      "Epoch 94/100, Step 351, Loss: 0.034238800406455994\n",
      "Epoch 94/100, Step 361, Loss: 0.024038223549723625\n",
      "Epoch 95/100, Step 1, Loss: 0.026542456820607185\n",
      "Epoch 95/100, Step 11, Loss: 0.030788270756602287\n",
      "Epoch 95/100, Step 21, Loss: 0.024028120562434196\n",
      "Epoch 95/100, Step 31, Loss: 0.025251271203160286\n",
      "Epoch 95/100, Step 41, Loss: 0.027213778346776962\n",
      "Epoch 95/100, Step 51, Loss: 0.029744116589426994\n",
      "Epoch 95/100, Step 61, Loss: 0.13565830886363983\n",
      "Epoch 95/100, Step 71, Loss: 0.07101257890462875\n",
      "Epoch 95/100, Step 81, Loss: 0.05385322868824005\n",
      "Epoch 95/100, Step 91, Loss: 0.036468129605054855\n",
      "Epoch 95/100, Step 101, Loss: 0.033104825764894485\n",
      "Epoch 95/100, Step 111, Loss: 0.034547753632068634\n",
      "Epoch 95/100, Step 121, Loss: 0.04628490284085274\n",
      "Epoch 95/100, Step 131, Loss: 0.02082647755742073\n",
      "Epoch 95/100, Step 141, Loss: 0.04370128735899925\n",
      "Epoch 95/100, Step 151, Loss: 0.06331469118595123\n",
      "Epoch 95/100, Step 161, Loss: 0.02136981673538685\n",
      "Epoch 95/100, Step 171, Loss: 0.02439253404736519\n",
      "Epoch 95/100, Step 181, Loss: 0.03336726874113083\n",
      "Epoch 95/100, Step 191, Loss: 0.022828856483101845\n",
      "Epoch 95/100, Step 201, Loss: 0.06366102397441864\n",
      "Epoch 95/100, Step 211, Loss: 0.03517603501677513\n",
      "Epoch 95/100, Step 221, Loss: 0.03313550725579262\n",
      "Epoch 95/100, Step 231, Loss: 0.03249914571642876\n",
      "Epoch 95/100, Step 241, Loss: 0.031128408387303352\n",
      "Epoch 95/100, Step 251, Loss: 0.019698025658726692\n",
      "Epoch 95/100, Step 261, Loss: 0.045203275978565216\n",
      "Epoch 95/100, Step 271, Loss: 0.01766985096037388\n",
      "Epoch 95/100, Step 281, Loss: 0.0296296626329422\n",
      "Epoch 95/100, Step 291, Loss: 0.027467165142297745\n",
      "Epoch 95/100, Step 301, Loss: 0.038853950798511505\n",
      "Epoch 95/100, Step 311, Loss: 0.025208918377757072\n",
      "Epoch 95/100, Step 321, Loss: 0.021769454702734947\n",
      "Epoch 95/100, Step 331, Loss: 0.04047434777021408\n",
      "Epoch 95/100, Step 341, Loss: 0.10047455132007599\n",
      "Epoch 95/100, Step 351, Loss: 0.036715224385261536\n",
      "Epoch 95/100, Step 361, Loss: 0.15502841770648956\n",
      "Epoch 96/100, Step 1, Loss: 0.021944548934698105\n",
      "Epoch 96/100, Step 11, Loss: 0.04674436151981354\n",
      "Epoch 96/100, Step 21, Loss: 0.03424842655658722\n",
      "Epoch 96/100, Step 31, Loss: 0.0356292724609375\n",
      "Epoch 96/100, Step 41, Loss: 0.01627480797469616\n",
      "Epoch 96/100, Step 51, Loss: 0.05426294356584549\n",
      "Epoch 96/100, Step 61, Loss: 0.017819074913859367\n",
      "Epoch 96/100, Step 71, Loss: 0.02469612844288349\n",
      "Epoch 96/100, Step 81, Loss: 0.017458954825997353\n",
      "Epoch 96/100, Step 91, Loss: 0.07055380195379257\n",
      "Epoch 96/100, Step 101, Loss: 0.036362312734127045\n",
      "Epoch 96/100, Step 111, Loss: 0.03747482970356941\n",
      "Epoch 96/100, Step 121, Loss: 0.10062852501869202\n",
      "Epoch 96/100, Step 131, Loss: 0.03362317755818367\n",
      "Epoch 96/100, Step 141, Loss: 0.02002243883907795\n",
      "Epoch 96/100, Step 151, Loss: 0.030411474406719208\n",
      "Epoch 96/100, Step 161, Loss: 0.02098371833562851\n",
      "Epoch 96/100, Step 171, Loss: 0.043091535568237305\n",
      "Epoch 96/100, Step 181, Loss: 0.02318032830953598\n",
      "Epoch 96/100, Step 191, Loss: 0.02446204423904419\n",
      "Epoch 96/100, Step 201, Loss: 0.020502271130681038\n",
      "Epoch 96/100, Step 211, Loss: 0.030255040153861046\n",
      "Epoch 96/100, Step 221, Loss: 0.04702642932534218\n",
      "Epoch 96/100, Step 231, Loss: 0.02234295755624771\n",
      "Epoch 96/100, Step 241, Loss: 0.02290748432278633\n",
      "Epoch 96/100, Step 251, Loss: 0.0213161688297987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100, Step 261, Loss: 0.01693134568631649\n",
      "Epoch 96/100, Step 271, Loss: 0.02141587994992733\n",
      "Epoch 96/100, Step 281, Loss: 0.02692672424018383\n",
      "Epoch 96/100, Step 291, Loss: 0.06052294746041298\n",
      "Epoch 96/100, Step 301, Loss: 0.01999417133629322\n",
      "Epoch 96/100, Step 311, Loss: 0.025944044813513756\n",
      "Epoch 96/100, Step 321, Loss: 0.027111582458019257\n",
      "Epoch 96/100, Step 331, Loss: 0.024091392755508423\n",
      "Epoch 96/100, Step 341, Loss: 0.035403985530138016\n",
      "Epoch 96/100, Step 351, Loss: 0.03156869485974312\n",
      "Epoch 96/100, Step 361, Loss: 0.04596955329179764\n",
      "Epoch 97/100, Step 1, Loss: 0.022454453632235527\n",
      "Epoch 97/100, Step 11, Loss: 0.05348461866378784\n",
      "Epoch 97/100, Step 21, Loss: 0.027130452916026115\n",
      "Epoch 97/100, Step 31, Loss: 0.03595319762825966\n",
      "Epoch 97/100, Step 41, Loss: 0.014694973826408386\n",
      "Epoch 97/100, Step 51, Loss: 0.01837245002388954\n",
      "Epoch 97/100, Step 61, Loss: 0.02216074988245964\n",
      "Epoch 97/100, Step 71, Loss: 0.019360220059752464\n",
      "Epoch 97/100, Step 81, Loss: 0.027408115565776825\n",
      "Epoch 97/100, Step 91, Loss: 0.04178375378251076\n",
      "Epoch 97/100, Step 101, Loss: 0.030331000685691833\n",
      "Epoch 97/100, Step 111, Loss: 0.035933610051870346\n",
      "Epoch 97/100, Step 121, Loss: 0.03887627273797989\n",
      "Epoch 97/100, Step 131, Loss: 0.028232567012310028\n",
      "Epoch 97/100, Step 141, Loss: 0.06670769304037094\n",
      "Epoch 97/100, Step 151, Loss: 0.024766454473137856\n",
      "Epoch 97/100, Step 161, Loss: 0.03267921879887581\n",
      "Epoch 97/100, Step 171, Loss: 0.10301481187343597\n",
      "Epoch 97/100, Step 181, Loss: 0.025613807141780853\n",
      "Epoch 97/100, Step 191, Loss: 0.018490605056285858\n",
      "Epoch 97/100, Step 201, Loss: 0.04976135864853859\n",
      "Epoch 97/100, Step 211, Loss: 0.037356648594141006\n",
      "Epoch 97/100, Step 221, Loss: 0.017266806215047836\n",
      "Epoch 97/100, Step 231, Loss: 0.023537198081612587\n",
      "Epoch 97/100, Step 241, Loss: 0.03089280240237713\n",
      "Epoch 97/100, Step 251, Loss: 0.05255207046866417\n",
      "Epoch 97/100, Step 261, Loss: 0.01785261742770672\n",
      "Epoch 97/100, Step 271, Loss: 0.02224605903029442\n",
      "Epoch 97/100, Step 281, Loss: 0.024888139218091965\n",
      "Epoch 97/100, Step 291, Loss: 0.03235364332795143\n",
      "Epoch 97/100, Step 301, Loss: 0.03551400080323219\n",
      "Epoch 97/100, Step 311, Loss: 0.04474126547574997\n",
      "Epoch 97/100, Step 321, Loss: 0.02876908704638481\n",
      "Epoch 97/100, Step 331, Loss: 0.025889644399285316\n",
      "Epoch 97/100, Step 341, Loss: 0.0333329476416111\n",
      "Epoch 97/100, Step 351, Loss: 0.04122165963053703\n",
      "Epoch 97/100, Step 361, Loss: 0.025556577369570732\n",
      "Epoch 98/100, Step 1, Loss: 0.04844951257109642\n",
      "Epoch 98/100, Step 11, Loss: 0.020521316677331924\n",
      "Epoch 98/100, Step 21, Loss: 0.030125558376312256\n",
      "Epoch 98/100, Step 31, Loss: 0.028371695429086685\n",
      "Epoch 98/100, Step 41, Loss: 0.04210071638226509\n",
      "Epoch 98/100, Step 51, Loss: 0.01632971130311489\n",
      "Epoch 98/100, Step 61, Loss: 0.11412037909030914\n",
      "Epoch 98/100, Step 71, Loss: 0.038852956146001816\n",
      "Epoch 98/100, Step 81, Loss: 0.0669616162776947\n",
      "Epoch 98/100, Step 91, Loss: 0.04976226016879082\n",
      "Epoch 98/100, Step 101, Loss: 0.046543676406145096\n",
      "Epoch 98/100, Step 111, Loss: 0.03227952495217323\n",
      "Epoch 98/100, Step 121, Loss: 0.07266399264335632\n",
      "Epoch 98/100, Step 131, Loss: 0.03223954141139984\n",
      "Epoch 98/100, Step 141, Loss: 0.04700975865125656\n",
      "Epoch 98/100, Step 151, Loss: 0.04261833429336548\n",
      "Epoch 98/100, Step 161, Loss: 0.027268752455711365\n",
      "Epoch 98/100, Step 171, Loss: 0.09511972963809967\n",
      "Epoch 98/100, Step 181, Loss: 0.03240320831537247\n",
      "Epoch 98/100, Step 191, Loss: 0.07564806193113327\n",
      "Epoch 98/100, Step 201, Loss: 0.037616390734910965\n",
      "Epoch 98/100, Step 211, Loss: 0.0279985424131155\n",
      "Epoch 98/100, Step 221, Loss: 0.05207406356930733\n",
      "Epoch 98/100, Step 231, Loss: 0.053491804748773575\n",
      "Epoch 98/100, Step 241, Loss: 0.031007463112473488\n",
      "Epoch 98/100, Step 251, Loss: 0.03423190489411354\n",
      "Epoch 98/100, Step 261, Loss: 0.02138916775584221\n",
      "Epoch 98/100, Step 271, Loss: 0.0536557100713253\n",
      "Epoch 98/100, Step 281, Loss: 0.06088532134890556\n",
      "Epoch 98/100, Step 291, Loss: 0.020842690020799637\n",
      "Epoch 98/100, Step 301, Loss: 0.016624515876173973\n",
      "Epoch 98/100, Step 311, Loss: 0.06915569305419922\n",
      "Epoch 98/100, Step 321, Loss: 0.026433652266860008\n",
      "Epoch 98/100, Step 331, Loss: 0.08675152063369751\n",
      "Epoch 98/100, Step 341, Loss: 0.026263849809765816\n",
      "Epoch 98/100, Step 351, Loss: 0.07487065345048904\n",
      "Epoch 98/100, Step 361, Loss: 0.35944145917892456\n",
      "Epoch 99/100, Step 1, Loss: 0.032547689974308014\n",
      "Epoch 99/100, Step 11, Loss: 0.020852847024798393\n",
      "Epoch 99/100, Step 21, Loss: 0.027149304747581482\n",
      "Epoch 99/100, Step 31, Loss: 0.03443864732980728\n",
      "Epoch 99/100, Step 41, Loss: 0.02524109184741974\n",
      "Epoch 99/100, Step 51, Loss: 0.030401354655623436\n",
      "Epoch 99/100, Step 61, Loss: 0.2495574951171875\n",
      "Epoch 99/100, Step 71, Loss: 0.09550002217292786\n",
      "Epoch 99/100, Step 81, Loss: 0.07295152544975281\n",
      "Epoch 99/100, Step 91, Loss: 0.10144644230604172\n",
      "Epoch 99/100, Step 101, Loss: 0.04663180932402611\n",
      "Epoch 99/100, Step 111, Loss: 0.1921495646238327\n",
      "Epoch 99/100, Step 121, Loss: 0.05223739147186279\n",
      "Epoch 99/100, Step 131, Loss: 0.03648016229271889\n",
      "Epoch 99/100, Step 141, Loss: 0.06550789624452591\n",
      "Epoch 99/100, Step 151, Loss: 0.13820987939834595\n",
      "Epoch 99/100, Step 161, Loss: 0.08559993654489517\n",
      "Epoch 99/100, Step 171, Loss: 0.07368870079517365\n",
      "Epoch 99/100, Step 181, Loss: 0.03173210099339485\n",
      "Epoch 99/100, Step 191, Loss: 0.07328785210847855\n",
      "Epoch 99/100, Step 201, Loss: 0.07053691893815994\n",
      "Epoch 99/100, Step 211, Loss: 0.03252987563610077\n",
      "Epoch 99/100, Step 221, Loss: 0.030385354533791542\n",
      "Epoch 99/100, Step 231, Loss: 0.09896707534790039\n",
      "Epoch 99/100, Step 241, Loss: 0.11189136654138565\n",
      "Epoch 99/100, Step 251, Loss: 0.06943950057029724\n",
      "Epoch 99/100, Step 261, Loss: 0.07554502785205841\n",
      "Epoch 99/100, Step 271, Loss: 0.04787371680140495\n",
      "Epoch 99/100, Step 281, Loss: 0.06849969178438187\n",
      "Epoch 99/100, Step 291, Loss: 0.10929973423480988\n",
      "Epoch 99/100, Step 301, Loss: 0.1315750777721405\n",
      "Epoch 99/100, Step 311, Loss: 0.04625014215707779\n",
      "Epoch 99/100, Step 321, Loss: 0.11589236557483673\n",
      "Epoch 99/100, Step 331, Loss: 0.09807148575782776\n",
      "Epoch 99/100, Step 341, Loss: 0.11371100693941116\n",
      "Epoch 99/100, Step 351, Loss: 0.10422834753990173\n",
      "Epoch 99/100, Step 361, Loss: 0.09482946991920471\n",
      "Epoch 100/100, Step 1, Loss: 0.03986385464668274\n",
      "Epoch 100/100, Step 11, Loss: 0.040960054844617844\n",
      "Epoch 100/100, Step 21, Loss: 0.03707224130630493\n",
      "Epoch 100/100, Step 31, Loss: 0.05245888605713844\n",
      "Epoch 100/100, Step 41, Loss: 0.04345806688070297\n",
      "Epoch 100/100, Step 51, Loss: 0.08391491323709488\n",
      "Epoch 100/100, Step 61, Loss: 0.060749586671590805\n",
      "Epoch 100/100, Step 71, Loss: 0.29393208026885986\n",
      "Epoch 100/100, Step 81, Loss: 0.0620560385286808\n",
      "Epoch 100/100, Step 91, Loss: 0.0786147192120552\n",
      "Epoch 100/100, Step 101, Loss: 0.08045528829097748\n",
      "Epoch 100/100, Step 111, Loss: 0.05334651842713356\n",
      "Epoch 100/100, Step 121, Loss: 0.07210464030504227\n",
      "Epoch 100/100, Step 131, Loss: 0.0265930388122797\n",
      "Epoch 100/100, Step 141, Loss: 0.06496817618608475\n",
      "Epoch 100/100, Step 151, Loss: 0.042930640280246735\n",
      "Epoch 100/100, Step 161, Loss: 0.07513954490423203\n",
      "Epoch 100/100, Step 171, Loss: 0.12001150101423264\n",
      "Epoch 100/100, Step 181, Loss: 0.09445922076702118\n",
      "Epoch 100/100, Step 191, Loss: 0.04184965044260025\n",
      "Epoch 100/100, Step 201, Loss: 0.07006225734949112\n",
      "Epoch 100/100, Step 211, Loss: 0.08047180622816086\n",
      "Epoch 100/100, Step 221, Loss: 0.12197908014059067\n",
      "Epoch 100/100, Step 231, Loss: 0.07739970833063126\n",
      "Epoch 100/100, Step 241, Loss: 0.08592262119054794\n",
      "Epoch 100/100, Step 251, Loss: 0.6300262212753296\n",
      "Epoch 100/100, Step 261, Loss: 0.05342070758342743\n",
      "Epoch 100/100, Step 271, Loss: 0.07451480627059937\n",
      "Epoch 100/100, Step 281, Loss: 0.1326463669538498\n",
      "Epoch 100/100, Step 291, Loss: 0.09224012494087219\n",
      "Epoch 100/100, Step 301, Loss: 0.04480723664164543\n",
      "Epoch 100/100, Step 311, Loss: 0.09388014674186707\n",
      "Epoch 100/100, Step 321, Loss: 0.04966867342591286\n",
      "Epoch 100/100, Step 331, Loss: 0.26093679666519165\n",
      "Epoch 100/100, Step 341, Loss: 0.06848115473985672\n",
      "Epoch 100/100, Step 351, Loss: 0.1015239804983139\n",
      "Epoch 100/100, Step 361, Loss: 0.12794063985347748\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "seq2seq_model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, captions) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = seq2seq_model(features, captions[:, :-1])\n",
    "        targets = captions[:, 1:].reshape(-1)\n",
    "        \n",
    "        # flattens the outputs for loss calculation\n",
    "        outputs = outputs.view(-1, outputs.size(-1))\n",
    "        \n",
    "        # computes the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Step {i+1}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c6dca4",
   "metadata": {},
   "source": [
    "### Save and Load Seq2Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c65bb46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danis\\AppData\\Local\\Temp\\ipykernel_15344\\1180570867.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  seq2seq_model.load_state_dict(torch.load('seq2seq_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the entire Seq2Seq model\n",
    "torch.save(seq2seq_model.state_dict(), 'seq2seq_model.pth')\n",
    "\n",
    "seq2seq_model.load_state_dict(torch.load('seq2seq_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b100f0d",
   "metadata": {},
   "source": [
    "### Save/Load Encoder/Decoder separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "67172231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danis\\AppData\\Local\\Temp\\ipykernel_15344\\4196068226.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder.load_state_dict(torch.load('encoder.pth'))\n",
      "C:\\Users\\danis\\AppData\\Local\\Temp\\ipykernel_15344\\4196068226.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  decoder.load_state_dict(torch.load('decoder.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model\n",
    "torch.save(encoder.state_dict(), 'encoder.pth')\n",
    "torch.save(decoder.state_dict(), 'decoder.pth')\n",
    "\n",
    "# Loading the model\n",
    "encoder.load_state_dict(torch.load('encoder.pth'))\n",
    "decoder.load_state_dict(torch.load('decoder.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9a83e0",
   "metadata": {},
   "source": [
    "### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "87131878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danis\\AppData\\Local\\Temp\\ipykernel_15344\\2609429317.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_caption = F.pad(torch.tensor(caption), (0, max_length - len(caption)), value=vocab.word2idx[vocab.pad_token])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.00944761940980335\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# BLEU score calculation function\n",
    "def calculate_bleu(reference, candidate):\n",
    "    smooth = SmoothingFunction().method4\n",
    "    return sentence_bleu([reference], candidate, smoothing_function=smooth)\n",
    "\n",
    "# Function to evaluate the model and calculate BLEU scores\n",
    "def evaluate_model(test_dataloader, seq2seq_model, vocab):\n",
    "    seq2seq_model.eval()\n",
    "    total_bleu_score = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, captions in test_dataloader:\n",
    "            # Forward pass through the model\n",
    "            outputs = seq2seq_model(features, captions[:, :-1])\n",
    "            _, predicted_indices = torch.max(outputs, dim=2)\n",
    "\n",
    "            # Iterate over each sample in the batch\n",
    "            for i in range(features.size(0)):\n",
    "\n",
    "                predicted_caption = vocab.denumericalize(predicted_indices[i])\n",
    "                reference_caption = vocab.denumericalize([idx for idx in captions[i].tolist() if idx != vocab.word2idx[vocab.pad_token]])\n",
    "\n",
    "                # Calculates BLEU score\n",
    "                bleu_score = calculate_bleu(reference_caption, predicted_caption)\n",
    "                total_bleu_score += bleu_score\n",
    "                total_samples += 1\n",
    "\n",
    "                # Print some examples (optional)\n",
    "#                 print(f\"Reference: {' '.join(reference_caption)}\")\n",
    "#                 print(f\"Predicted: {' '.join(predicted_caption)}\")\n",
    "#                 print(f\"BLEU Score: {bleu_score}\\n\")\n",
    "\n",
    "    # Computes average BLEU score\n",
    "    average_bleu_score = total_bleu_score / total_samples if total_samples > 0 else 0\n",
    "    print(f\"Average BLEU Score: {average_bleu_score}\")\n",
    "\n",
    "\n",
    "evaluate_model(test_dataloader, seq2seq_model, vocab)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
